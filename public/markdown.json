{"version":1,"datetime":1572344575349,"dicts":{"\\0001-positioning\\0001-positioning.md":"---\r\nNumber: \"0001\"\r\nCategory: Informational\r\nStatus: Draft\r\nAuthor: The Nervos Team\r\nOrganization: Nervos Foundation\r\nCreated: 2019-09-12\r\n---\r\n\r\n# The Nervos Network Positioning Paper\r\n\r\n## 1.Purpose of This Paper\r\n\r\nThe Nervos Network is made up of a number of protocols and innovations. It's important to have clear documentation and technical specifications on key protocol design and implementations - for which we utilize an [RFC](https://github.com/nervosnetwork/rfcs) (request for comment) process. However, we feel it's equally important that we help our communities to understand what we try to accomplish, the trade-offs we have made, and how we have arrived at our current design decisions.\r\n\r\nWe start this document with a detailed examination of the problems that public permissionless blockchains face today and the existing solutions attempting to solve them. We hope this provides the necessary context for our readers to understand our own rationale on how best to approach these challenges, and our underlying design decisions. We then provide a high-level walkthrough of all parts of the Nervos Network, with a focus on how they work together to support the overall vision of the network.\r\n\r\n## 2. Background\r\n \r\nScalability, sustainability and interoperability are among the largest challenges public permissionless blockchains face today. While many projects claim to have solutions to these problems, it's important to understand where these problems come from and put solutions in the context of possible trade-offs.\r\n\r\n### 2.1 Scalability\r\n\r\nBitcoin[1] was the first public permissionless blockchain, designed to be used as peer-to-peer electronic cash. Ethereum[2] made more use cases possible and created a general purpose decentralized computing platform. However, both of these platforms impose limitations on their transaction capabilities - Bitcoin caps its block size and Ethereum caps its block gas limit. These are necessary steps to ensure long-term decentralization, however they also limit the capabilities of both platforms. \r\n\r\nThe blockchain community has proposed many scalability solutions in recent years. In general, we can divide these solutions into two categories: on-chain scaling and off-chain scaling.\r\n\r\nOn-chain scaling solutions aim to expand the throughput of the consensus process and create blockchains with native throughput that rivals centralized systems. Off-chain scaling solutions only use the blockchain as a secure asset and settlement platform, while moving nearly all transactions to upper layers.\r\n\r\n#### 2.1.1 On-chain Scaling with a Single Blockchain\r\n\r\nThe most straightforward way to increase the throughput of a blockchain is to increase its supply of block space. With additional block space, more transactions can flow through the network and be processed. Increasing the supply of block space in response to increased transaction demand also allows for transaction fees to remain low.\r\n\r\nBitcoin Cash (BCH) adopts this approach to scale its peer-to-peer payment network. The Bitcoin Cash protocol began with a maximum block size of 8 MB, which was later increased to 32 MB, and which will continue to be increased indefinitely as transaction demand increases. For reference, following Bitcoin's (BTC) implementation of Segregated Witness in August 2017, the Bitcoin protocol now allows for an average block size of around 2 MB.\r\n\r\nIn the scope of a datacenter, the math works out. If 7.5 billion people each create 2 on-chain transactions per day, the network will require production of 26 GB blocks every 10 minutes, leading to a blockchain growth rate of 3.75 TB per day or 1.37 PB per year[3]. These storage and bandwidth requirements are reasonable for any cloud service today.\r\n\r\nHowever, constraining node operation to a datacenter environment leads to a single viable network topology and forces compromises in security (the fork rate of the blockchain will increase as data transmission requirements across the network increase), as well as decentralization (the full node count will be reduced as the cost of consensus participation increases).\r\n\r\nFrom an economic standpoint, an ever-increasing block size does alleviate fee pressure felt by users. Analysis of the Bitcoin network has shown that fees remain flat until a block is about 80% full, and then rise exponentially[4].\r\n\r\nThough placing the burden of a growing network's costs on its operators may seem to be a reasonable decision, it could be short-sighted for two reasons: \r\n\r\n- Suppression of transaction fees forces miners to rely predominantly on compensation from new coin issuance (block rewards). Unless inflation is a permanent part of the protocol, new coin issuance will eventually stop (when the total coin hard-cap is reached), and miners will receive neither block rewards nor significant transaction fees. The economic impact of this will severely compromise the security model of the network.\r\n- The cost of running a full node becomes prohibitively expensive. This removes the ability of regular users to independently verify a blockchain's history and transactions, forcing reliance on service providers such as exchanges and payment processors to ensure the integrity of the blockchain. This trust requirement negates the core value proposition of public permissionless blockchains as peer-to-peer, trustless distributed systems. \r\n\r\nTransaction cost optimized platforms such as Bitcoin Cash face significant competition from other blockchains (permissioned and permissionless), as well as traditional payment systems. Design decisions that improve security or censorship resistance will incur associated costs and in turn increase the cost of using the platform. Taking into account a competitive landscape, as well as the network's stated objectives, it is likely that lower costs will be the overarching goal of the network, at the expense of any other considerations.\r\n\r\nThis goal is consistent with our observations of transactional network usage. Users of these systems are indifferent to significant long-run trade-offs because they will only utilize the network for a short time. Once their goods or services have been received and their payment has been settled, these users no longer have any concern for the network's effective operation. The acceptance of these trade-offs is apparent in the widespread use of centralized crypto-asset exchanges, as well as more centralized blockchains. These systems are popular primarily for their convenience and transactional efficiency.\r\n\r\nSome smart contract platforms have taken similar approaches to scaling blockchain throughput, allowing only a limited set of \"super computer\" validators to participate in the consensus process and independently validate the blockchain.\r\n\r\nThough compromises in regard to decentralization and network security allow for cheaper transactions and may be convenient for a set of users, the compromised long-term security model, cost barrier to independently verify transactions, and the likely concentration and entrenchment of node operators lead us to believe that this is not a proper approach for scaling public blockchains.\r\n\r\n#### 2.1.2 On-chain Scaling through Multiple Chains\r\n\r\nOn-chain scaling through multiple chains can be accomplished through sharding, as seen in Ethereum 2.0, or application chains, as seen in Polkadot. These designs effectively partition the global state and transactions of the network into multiple chains, allowing each chain to quickly reach local consensus, and later the entirety of the network to reach global consensus through the consensus of the \"Beacon Chain\" or the \"Relay Chain\".\r\n\r\nThese designs allow the multiple chains to utilize a shared security model, while allowing high throughput and fast transactions inside shards (Ethereum) or para-chains (Polkadot). Though each of these systems is a network of interconnected blockchains, they differ in regard to the protocols running on each chain. In Ethereum 2.0, every shard runs the same protocol, while in Polkadot, each para-chain can run a customized protocol, created through the Substrate framework.\r\n\r\nIn these multi-chain architectures, each dApp (or instance of a dApp) only resides on a single chain. Though developers today are accustomed to the ability to build dApps that seamlessly interact with any other dApp on the blockchain, design patterns will need to adapt to new multi-chain architectures. If a dApp is split across different shards, mechanisms will be required to keep state synced across different instances of the dApp (residing on different shards). Additionally, though layer 2 mechanisms can be deployed for fast cross-shard communication, cross-shard transactions will require global consensus and introduce confirmation latency. \r\n\r\nWith these asynchronous transactions, the infamous \"train-and-hotel\" problem arises. When two transactions must be atomic (for example booking a train ticket and a hotel room on two different shards), new solutions are required. Ethereum introduces contract \"yanking\", in which a dependent contract is deleted on one shard, created on a second shard (that contains the other dependent contract), and both transactions are then executed on the second shard. However, the yanked contract would then be unavailable on the original shard, introducing usability issues, and again requiring new design patterns.\r\n\r\nSharding has its own advantages and challenges. If shards can be truly independent and cross-shard needs are minimal, a blockchain can linearly scale its throughput by increasing the number of shards. This is best suited for self-contained applications that don't require outside state or collaboration with other applications. \r\n\r\nA sharded architecture can be problematic for applications that are developed by composing together \"building block\" applications (this is known as the \"composability problem\"). Composability is especially relevant in the decentralized finance (DeFi) space, where more advanced products tend to be built on top of other building block products. \r\n\r\nOn a more technical note, sharding typically requires a \"1 + N\" topology, in which N chains connect to one meta-chain, introducing an upper bound on the number of shards a meta-chain can support without itself running into scalability issues.\r\n\r\nWe observe significant value in a unified global state, allowing an ecosystem of interdependent applications to emerge and developers to innovate at the edges, similar to web developers' use of libraries for lower-level concerns and open APIs for service integration. A much simpler development experience is enabled when developers don't have to consider synchronicity (in cross-shard asset transfer or messaging passing), as well as a superior user experience, resulting from consistency in the architectural concerns of blockchain interactions.\r\n\r\nWe recognize that sharding is a promising scalability solution (in particular for less interdependent applications), however we believe it is beneficial to have a design that concentrates the most valuable state on a single blockchain, allowing composability. With this design, off-chain scaling approaches are utilized to allow for higher throughput.  \r\n\r\n#### 2.1.3 Off-chain Scaling through Layer 2\r\n\r\nIn layer 2 protocols, the base layer blockchain acts as a settlement (or commitment) layer, while a second layer network routes cryptographic proofs that allow participants to \"take delivery of\" the cryptocurrency. All activities of the second layer are cryptographically secured by the underlying blockchain and the base layer is only used to settle amounts entering/exiting the second layer network, and for dispute resolution. These designs operate without delegation of custody (or risk of loss) of funds and enable instant, nearly free transactions. \r\n\r\nThese technologies demonstrate how a store of value network such as Bitcoin could be used for everyday payments. The most typical example of a layer 2 solution in practice is a payment channel between a customer and a coffee shop. Let's assume Alice visits the Bitcoin Coffee Shop every morning. At the beginning of the month, she deposits funds into a Lightning payment channel she has opened with the coffee shop. As she visits each day, she cryptographically signs the coffee shop's right to take some of the funds, in exchange for her coffee. These transactions happen instantly and are completely peer-to-peer, \"off-chain\", allowing for a smooth customer experience. The Lightning channel is trustless, Alice or the coffee shop can close the channel at any time, taking the funds they are owed at that time. \r\n\r\nPayment channel technologies such as Lightning are only one example of an off-chain scaling technique; there are many maturing technologies that can safely scale blockchain throughput in this way. While payment channels include off-chain agreements to channel balances between two parties, state channels include off-chain agreements to arbitrary state between channel participants. This generalization can be the basis of scalable, trustless, decentralized applications. A single state channel can even be utilized by multiple applications, allowing for even greater efficiency. When one party is ready to exit the channel, they can submit the agreed upon cryptographic proof to the blockchain, which will then execute the agreed state transitions.\r\n\r\nA side-chain is another construction that allows for increased throughput, though via trusted third party blockchain operators. With a two-way peg to a blockchain with reliable, trustless consensus, funds can be moved back and forth between the main-chain and side-chain. This allows for a high volume of trusted transactions on the side-chain, with later net settlement on the main-chain. Side-chain transactions have minimal fees, fast confirmation and high throughput. Though side-chains offer a superior experience in some regard, they do compromise on security. There is however, a great deal of research into trustless side-chains, which can provide the same performance improvements without compromising security.\r\n\r\nAn example of a trustless side-chain technology is Plasma (covered in 5.4), a side-chain architecture that leverages a trust root on a blockchain with broad global consensus. Plasma chains offer the same performance improvements as centralized side-chains, however do so while offering security guarantees. In the event a Plasma chain operator is malicious or malfunctioning, users are provided a mechanism that allows them to safely withdraw their side-chain assets to the main-chain. This is done without the cooperation of the Plasma chain operator, offering users the convenience of side-chain transactions, as well as the security of a layer 1 blockchain.\r\n\r\nOff-chain scaling allows for decentralization, security and scalability. By moving everything except settlement transactions and disputes off-chain, a public blockchain's limited global consensus is efficiently utilized. Diverse layer 2 protocols can be implemented based on application requirements, affording flexibility to developers and users. As more participants are added to the network, performance is not impacted and all parties can share the security guarantees offered by layer 1 consensus.\r\n\r\n### 2.2 Sustainability\r\n\r\nSustaining the long-term operation of an autonomous, ownerless public blockchain presents quite the challenge. Incentives must be balanced among diverse stakeholders and the system must be designed in a way that allows for widespread full node operation and public verifiability. Hardware requirements must remain reasonable, while supporting an open, global network.\r\n\r\nAdditionally, once a public blockchain is in operation, it is very difficult to change the underlying rules governing the protocol. From the start, the system must be designed to be sustainable. In this interest, we have conducted a thorough inventory of the challenges in building sustainable, permissionless blockchains.\r\n\r\n#### 2.2.1 Decentralization\r\n\r\nOne of the largest long-term threats public blockchains face is an ever-increasing barrier of independent participation and transaction verification, reflected in the cost of full node operation. Full nodes allow blockchain participants to independently verify the on-chain state/history, and hold miners or validators of the network accountable by refusing to route invalid blocks. As the cost of full nodes increases and their numbers decline, participants in the network are increasingly forced to rely on professional service operators to provide both history and current state, eroding the fundamental trust model of open and permissionless blockchains. \r\n\r\nFor a full node to keep up with the progression of the blockchain, it must have adequate computational throughput to validate transactions, bandwidth throughput to receive transactions, and storage capacity to store the entire global state. To control a full node's operating cost, the protocol has to take measures to bound the throughput or capacity growth of all three of these resources. Most blockchain protocols bound their computational or bandwidth throughput, but very few bound the growth of the global state. As these chains grow in size and length of operation, full node operation costs will irreversibly increase.\r\n\r\n#### 2.2.2 Economic Models\r\n\r\nWhile there has been a lot of research into consensus protocols in recent years, we believe crypto-economics is an understudied field. Broadly speaking, current crypto-economic models for layer 1 protocols are primarily focused on incentives and punishments to ensure network consensus, and native tokens are mostly used to pay transaction fees or to satisfy staking requirements that provide Sybil resistance.\r\n\r\nWe believe that a well-designed economic model should go beyond the consensus process and ensure the long-term sustainability of the protocol as well. In particular, the economic model should be designed with the following goals:\r\n\r\n- the network should have a sustainable way to compensate service providers (typically miners or validators), ensuring that the network remains sustainably secure\r\n- the network should have a sustainable way to maintain a low barrier to participation, ensuring that the network remains decentralized over time\r\n- the resources of the public network should be efficiently and fairly allocated\r\n- the blockchain's native token must have intrinsic value\r\n\r\n#### 2.2.3 Analysis of Bitcoin's Economic Model\r\n\r\nThe Bitcoin protocol caps the size of blocks and enforces a fixed block time. This makes the network's bandwidth throughput a scarce resource that users must bid on through transaction fees. Bitcoin Script doesn't allow loops, making the length of the script a good approximation of its computational complexity. In general, greater demand for block space translates into higher transaction fees for users. Additionally, the more inputs, outputs or computational steps that are involved in a transaction, the more a user will also pay in transaction fees.\r\n\r\nThe intrinsic value of Bitcoin comes almost entirely from its monetary premium (society's willingness to treat it as money) and in particular, the willingness to hold it as a store of value. Because miner income is denominated in BTC, this perception has to hold for Bitcoin's economic model to be sustainable. In other words, Bitcoin's security model is circular - it depends on the collective belief that the network is sustainably secure and can therefore be used as a monetary store of value.\r\n\r\nBitcoin's block size cap effectively sets the barrier for network participation - the lower the block size cap is, the easier it is for non-professionals to run full nodes. The Bitcoin global state is its UTXO set, with its growth rate also effectively capped by the block size limit. Users are incentivized to create and utilize UTXOs efficiently; creating more UTXO's translates into higher transaction fees. However, no incentives are provided to encourage combining of UTXOs and reduction of the size of the global state; once a UTXO is created, it will occupy the global state for free until it is spent.\r\n\r\nBitcoin's transaction fee-based economic model is a fair model to allocate its bandwidth throughput, the scarce resource imposed by the protocol. It's a suitable economic model for a peer-to-peer payment system, but is a poor choice for a true store of value platform. Bitcoin users that utilize the blockchain to store value pay transaction fees only once, but can then occupy state forever, enjoying ongoing security provided by miners, who are required to make continuous resource investments.\r\n\r\nBitcoin has a total supply hard-cap and its new issuance via block rewards will eventually drop to zero. This could cause two problems:\r\n\r\nFirst, if Bitcoin continues to succeed as a store of value, the unit value of BTC will continue to increase, and the total value the network secures will also increase (as more monetary value moves on to the network). A store of value platform has to be able to raise its security budget as the value it protects increases over time, otherwise, it invites attackers to double spend and steal the assets of the network. \r\n\r\nWhen the cost to break protocol security is less than the profit they can earn acting honestly, attackers will always attack. This is analogous to a city that has to raise its military spending as the wealth inside the city increases. Without this investment, sooner or later the city will be attacked and looted. \r\n\r\nWith the existence of block rewards, Bitcoin is able to scale security to the aggregate value it stores - if Bitcoin's price doubles, the income that miners receive from block rewards will also double, therefore they can afford to produce twice the hash rate, making the network twice as expensive to attack. \r\n\r\nThis however changes when the predictable block rewards drop to zero. Miners will have to rely entirely on transaction fees; their income will no longer scale to the value of the Bitcoin asset, but will be determined by the transaction demand of the network. If transaction demand is not high enough to fill the available block space, total transaction fees will be minuscule. Since transaction fees are strictly a function of block space demand and independent from the price of a Bitcoin, this will have a profound impact on Bitcoin's security model. For Bitcoin to remain secure, we'd have to assume consistent, over-capacity transaction demand, that also scales to the price of Bitcoin. These are very strong assumptions. \r\n\r\nSecond, when the predictable block rewards stop, variance in per block income for miners increases, and provides incentives for miners to fork, instead of advancing the blockchain. In the extreme case, when a miner's mempool is empty and they receive a block loaded with fees, their incentive is to fork the chain and steal the fees, as opposed to advancing the chain and producing a block with potentially no income[5]. This is known as the \"fee sniping\" challenge in the Bitcoin community, to which a satisfying solution has not yet been found, without removing Bitcoin's hard-cap.\r\n\r\n#### 2.2.4 Analysis of the Economic Model of Smart Contract Platforms\r\n\r\nThe typical economic model of smart contract platforms faces even more challenges. Let's use Ethereum as an example. Ethereum's scripting allows loops, therefore the length of a script doesn't reflect the script's computational complexity. This is the reason Ethereum doesn't cap block size or bandwidth throughput, but computational throughput (expressed in the block gas limit).\r\n\r\nTo get their transactions recorded on the Ethereum blockchain, users bid on the per computation cost they're willing to pay in transaction fees. Ethereum uses the concept of \"gas\" as measurement of computational cost priced in ETH, and the \"gas price\" rate control ensures that the cost per step of computation is independent of price movements of the native token. The intrinsic value of the ETH token comes from its position as the payment token of the decentralized computation platform; it is the only currency that can be used to pay for computation on Ethereum.\r\n\r\nEthereum's global state is represented with the EVM's state trie, the data structure that contains the balances and internal state of all accounts. When new accounts or contract values are created, the size of the global state expands. Ethereum charges fixed amounts of gas for insertion of new values into its state storage and offers a fixed \"gas stipend\" that offsets a transaction's gas costs when values are removed. \r\n\r\nA \"pay once, occupy forever\" storage model doesn't match the ongoing cost structure of miners and full nodes, and the model provides no incentive for users to voluntarily remove state or remove state sooner. As a result, Ethereum has experienced rapid growth of its state size. A larger state size slows down transaction processing and raises the operating cost of full nodes. Without strong incentives to clear state, this is a trend that's bound to continue.\r\n\r\nSimilar to Bitcoin, Ethereum's demand-driven gas pricing is a fair model to allocate its computational throughput, the platform's scarce resource. The model also serves Ethereum's purpose as a decentralized computation system. However, its state storage fee model doesn't match its potential proposition as a decentralized state or asset storage platform. Without a cost for long-term state storage, it will always be in users' interests to occupy state forever for free. Without scarcity of state storage capacity, neither a market, nor supply and demand dynamics can be established. \r\n\r\nUnlike Bitcoin, which specifies the block size limit in its core protocol, Ethereum allows miners to dynamically adjust the block gas limit when they produce blocks. Miners with advanced hardware and significant bandwidth are able to produce more blocks, effectively dominating this voting process. Their interest is to adjust the block gas limit upward, raise the bar of participation and force smaller miners out of the competition. This is another factor that contributes to the quickly rising cost of full node operation.\r\n\r\nSmart contract platforms like Ethereum are multi-asset platforms. They support issuance and transactions of all types of crypto-assets, typically represented as \"tokens\". They also provide security to not only their own native tokens, but the value of all crypto-assets on the platform. \"Store of value\" in a multi-asset context therefore refers to the value preservation property that benefits both the platform's native tokens and the crypto-assets stored on the platform.\r\n\r\nWith its block rewards, Bitcoin has an excellent \"store of value\" economic model. Miners are paid a fixed block reward denominated in BTC, and thus their income rises along with the price of BTC. Therefore, the platform has the ability to raise revenue for miners to increase security (measured by the cost of attack) while maintaining a sustainable economic model.\r\n\r\nFor multi-asset platforms, it becomes much more challenging to fulfill this requirement, because \"value\" can be expressed with crypto-assets beyond the native token. If the value of crypto-assets secured by the platform increases, but network security doesn't, it becomes more profitable to attack the platform's consensus process to double spend crypto-assets stored on the platform.\r\n\r\nFor a multi-asset smart contract platform to function as a store of value, proper incentives must be put in place to align in the growth in value of a network's assets with its underlying security. Or put another way, the platform's native token must be a good value capture of the platform's aggregate asset value. If the intrinsic value of a platform's native token is limited to transaction fee payment, its value would be determined solely by transaction demand, instead of the demand of asset storage.\r\n\r\nSmart contract platforms that are not designed to function as a store of value have to rely on the native token's monetary premium (the willingness of people to hold the tokens beyond their intrinsic value) to support its ongoing security. This is only feasible if one platform dominates with unique features that can't be found elsewhere, or out-competes others by delivering the lowest possible cost of transactions.\r\n\r\nEthereum currently enjoys such dominance and can therefore maintain its monetary premium. However, with the rise of competing platforms, many designed for higher TPS and providing similar functionality, it's an open question as to whether reliance on a monetary premium alone can sustain a blockchain platform's security, especially if the native tokens are explicitly not designed or believed to be money. Furthermore, even if a platform can provide unique features, its monetary premium can be abstracted away by the user interface through efficient swaps (very likely when mass adoption of blockchain finally comes). Users would hold assets they're most familiar with, such as Bitcoin or stable coins, and acquire platform tokens just in time to pay for transaction fees. In either case, the foundation of a platform's crypto-economics would collapse.\r\n\r\nLayer 1 multi-asset platforms have to provide sustainable security for all of the crypto-assets they secure. In other words, they have to have an economic model designed for a store of value.\r\n\r\n#### 2.2.5 Funding of Core Protocol Development\r\n\r\nPublic permissionless blockchains are public infrastructure. Initial development of these systems requires a great deal of funding, and once they are in operation require ongoing maintenance and upgrades. Without dedicated people maintaining these systems, they run the risk of catastrophic bugs and sub-optimal operation. The Bitcoin and Ethereum protocols do not provide a native mechanism to ensure funding of ongoing development, thus rely on the continued engagement of businesses with aligned interests and altruistic open source communities. \r\n\r\nDash was the first project to utilize a treasury to ensure ongoing development was funded in-protocol. While sustainably supporting the protocol's development, this design makes a compromise in regard to the sustainability of the value of the cryptocurrency. Like most blockchain treasuries, this model relies on inflation-based funding, which erodes the value of long-term holdings. \r\n\r\nThe Nervos Network uses a treasury model that provides sustainable funding for core development. Treasury funds come from targeted inflation of short-term token holders, while the effects of this inflation are mitigated for long-term holders. More information about this mechanism is described in (4.6).\r\n\r\n### 2.3 Interoperability\r\n\r\nInteroperability across blockchains is an often-discussed topic, and many projects have been proposed specifically to address this challenge. With reliable transactions across blockchains, true network effects can be realized in the decentralized economy. \r\n\r\nThe first example of blockchain interoperability was atomic swaps between Bitcoin and Litecoin. The trustless exchange of Bitcoin for Litecoin and vice-versa is made possible not through in-protocol mechanisms, but through a shared cryptographic standard (specifically usage of the SHA2-256 hash function).\r\n\r\nSimilarly, the design of Ethereum 2.0 allows for interconnection of many shard chains, all running the same protocol and utilizing the same cryptographic primitives. This uniformity will be valuable when customizing the protocol for inter-shard communication, however Ethereum 2.0 will not be interoperable with other blockchains that do not utilize the same cryptographic primitives.\r\n\r\nNetworks of blockchains such as Polkadot or Cosmos go one-step further, allowing blockchains built with the same framework (Cosmos SDK for Cosmos and Substrate for Polkadot) to communicate and interact with one another. These frameworks provide developers some flexibility in building their own protocols, and ensure the availability of identical cryptographic primitives, allowing each chain to parse one another's blocks and cross-validate transactions. However, both protocols rely on bridges or \"pegging zones\" to connect to blockchains that are not constructed with their own frameworks, introducing an additional layer of trust. To demonstrate: though Cosmos and Polkadot enable \"networks of blockchains\", the Cosmos and Polkadot networks are not designed to be interoperable with each other. \r\n\r\nThe crypto-economics of cross-chain networks may need further study as well. For both Cosmos and Polkadot, native tokens are used for staking, governance and transaction fees. Putting aside the crypto-economic dynamics introduced by staking, which can't alone give a native token intrinsic value (discussed in 4.2.4), reliance on cross-chain transactions to capture ecosystem value can be a weak model. In particular, cross-chain transactions are a weakness, not a strength of multi-chain networks, just as cross-shard transactions are a weakness of sharded databases. They introduce latency, as well as the loss of atomicity and composability. There is a natural tendency for applications that need to interact with each other to eventually move to reside on the same blockchain to reduce cross-chain overhead, reducing the demand for cross-chain transactions and therefore demand for the native token.\r\n\r\nCross-chain networks benefit from network effects - the more interconnected chains there are in a network, the more valuable the network is, and the more attractive it is to potential new participants in the network. Ideally, such value would be captured by the native token and used to further encourage the growth of the network. However, in a pooled security network such as Polkadot, higher cost of network participation becomes a deterrent for the network to accrue further value. In a loosely connected network like Cosmos, if we assume same cross-chain transaction demand and fees, higher cost of staking participation lowers the expected return for validators, discouraging further staking participation.\r\n\r\nWith its layered approach, the Nervos Network is also a multi-chain network. Architecturally, Nervos uses the cell model and a low-level virtual machine to support true customization and user-created cryptographic primitives, enabling interoperability across heterogeneous blockchains (covered in 4.4.1). Crypto-economically, the Nervos Network concentrates value (instead of message passing) to its root chain. This mechanism raises the network's security budget as the aggregate value secured by the network rises. This is covered in detail in (4.4).\r\n\r\n## 3. Core Principles of the Nervos Network\r\n\r\nNervos is a layered network built to support the needs of the decentralized economy. There are several reasons that we believe a layered approach is the right way to build a blockchain network. There are many well known trade-offs in building blockchain systems, such as decentralization vs. scalability, neutral vs. compliant, privacy vs. openness, store of value vs. transaction cost and cryptographic soundness vs. user experience. We believe that all of these conflicts arise because of attempts to address completely opposing concerns with a single blockchain. \r\n\r\nWe believe that the best way to construct a system is not to build an all-encompassing single layer, but rather to decouple concerns and address them at different layers. By doing this, the layer 1 blockchain can focus on being secure, neutral, decentralized and open public infrastructure, while smaller, layer 2 networks can be specially-designed to best suit the context of their usage.\r\n\r\nIn the Nervos Network, the layer 1 protocol (the Common Knowledge Base) is the value preservation layer of the entire network. It is philosophically inspired by Bitcoin and is an open, public and proof of work-based blockchain, designed to be maximally secure and censorship-resistant, to serve as a decentralized custodian of value and crypto-assets. Layer 2 protocols leverage the security of the layer 1 blockchain to provide unbounded scalability and minimal transaction fees, and also allow for application-specific trade-offs in regard to trust models, privacy and finality.\r\n\r\nHere are the core principles that led to the design of the Nervos Network:\r\n\r\n- A sustainable, multi-asset layer 1 blockchain has to be crypto-economically designed to be a store of value.\r\n- Layer 2 offers the best scaling options, bringing nearly unlimited transactional capabilities, minimal transaction costs and an improved user experience. Layer 1 blockchains should be designed to complement, not compete with layer 2 solutions.\r\n- Proof of Work as a Sybil resistance method is essential for layer 1 blockchains.\r\n- The layer 1 blockchain must provide a generic programming model for interactive protocols and blockchain interoperability, and to allow the protocol to be maximally customizable and easy to upgrade.\r\n- To best allocate resources and avoid the \"tragedy of the commons\", state storage has to have a clear and fine-grained ownership model. To deliver consistent long-term rewards to miners (regardless of transaction demand), state occupation must have an ongoing cost.\r\n\r\n## 4. The Nervos Common Knowledge Base\r\n\r\n### 4.1 Overview\r\n\r\n\"Common knowledge\" is defined as knowledge that is known by everyone or nearly everyone, usually with reference to the community in which the term is used. In the context of blockchains in general, and the Nervos Network in particular, \"common knowledge\" refers to state verified by global consensus and accepted by all in the network.\r\n\r\nThe properties of common knowledge allow us to collectively treat the cryptocurrency stored on public blockchains as money. For example, the balances and history of all addresses on Bitcoin are common knowledge for Bitcoin users, because they are able to independently replicate the shared ledger, verify the global state since the genesis block, and know that anyone else can do the same. This common knowledge allows people to transact completely peer-to-peer without putting trust in any third party.\r\n\r\nThe Nervos Common Knowledge Base (CKB) is designed to store all kinds of common knowledge, not limited to money. For example, the CKB could store user-defined crypto-assets, such as fungible and non-fungible tokens, as well as valuable cryptographic proofs that provide security for higher-layer protocols, such as payment channels (5.2) and commit chains (5.4).\r\n\r\nBoth Bitcoin and the Nervos CKB are common knowledge storage and verification systems. Bitcoin stores its global state as the UTXO set, and verifies state transitions through hard-coded rules and scripts embedded in transactions. The Nervos CKB generalizes Bitcoin's data structure and scripting capabilities, stores global state as the set of active programmable cells, and verifies state transitions through user-defined, Turing-complete scripts that run in a virtual machine.\r\n\r\nWhile the Nervos CKB has full smart contract capabilities like those of Ethereum and other platforms, its economic model is designed for common knowledge preservation, instead of payment for decentralized computation.\r\n\r\n### 4.2 Consensus\r\n\r\nBitcoin's Nakamoto Consensus (NC) is well-received due to its simplicity and low communication overhead. However, NC suffers from two drawbacks: 1) its transaction processing throughput is far from satisfactory, and 2) it is vulnerable to selfish mining attacks, in which attackers can gain additional block rewards by deviating from the protocol's prescribed behavior.\r\n\r\nThe CKB consensus protocol is a variant of NC that raises its performance limit and selfish mining resistance while keeping its merits. By identifying and eliminating the bottleneck in NC's block propagation latency, our protocol supports very short block intervals without sacrificing security. A shortened block interval not only increases throughput, but also lowers transaction confirmation latency. By incorporating all valid blocks into the difficulty adjustment calculation, selfish mining is no longer profitable in our protocol.\r\n\r\n#### 4.2.1 Increasing Throughput\r\n\r\nNervos CKB increases the throughput of PoW consensus with a consensus algorithm derived from Nakamoto Consensus. The algorithm uses the blockchain's orphan rate (the percentage of valid blocks that are not part of the canonical chain) as a measurement of connectivity across the network.\r\n\r\nThe protocol targets a fixed orphan rate. In response to a low orphan rate target difficulty is lowered (increasing the rate of block production) and when the orphan rate crosses a defined threshold, target difficulty is increased (decreasing the rate of block production).\r\n\r\nThis allows for utilization of the network's entire bandwidth capabilities. A low orphan rate indicates that the network is well-connected and can handle greater data transmission; the protocol then increases throughput under these conditions.\r\n\r\n#### 4.2.2 Eliminating the Block Propagation Bottleneck\r\n\r\nThe bottleneck in any blockchain network is block propagation. The Nervos CKB consensus protocol eliminates the block propagation bottleneck by modifying transaction confirmation into a two step process: 1) propose and 2) commit.\r\n\r\nA transaction must first be proposed in the \"proposal zone\" of a block (or one of its uncles). The transaction will then be committed if it appears in a block's \"commitment zone\" within a defined window following its proposal. This design eliminates the block propagation bottleneck, as a new block's committed transactions will have already been received and verified by all nodes when proposed.\r\n\r\n#### 4.2.3 Mitigating Selfish Mining Attacks\r\n\r\nOne of the most fundamental attacks on Nakamoto Consensus is selfish mining. In this attack, malicious miners gain unfair block rewards by deliberately orphaning blocks mined by others.\r\n\r\nResearchers observe that the unfair profit opportunity is rooted in the difficulty adjustment mechanism of Nakamoto Consensus, which neglects orphaned blocks when estimating the network's computing power. This leads to lower mining difficulty and higher time-averaged block rewards.\r\n\r\nThe Nervos CKB consensus protocol incorporates uncle blocks into the difficulty adjustment calculation, making selfish mining no longer profitable. This holds regardless of attack strategy or duration; a miner is unable to gain unfair rewards through any combination of honest and selfish mining.\r\n\r\nOur analysis shows that with a two-step transaction confirmation process, de facto selfish mining is also eliminated via a limited attack time window.\r\n\r\nFor a detailed understanding of our consensus protocol, please read [here](https://github.com/nervosnetwork/rfcs/blob/master/rfcs/0020-ckb-consensus-protocol/0020-ckb-consensus-protocol.md).\r\n\r\n#### 4.2.4 Proof of Work vs Proof of Stake\r\n\r\nProof of Work (PoW) and Proof of Stake (PoS) systems are both vulnerable to concentrations of power, however the qualities of the systems provide very different operating realities for those in power.\r\n\r\nPoW mining incurs real-world expenses that can exceed mining proceeds without diligent cost supervision. Those in power are required to stay innovative, pursue sound business strategies and continue to invest in infrastructure to remain dominant. Mining equipment, mining pool operations and access to cheap energy are all subject to changes from technological innovation. It is difficult to maintain monopolization of all three over long periods of time.\r\n\r\nIn contrast, block creators in PoS systems are rewarded in a deterministic way, based on amount staked, with very low operational capital requirements. As the system grows, the impact of natural advantages provided to first moving businesses and individuals will grow. In a PoS system, it is possible that power concentrates in the hands of a few stakers. Though PoW systems have a similar problem with mining concentration, the cost to remain in power in a PoS system is significantly lower.\r\n\r\nIn addition, PoS validators have one unique power: control of the validator set. Acceptance of a transaction that allows a validator to join the consensus group is in the hands of existing validators. Colluding efforts to influence the validator set through transaction censorship and ordering manipulation would be difficult to detect, as well as difficult to punish. Conversely, consensus participation in PoW systems is truly open and isn't subject to the current power structure. Advantages are not given to early participants of the system.\r\n\r\nRegarding token economics, while it is believed that staking can attract capital looking to earn yield (and therefore increase demand for the native token), this is not the whole picture. All PoS projects will eventually see their staking rate stabilize, and capital entering and leaving the pool of staked capital would then be roughly the same. The staking mechanism by itself will not increase demand for the native token. In other words, though the introduction of staking provides demand for the native token in the initial phase of a project (as the staking rate rises), staking alone can't provide long-term demand for the native token and therefore can't be a native token's only intrinsic value.\r\n\r\nLong-term token holders in a PoS system have 3 options: they can 1) manage infrastructure and run a validating node on their own to receive new issuance, 2) delegate their tokens to a third party and trust their integrity and infrastructure, or 3) have the value of their tokens diluted by ongoing issuance. None of these options are particularly attractive to long-term, store of value oriented token holders.\r\n\r\nWe believe that PoW's permissionless participation is a requirement for infrastructure at the foundation of global economic activity. The foremost goal of layer 1 is to ensure that the blockchain is as decentralized, secure and neutral as possible. While PoS systems have a role to play in the decentralized economy, in our opinion they do not meet the requirements of a truly open and decentralized layer 1.\r\n\r\n#### 4.2.5 Proof of Work Function\r\n\r\nNervos CKB blocks can be proposed by any node, provided that 1) the block is valid; and 2) the proposer has solved a computationally difficult puzzle called the proof-of-work. The proof-of-work puzzle is defined in terms of the block that is being proposed; this guarantees that the solution to the puzzle uniquely identifies a block.\r\n\r\nBitcoin's proof-of-work requires finding a valid nonce such that the result of applying a hash function on the block header satisfies a certain level of difficulty. For Bitcoin, the hash function is twice-iterated SHA2–256. While SHA2 was a good choice for Bitcoin, the same is not true for cryptocurrencies that come after it. A large amount of dedicated hardware has been developed to mine Bitcoin, a great deal of which sits idle, having been rendered obsolete by efficiency improvements. \r\n\r\nA new cryptocurrency utilizing the same proof-of-work puzzle would make this deprecated hardware useful once again. Even up-to-date hardware can be rented and re-purposed to mine a new coin. The distribution of mining power for a SHA2-based coin would be very difficult to predict and susceptible to sudden and large changes. This argument also applies to algorithmic optimizations tailored to SHA2, which have been developed to make software computation of the function cheaper as well.\r\n\r\nFor a new cryptocurrency, it makes sense to define the proof-of-work puzzle in terms of a function that has not yet been used by other cryptocurrencies. For Nervos CKB, we went a step further and chose to define it in terms of a proof-of-work function that could not have been the subject of premature optimization, because it is new.\r\n\r\nHowever, the intended unavailability of mining hardware is only the case initially. In the long run, deployments of dedicated mining hardware are beneficial, significantly increasing the challenges of attacking the network. Therefore, in addition to being new, an ideal proof-of-work function for a new cryptocurrency is also simple, significantly lowering the barrier for hardware development.\r\n\r\nSecurity is the obvious third design goal. While a known vulnerability could be exploited by all miners equally, and would merely result in a higher difficulty, an undisclosed vulnerability could lead to a mining optimization that provides the discoverer(s) an advantage in excess of their contributed mining power share. The best way to avoid this situation is to make a strong argument for invulnerability.\r\n\r\n#### 4.2.6 Eaglesong\r\n\r\nEaglesong is a new hash function developed specifically for Nervos CKB proof-of-work, but is also suitable in other use cases in which a secure hash function is needed. The design criteria were exactly as listed above: novelty, simplicity and security. We wanted a design that was simultaneously novel enough to constitute a small step forward for science, as well as close enough to existing designs to make a strong security argument. \r\n\r\nTo this end, we chose to instantiate the sponge construction (as used in Keccak/SHA3) with a permutation built from ARX operations (addition, rotation, and xor); the argument for its security is based on the wide trail strategy (the same argument underlying AES).\r\n\r\nTo the best of our knowledge, Eaglesong is the first hash function (or function, for that matter) that successfully combines all three design principles.\r\n\r\nYou can read more about Eaglesong [here](https://medium.com/nervosnetwork/the-proof-of-work-function-of-nervos-ckb-3cc8364464d9).\r\n\r\n\r\n### 4.3 Cell Model\r\n\r\nNervos CKB utilizes the Cell Model, a new construction that can provide many of the benefits of the Account model (utilized in Ethereum), while preserving the asset ownership and proof-based verification properties of the UTXO model (utilized in Bitcoin).\r\n\r\nThe cell model is focused on state. Cells contain arbitrary data, which could be simple, such as a token amount and an owner, or more complex, such as code specifying verification conditions for a token transfer. The CKB's state machine executes scripts associated with cells to ensure the integrity of a state transition.\r\n\r\nIn addition to storing data of their own, cells can reference data in other cells. This allows for user-owned assets and the logic governing them to be separated. This is in contrast to account-based smart contract platforms, in which state is internal property of a smart contract and has to be accessed through smart contract interfaces. On Nervos CKB, cells are independent state objects that are owned, and can be referenced and passed around directly. Cells can express true \"bearable assets\", belonging to their owners (just as UTXOs are bearable assets to Bitcoin owners), while referencing a cell that holds logic ensuring the integrity of state transitions.\r\n\r\nCell model transactions are also state transition proofs. A transaction's input cells are removed from the set of active cells and output cells are added to the set. Active cells comprise the global state of the Nervos CKB, and are immutable: once cells have been created, they cannot be changed. \r\n\r\nThe Cell model is designed to be adaptable, sustainable, and flexible. It can be described as a generalized UTXO model and can support user-defined tokens, smart contracts and diverse layer 2 protocols.\r\n\r\nFor deeper understanding of the Cell Model, please see [here](https://medium.com/nervosnetwork/https-medium-com-nervosnetwork-cell-model-7323fca57571).\r\n\r\n\r\n### 4.4 Virtual Machine\r\n\r\nWhile many next-generation blockchain projects utilize WebAssembly as the foundation of a blockchain virtual machine, Nervos CKB includes the unique design choice of a virtual machine (CKB-VM) based on the RISC-V instruction set.\r\n\r\nRISC-V is an open-source RISC instruction set architecture that was created in 2010 to facilitate development of new hardware and software, and is a royalty-free, widely understood and widely audited instruction set.\r\n\r\nWe have found numerous advantages to using RISC-V in a blockchain context:\r\n\r\n- Stability: The RISC-V core instruction set has been finalized and frozen, as well as widely implemented and tested. The core RISC-V instruction set is fixed and will never require an update.\r\n- Open and Supported: RISC-V is provided under a BSD license and supported by compilers such as GCC and LLVM, with Rust and Go language implementations under development. The RISC-V Foundation includes more than 235 member organizations furthering the instruction set's development and support.\r\n- Simplicity and Extensibility: The RISC-V instruction set is simple. With support for 64-bit integers, the set contains only 102 instructions. RISC-V also provides a modular mechanism for extended instruction sets, enabling the possibility of vector computing or 256-bit integers for high-performance cryptographic algorithms.\r\n- Accurate Resource Pricing: The RISC-V instruction set can be run on a physical CPU, providing an accurate estimation of the machine cycles required for executing each instruction and informing virtual machine resource pricing.\r\n\r\nCKB-VM is a low-level RISC-V virtual machine that allows for flexible, Turing-complete computation. Through use of the widely implemented ELF format, CKB-VM scripts can be developed with any language that can be compiled to RISC-V instructions.\r\n\r\n#### 4.4.1 CKB-VM and the Cell Model\r\n\r\nOnce deployed, existing public blockchains are more or less fixed. Upgrading foundational elements, such as cryptographic primitives, involve multi-year undertakings or are simply not possible.\r\n\r\nCKB-VM takes a step back, and moves primitives previously built into custom VMs to cells on top of the virtual machine. Though CKB scripts are more low-level than smart contracts in Ethereum, they carry the significant benefit of flexibility, enabling a responsive platform and foundation for the progressing decentralized economy.\r\n\r\nCells can store executable code and reference other cells as dependencies. Almost all algorithms and data structures are implemented as CKB scripts stored within cells. By keeping the VM as simple as possible and offloading program storage to cells, updating key algorithms is as simple as loading the algorithm into a new cell and updating existing references.\r\n\r\n#### 4.4.2 Running Other Virtual Machines on the CKB-VM\r\n\r\nThanks to the low-level nature of the CKB-VM and the availability of tooling in the RISC-V community, it's easy to compile down other VMs (such as Ethereum's EVM) directly into the CKB-VM. This has several advantages: \r\n\r\n- Smart contracts written in specialized languages running on other virtual machines can be easily ported to run on the CKB-VM. (Strictly speaking, they'd be running on their own VM that's compiled to run inside of the CKB-VM.)\r\n- The CKB can verify dispute resolution state transitions of layer 2 transactions, even if the rules of the state transitions are written to run in a virtual machine other than CKB-VM. This is one of the key requirements to support trustless layer 2 general purpose side-chains. \r\n\r\nFor a technical walkthrough of the CKB-VM, please see [here](https://medium.com/nervosnetwork/an-introduction-to-ckb-vm-9d95678a7757).\r\n\r\n### 4.5 Economic Model\r\n\r\nThe native token of the Nervos CKB is the \"Common Knowledge Byte\", or CKByte for short. CKBytes entitle a token holder to occupy part of the total state storage of the blockchain. For example, by holding 1000 CKBytes, a user is able to create a cell of 1000 bytes in capacity or multiple cells adding up to 1000 bytes in capacity. \r\n\r\nUsing CKBytes to store data on the CKB creates an opportunity cost to CKByte owners; they will not be able to deposit occupied CKBytes into the NervosDAO to receive a portion of the secondary issuance. CKBytes are market priced, and thus an economic incentive is provided for users to voluntarily release state storage to meet the high demand of expanding state. After a user releases state storage, they will receive an amount of CKBytes equivalent to the size of state (in bytes) their data was occupying.\r\n\r\nThe economic model of the CKB allows issuance of the native token to bound state growth, maintaining a low barrier of participation and ensuring decentralization. As CKBytes become a scarce resource, they can be priced and allocated most efficiently.\r\n\r\nThe genesis block of the Nervos Network will contain 33.6 billion CKBytes, of which 8.4 billion will be immediately burned. New issuance of CKBytes includes two parts - base issuance and secondary issuance. Base issuance is limited to a finite total supply (33.6 billion CKBytes), with an issuance schedule similar to Bitcoin. The block reward halves approximately every 4 years, until reaching 0 new issuance. All base issuance is awarded to miners as incentives to protect the network. The secondary issuance has a constant issuance rate of 1.344 billion CKBytes per year and is designed to impose an opportunity cost for state storage occupation. After the base issuance stops, there will only be secondary issuance.\r\n\r\nNervos CKB includes a special smart contract called the NervosDAO, which functions as an \"inflation shelter\" against the effects of the secondary issuance. CKByte owners can deposit their tokens into the NervosDAO and receive a portion of secondary issuance that exactly offsets inflationary effects from secondary issuance. For long-term token holders, as long as they lock their tokens in the NervosDAO, the inflationary effect of secondary issuance is only nominal. With the effects of secondary issuance mitigated, these users are effectively holding hard-capped tokens like Bitcoin.\r\n\r\nWhile CKBytes are being used to store state, they cannot be used to earn secondary issuance rewards through the NervosDAO. This makes the secondary issuance a constant inflation tax, or \"state rent\" on state storage occupation. This economic model imposes state storage fees proportional to both the space and time of occupation. It is more sustainable than the \"pay once, occupy forever\" model used by other platforms, and is more feasible and user-friendly than other state rent solutions that require explicit payments.\r\n\r\nMiners are compensated with both block rewards and transaction fees. For block rewards, when a miner mines a block, they would receive the block's full base issuance reward, and a portion of secondary issuance. The portion is based on state occupation, for example: if half of all native tokens are being used to store state, a miner would receive half of the secondary issuance reward for the block. Additional information about the distribution of secondary issuance is included in the next section (4.6). In the long term, when base issuance stops, miners will still receive \"state rent\" income that's independent of transactions, but tied to the adoption of the Nervos Common Knowledge Base.\r\n\r\nIn an analogy, CKBytes can be thought of as land, while crypto-assets stored on the CKB can be thought of as houses. Land is required to build a house, and CKBytes are required to store assets on the CKB. As demand to store assets on CKB rises, demand for CKBytes rises as well. As the value of assets stored rises, the value of CKBytes rises as well.\r\n\r\nThe Nervos CKB is designed to translate demand for a multitude of assets into demand for a single asset, and use it to compensate the miners to secure the network.\r\n\r\nFor more detailed explanation on the economic model, please see [here](https://github.com/nervosnetwork/rfcs/blob/master/rfcs/0015-ckb-cryptoeconomics/0015-ckb-cryptoeconomics.md). \r\n\r\n\r\n### 4.6 Treasury\r\n\r\nThe portion of secondary issuance that doesn't go to 1) miners or 2) long-term holders with tokens locked in the NervosDAO, will go toward a treasury fund. To demonstrate: if 60% of issued CKBytes are used to store state and 30% of the CKBytes are deposited into the NervosDAO, miners will receive 60% of the secondary issuance, the NervosDAO (long-term holders) will receive 30% of the secondary issuance, and 10% of the secondary issuance will go to the treasury. \r\n\r\nThe treasury fund will be used to fund ongoing research and development of the protocol, as well as building the ecosystem of the Nervos Network. The use of the treasury funds will be open, transparent and on-chain for everyone to see. Compared to an inflation-based treasury funding model, this model doesn't dilute long-term token holders (who have deposited their tokens into the NervosDAO). Funding of protocol development is strictly derived from the opportunity cost to short-term token holders.\r\n\r\nThe treasury won't be activated immediately upon the main-net launch of the Nervos Common Knowledge Base. With the community's approval, it will be activated with a hard-fork later, only after the Nervos Foundation has exhausted the Ecosystem Fund, included in the Genesis block. Prior to activation of the treasury, this portion of the secondary issuance will be burned.\r\n\r\n\r\n### 4.7 Governance\r\n\r\nGovernance is how society or groups within it organize to make decisions. Every relevant party with an interest in the system should be involved in this process. In regard to a blockchain, this should include not only users, holders, miners, researchers and developers, but also service providers such as wallets, exchanges and mining pools as well. Various stakeholder groups have diverse interests and it is almost impossible to align everyone's incentives. This is why blockchain governance is a complicated and controversial topic. If we consider a blockchain as a large social experiment, governance requires a more sophisticated design than any other part of the system. After ten years of evolution, we still haven't identified general best practices or sustainable processes for blockchain governance.\r\n\r\nSome projects conduct governance via a \"benevolent dictator for life\" (such as Linus Torvalds to Linux). We acknowledge that this makes a project highly efficient, cohesive, and also charming: people love heroes; however, this is contradictory to decentralization, the core value of blockchain. \r\n\r\nSome projects entrust a distinguished off-chain committee with far-reaching decision-making power, such as the ECAF (EOSIO Core Arbitration Forum) on EOS. However, these committees lack the essential power to guarantee participants will abide by their decisions, which could have played a role in the decision to shut down the ECAF earlier this year. \r\n\r\nSome projects, such as Tezos, go further, and implement on-chain governance to ensure all participants abide by voted upon decisions. This also avoids any impacts of discord between developers and miners (or full node users). Note that on-chain governance is different from a simple on-chain vote, if a proposed feature or patch has acquired enough votes through on-chain governance, the chain code will be updated automatically, miners or full nodes do not have any means of controlling this change. Polkadot takes an even more sophisticated approach to on-chain governance, utilizing an elected council, referendum process for stake-weighted voting and positive/negative bias mechanisms to account for voter turnout. \r\n\r\nHowever, despite its straightforwardness, on-chain governance in practice is not as elegant as it is presented. First of all, votes only reflect the interest of token holders, while simply ignoring all other parties. Secondly, a low voting rate is a long-standing problem in both the blockchain world and real world. How can results be in the best interest of the majority if only a minority vote? Last but most importantly, a hard fork should always be considered as final recourse for all stakeholders. Given the excellent data availability provided by the wide replication of a permissionless blockchain, forking away from the existing chain with full data preservation and without interruption should always be an option. A hard fork could never be implemented via on-chain governance.\r\n\r\nThere are not yet viable answers to the questions of governance, so for Nervos Network we will take an evolving approach. We expect the community to develop organically in the early days and over time, as more tokens are mined, mining becomes more distributed, and more developers are engaged, governance responsibilities will gradually become more decentralized. Over the long term, community-based governance will manage the protocol upgrade process and resource allocation from the treasury.\r\n\r\nNervos CKB is designed to be decentralized autonomous infrastructure that could last for hundreds of years, which means there are certain things that demand our best effort as a community to hold true, no matter how this network evolves. The 3 core invariants are:\r\n\r\n- Issuance schedule is completely fixed, thus shall never change.\r\n- State/data stored in cells shall not be tampered with.\r\n- Existing scripts' semantics shall not be changed.\r\n\r\nCommunity-based governance for blockchains is a very new field and there are many worthy on-going experiments. We recognize that this is not a trivial topic, and time is required to fully study, observe, and iterate to arrive at an optimal approach. We're taking a conservative approach to community-based governance in the short-term, while remaining fully committed to this direction in the long run.\r\n\r\n## 5. Overview of Layer 2 Solutions\r\n\r\n### 5.1 What is Layer 2?\r\n\r\nA blockchain network's layer 1 is defined by constraints. An ideal layer 1 blockchain makes no compromises on security, decentralization and sustainability, however, this creates challenges related to scalability and transaction costs. Layer 2 solutions are built on top of layer 1 protocols, allowing computation to be moved off-chain with mechanisms to securely settle back to the layer 1 blockchain.\r\n\r\nThis is similar to net settlement in today's banking system or SEC-mandated regulatory filings. By reducing the amount of data requiring global consensus, the network can serve more participants and facilitate more economic activity than it would have been able to otherwise, while still maintaining the properties of decentralization.\r\n\r\nLayer 2 users depend on security provided by the layer 1 blockchain, and utilize this security when moving assets between layers or settling a dispute. This function is similar to a court system: the court doesn't have to monitor and validate all transactions, but only serves as a place to record key evidence and to settle disputes. Similarly, in a blockchain context, the layer 1 blockchain allows participants to transact off-chain, and in the case of a disagreement provides them with the ability to bring cryptographic evidence to the blockchain and penalize dishonesty. \r\n\r\n### 5.2 Payment and State Channels\r\n\r\nPayment channels are created between two parties that transact often. They provide a low-latency, immediate payment experience that transactions done directly on a global blockchain could never provide. Payment channels function similar to a bar tab - you can open a tab with a bartender and keep ordering drinks, but only settle the tab and pay the final amount when you're ready to leave the bar. In the operation of a payment channel, participants exchange messages containing cryptographic commitments to their balances and can update these balances an unlimited number of times off-chain, before they're ready to close the channel and settle balances back on the blockchain.\r\n\r\nPayment channels can be unidirectional or bidirectional. Unidirectional payment channels flow from Party A to Party B, similar to the bar tab example above. Party A deposits the maximum amount they might spend with Party B, and then slowly signs over funds as they receive goods or services.\r\n\r\nBidirectional payment channels are more complicated, but start to show the scope of possibilities for layer 2 technologies. In these payment channels, funds flow back and forth between parties. This allows for \"rebalancing\" of payment channels and opens up the possibility of payments across channels through a shared counterparty. This enables networks of payment channels, such as Bitcoin's Lightning Network. Funds can be transferred from Party A to Party B without a direct channel between them, as long as Party A can find a path through an intermediary with connections open to both parties.\r\n\r\nJust as payment channels can scale on-chain payments, state channels can scale any on-chain transactions. While a payment channel is limited to managing balances between two parties, a state channel is an agreement on arbitrary state, enabling everything from a game of trustless chess to scalable decentralized applications.\r\n\r\nSimilar to a payment channel, the parties open a channel, exchange cryptographic signatures over time and submit a final state (or result) to an on-chain smart contract. The smart contract will then execute based on this input, settling the transaction according to rules encoded in the contract.\r\n \r\nA \"generalized state channel\" is a powerful state channel construction, allowing a single state channel to support state transitions across multiple smart contracts. This reduces the state bloat inherent in a \"one channel per application\" architecture and also allows for easy on-boarding with the ability to utilize state channels users already have open. \r\n\r\n### 5.3 Side-chains\r\n\r\nA side-chain is a separate blockchain that's attached to a trustless blockchain (main-chain) with a two-way peg. To utilize the side-chain, a user would send funds to a specified address on the main-chain, locking these funds under control of the side-chain operators. Once this transaction is confirmed and a safety period has passed, a proof can be communicated to side-chain operators detailing the deposit of funds. The operators will then create a transaction on the side-chain, distributing the appropriate funds. These funds can then be spent on the side-chain with low fees, fast confirmation and high throughput.\r\n\r\nThe main drawback of side-chains is that they require additional security mechanisms and security assumptions. The simplest side-chain construction, a federated side-chain, places trust in a multi-signature group of operators. On smart contract platforms, security models can be fine-tuned with token incentives or bonding/challenging/slashing economic games. \r\n\r\nCompared to other off-chain general purpose scaling solutions, side-chains are easier to understand and implement. For types of applications that allow creation of a trust model that's acceptable to their users, side-chains can be a practical solution.\r\n\r\n### 5.4 Commit-chains\r\n\r\nOn commit-chains[6], such as Plasma[7], a layer 2 chain is constructed that leverages a trust root on a layer 1 blockchain (root-chain) with broad global consensus. These commit-chains are secure; in the event a chain operator is malicious or dysfunctional, users can always withdraw their assets through a mechanism on the root-chain.\r\n\r\nA commit-chain operator is trusted to execute transactions correctly and publish periodic updates to the root-chain. Under all conditions, except for a prolonged censorship attack on the root-chain, assets on the commit-chains will remain safe. Similar to federated side-chains, commit-chain designs offer a superior user experience compared to trustless blockchains. However, they do so while maintaining stronger security guarantees.\r\n  \r\nThe commit-chain is secured by a set of smart contracts running on the root-chain. Users deposit assets into this contract and the commit-chain operator then provides them assets on the commit-chain. The operator will periodically publish commitments to the root-chain, which users can later utilize to prove asset ownership through Merkle proofs, an \"exit\", in which commit-chain assets are withdrawn to the root-chain.\r\n\r\nThis describes the general notion of commit-chain designs, the basis of an emerging family of protocols including Plasma. The Plasma white paper[7] released by Vitalik Buterin and Joseph Poon in 2017 lays out an ambitious vision. Though all Plasma chains are currently asset-based, and can only store fungible and non-fungible token ownership (and transfers), trustless code execution (or smart contracts) is an active area of research.\r\n\r\n### 5.5 Verifiable Off-Chain Computations\r\n\r\nCryptography provides a tool seemingly tailored to the dynamics of expensive on-chain verification and inexpensive off-chain computation: interactive proof systems. An interactive proof system is a protocol with two participants, the Prover and the Verifier. By sending messages back and forth, the Prover will provide information to convince the Verifier that a certain claim is true, whereas the Verifier will examine what is provided and reject false claims. Claims that the Verifier cannot reject are accepted as true.\r\n\r\nThe principal reason why the Verifier does not simply verify the claim naïvely on his own is efficiency — by interacting with a Prover, the Verifier can verify claims that would be prohibitively expensive to verify otherwise. This complexity gap can come from a variety of sources: 1) the Verifier may be running lightweight hardware that can support only space-bounded or time-bounded (or both) computations, 2) naïve verification may require access to a long sequence of nondeterministic choices, 3) naïve verification may be impossible because the Verifier does not possess certain secret information. \r\n\r\nWhile the secrecy of important information is certainly a relevant constraining factor in the context of cryptocurrencies, a more relevant constraining factor in the context of scalability is the cost of on-chain verification, especially in contrast to relatively cheap off-chain computation.\r\n\r\nIn the context of cryptocurrencies, significant attention has been directed towards zk-SNARKs (zero-knowledge, succinct non-interactive arguments of knowledge). This family of non-interactive proof systems revolves around the arithmetic circuit, which encodes an arbitrary computation as a circuit of additions and multiplications over a finite field. For instance, the arithmetic circuit can encode \"I know a leaf in this Merkle tree\".\r\n\r\nzk-SNARK proofs are constant-size (hundreds of bytes) and verifiable in constant time, although this Verifier-efficiency comes at a cost: a trusted setup and a structured reference string are required, in addition to pairing-based arithmetic (of which concrete cryptographic hardness remains an object of concern). \r\n\r\nAlternative proof systems provide different trade-offs. For instance, Bulletproofs have no trusted setup and rely on the much more common discrete logarithm assumption, however have logarithmic-size proofs (though still quite small) and linear-time Verifiers. zk-STARKs provide an alternative to zk-SNARKs in terms of scalability, without a trusted setup and rely only on rock-solid cryptographic assumptions, although the produced proof is logarithmic in size (and quite large: hundreds of kilobytes).\r\n\r\nIn the context of a multi-layer cryptocurrency ecosystem such as the Nervos Network, interactive proofs offer the ability to offload expensive Prover-side computations to layer 2 while requiring only modest Verifier-side work from layer 1. This intuition is captured, for instance, in Vitalik Buterin's ZK Rollup protocol[8]: a permissionless relayer gathers transactions off-chain and periodically updates a Merkle root stored on chain. Every such root update is accompanied by a zk-SNARK that shows that only valid transactions were accumulated into the new Merkle tree. A smart contract verifies the proof and allows the Merkle root to be updated only if the proof is valid.\r\n\r\nThe construction outlined above should be able to support more complex state transitions beyond simple transactions, including DEX's, multiple tokens, and privacy-preserving computation.\r\n\r\n### 5.6 Economic Model of Layer 2 Solutions\r\n\r\nWhile layer 2 solutions provide impressive scalability, the token economics of these systems may pose design challenges.\r\n\r\nLayer 2 token economics may involve compensation for critical infrastructure (such as validators and watchtowers), as well as application-specific incentive design. Critical layer 2 infrastructure tends to work better with a duration-based, subscription model. In the Nervos Network, this pricing structure can be easily implemented through the CKB's opportunity cost-based payment method. Service providers can collect fees on their users' \"deposits\" through the NervosDAO. Layer 2 developers can then focus token economic models on incentives specific to their applications.\r\n\r\nIn a way, this pricing model is exactly how users pay for state storage on the CKB as well. They're essentially paying a subscription fee to miners with the distribution of their inflation rewards issued by the NervosDAO.\r\n\r\n## 6. The Nervos Network\r\n\r\n### 6.1 Layer 1 as a Multi-asset Store of Value Platform\r\n\r\nWe believe that a layer 1 blockchain has to be built as a store of value. To maximize long-term decentralization, it has to be based on proof of work consensus with an economic model designed around state storage occupation, instead of transaction fees. The Common Knowledge Base (CKB) is a proof of work-based, multi-asset, store of value blockchain with both its programming and economic models designed around state.\r\n\r\nThe CKB is the base layer of the Nervos Network, with the highest security and highest degree of decentralization. Owning and transacting assets on the CKB comes with the highest cost, however provides the most secure and accessible asset storage in the network and allows for maximum composability. The CKB is best suited for high value assets and long-term asset preservation.\r\n\r\nThe Common Knowledge Base is the first layer 1 blockchain built specifically to support layer 2 protocols:\r\n\r\n- The CKB is designed to complement layer 2 protocols, focusing on security and decentralization, instead of overlapping layer 2 priorities such as scalability.\r\n- The CKB models its ledger around state, instead of accounts. Cells are essentially self-contained state objects that can be referenced by transactions and passed around between layers. This is ideal for a layered architecture, where the objects referenced and passed between layers are pieces of state, instead of accounts.\r\n- The CKB is designed as a generalized verification machine, instead of computation engine. This allows the CKB to serve as a cryptographic court, that verifies off-chain state transitions.\r\n- The CKB allows developers to easily add custom cryptographic primitives. This future-proofs the CKB, allowing for verification of proofs generated by a variety of layer 2 solutions.\r\n\r\nThe Common Knowledge Base aims to be the infrastructure to store the world's most valuable common knowledge, with the best-in-class layer 2 ecosystem providing the most scalable and efficient blockchain transactions.\r\n\r\n### 6.2 Scale with Layer 2 Solutions\r\n\r\nWith its layered architecture, the Nervos Network can scale on layer 2 to any number of participants, while still maintaining the vital properties of decentralization and asset preservation. Layer 2 protocols can make use of any type of layer 1 commitment or cryptographic primitive, enabling great flexibility and creativity in designing transactional systems to support a growing layer 2 user base. Layer 2 developers can choose their own trade-offs in regard to throughput, finality, privacy and trust models that work best in the context of their applications and users.\r\n\r\nIn the Nervos Network, layer 1 (CKB) is used for state verification, while layer 2 is responsible for state generation. State channels and side-chains are examples of state generation, however any type of generate-verify pattern is supported, such as a zero-knowledge proof generation cluster. Wallets also operate at layer 2, running arbitrary logic, generating new state and submitting state transitions to the CKB for validation. Wallets in the Nervos Network are very powerful because they are state generators, with full control over state transitions.\r\n\r\nSide-chains are developer-friendly and provide a good user experience. They do however, rely on the honesty of their validators. If the validators behave maliciously, users are in danger of losing their assets. Nervos Network provides an open-source and easy-to-use side-chain stack for launching side-chains on the CKB, consisting of a Proof-of-Stake blockchain framework called \"Muta\" and a side-chain solution based on it called \"Axon\".\r\n\r\nMuta is a highly customizable, high-performance blockchain framework designed to support Proof-of-Stake, BFT consensus and smart contracts. It features a high throughput and low latency BFT consensus \"Overlord\", and supports various virtual machines including CKB-VM, EVM and WASM. Different virtual machines can be used in a single Muta blockchain simultaneously, with cross-VM interoperability. Muta greatly lowers the barrier for developers to build high performance blockchains, while still allowing maximum flexibility to customize their protocols.\r\n\r\nAxon is a complete solution built with Muta to provide developers a turnkey side-chain on top of the Nervos CKB, with a practical security and token economic model. Axon solutions use the CKB for secure asset custody, and use token-based governance mechanism to manage the side-chain validators. Cross-chain protocols for interactions between an Axon side-chain and the CKB, as well as between Axon side-chains will also be built-in. With Axon, developers can focus on building applications, instead of building infrastructure and cross-chain protocols. \r\n\r\nBoth Muta and Axon are currently under heavy development. We'll open source the frameworks soon, and RFCs for both Muta and Axon are also on the way.\r\n\r\nLayer 2 protocols are a flourishing area of research and development. We foresee a future in which all layer 2 protocols are standardized and seamlessly interoperate. However, we acknowledge that layer 2 solutions are still maturing, and we're often still pushing the boundaries of what they can do, as well as finding their acceptable trade-offs. We've seen early promising solutions, but there's still plenty of research to conduct on subjects such as interoperability, security and economic models in layer 2 designs.\r\n\r\n### 6.3 Sustainability\r\n\r\nIn the interest of long-term sustainability, the Nervos Common Knowledge Base bounds state, imposes a cost on on-chain storage and provides incentives for users to clear their state storage. A bounded state keeps the requirements for full node participation low, ensuring nodes can be run on low-cost hardware. Robust full node participation increases decentralization and in turn, security.\r\n\r\nBy imposing a time-proportional \"state-rent\" cost on state storage, the Nervos Common Knowledge Base mitigates the tragedy of the commons faced by many blockchains in a \"pay once, store forever\" paradigm. Implemented through \"targeted inflation\", this state rent mechanism provides a smooth user experience while imposing a cost on state storage.\r\n\r\nThis inflation cost can be targeted because users own the consensus space their data occupies. This model also includes a native mechanism for users to remove their state from the consensus space. Coupled with the economic incentives of state rent, this ensures that state size will always be moving toward the minimum amount of data required by network participants.\r\n\r\nIndividually owned state also significantly reduces developers' costs. Instead of being required to purchase CKBytes for the state requirements of all their users, developers only have to purchase enough CKBytes to store the verification code required by their application. Each user would use their own cells to store their tokens and would be fully responsible for their assets.\r\n\r\nFinally, state rent provides an ongoing reward to miners through new token issuance. This predictable income incentivizes miners to advance the blockchain, instead of forking profitable blocks to take the transaction fees.\r\n\r\n### 6.4 Aligned Incentives\r\n\r\nThe economic model of the Common Knowledge Base is designed to align incentives for all participants in the ecosystem.\r\n\r\nThe Nervos Common Knowledge Base is built explicitly for secure value preservation, instead of cheap transaction fees. This critical positioning will attract store of value users, similar to the user community of Bitcoin, instead of medium of exchange users.\r\n\r\nMedium of exchange use cases have a tendency to always push a blockchain network toward centralization, in pursuit of greater efficiency and low fees. Without significant fee income for infrastructure operators that secure the network (miners or validators), security must be funded through monetary inflation, or is simply under-funded. Monetary inflation is detrimental to long-term holders, and under-funded security is detrimental to any stakeholder of the network.\r\n\r\nStore of value users however, have strong demands for censorship resistance and asset security. They rely on miners to provide this, and in turn compensate them for their role. In a store of value network, these parties have aligned interests.\r\n\r\nBy aligning the incentives of all participants, a united Nervos community can grow, and the aligned economic system of the network is also expected be hard-fork resistant.\r\n\r\n### 6.5 Value Capture and Value Generation\r\n\r\nFor any blockchain to remain secure as the value of assets secured by the platform increases, the system must have a mechanism to capture value as the value of assets secured grows. By bounding state, the CKB makes the state space a scarce and market-priced resource. As demand for asset storage on the network rises, the system is expected to better compensate the miners for securing such assets.\r\n\r\nAs a value preserving platform, the intrinsic value of the CKB as a platform is determined by the amount of security it provides to the assets it preserves. As the value of assets secured rises, the value capture mechanism of the CKB economic model is able to automatically raise the CKB's security budget to attract more mining resources, making the platform more secure. Not only is this important to make the platform sustainable, it also provides a path of growth for the platform's intrinsic value - as the platform becomes more secure, it also becomes more attractive to higher-value assets, generating more demand. Obviously, this is bound by the overall aggregate value that will eventually move to the blockchain space.\r\n\r\nOver time, we expect the economic density of the CKB to increase. CKBytes will be used for high-value asset storage and low-value assets will to move to blockchains connected to the CKB, such as layer 2 side-chains. Instead of directly securing assets, the CKB can be used as a trust root to secure an entire side-chain’s ecosystem through, for example,  a few hundred bytes of cryptographic proofs. The economic density of such proofs is extraordinarily high, further supporting the demand curve of storage space: analogous to a small parcel of land significantly increasing its economic density by supporting a skyscraper.\r\n\r\nFinally, through the design of the NervosDAO and its \"inflation shelter\" function, long-term token holders will always retain a fixed percentage of total issuance, making the native token itself a robust store of value.\r\n\r\n### 6.6 Bridging the Regulatory Gap\r\n\r\nPermissionless blockchains allow total decentralization in asset issuance and transaction. This is what makes them valuable, but is also the reason they aren't compatible with real-world financial and judicial systems.\r\n\r\nThe emergence of a layered architecture provides the opportunity to create regulatory compliant portions of an unregulated, permissionless blockchain. For example, users can store their decentralized assets on layer 1, enjoy absolute property ownership of these assets, and can also process real-world business on layer 2, where they are subject to regulatory and legal constraints.\r\n\r\nTake for example cryptocurrency exchanges - countries such as Japan and Singapore have issued licenses to exchanges and created regulatory requirements. A compliant exchange or a branch of a global exchange could build a layer 2 trading chain, import user identities and assets and then conduct legal business in accordance with local regulatory requirements.\r\n\r\nIssuance and transaction of real-world assets become possible within a layered blockchain construction. Real-world assets can flow to the blockchain ecosystem through a regulated layer 2 side-chain to the permissionless layer 1 blockchain, allowing these assets access to the largest ecosystem of composable, decentralized financial services.\r\n\r\nIn the future, it is expected that the Nervos Network will also use layer 2 side-chains and applications as the foundation of large-scale user adoption, in cooperation with leading companies in this space.\r\n\r\n# References\r\n\r\n[1] Satoshi Nakamoto. \"Bitcoin: A Peer-to-Peer Electronic Cash System\". 31 Oct 2008, https://bitcoin.org/bitcoin.pdf\r\n\r\n[2] Vitalik Buterin. \"Ethereum White Paper: A Next Generation Smart Contract & Decentralized Application Platform\". Nov 2013 http://blockchainlab.com/pdf/Ethereum_white_paper-a_next_generation_smart_contract_and_decentralized_application_platform-vitalik-buterin.pdf \r\n\r\n[3] With an average Bitcoin transaction size of 250 bytes:\r\n(2 * 250 * 7,500,000,000) / (24 * 6) = 26,041,666,666 byte blocks (every 10 minutes); \r\n26,041,666,666 * (24 * 6) = 3,750,000,000,000 bytes (blockchain growth each day);\r\n3,750,000,000,000 * 365.25 = 1,369,687,500,000,000 bytes (blockchain growth each year)\r\n\r\n[4] Gur Huberman, Jacob Leshno, Ciamac C. Moallemi. \"Monopoly Without a Monopolist: An Economic Analysis of the Bitcoin Payment System\". Bank of Finland Research Discussion Paper No. 27/2017. 6 Sep 2017, https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3032375\r\n\r\n[5] Miles Carlsten, Harry Kalodner, S. Matthew Weinberg, Arvind Narayanan. \"On the Instabiliity of Bitcoin Without the Block Reward\". Oct 2016, https://www.cs.princeton.edu/~smattw/CKWN-CCS16.pdf\r\n\r\n[6] Lewis Gudgeon, Perdo Moreno-Sanchez, Stefanie Roos, Patrick McCorry, Arthur Gervais. \"SoK: Off The Chain Transactions\". 17 Apr 2019, https://eprint.iacr.org/2019/360.pdf\r\n\r\n[7] Joseph Poon, Vitalik Buterin. \"Plasma: Scalable Autonomous Smart Contracts\". 11 Aug 2017, https://plasma.io/plasma.pdf\r\n\r\n[8] Vitalik Buterin. \"On-chain scaling to potentially ~500 tx/sec through mass tx validation\". 22 Sep 2018, https://ethresear.ch/t/on-chain-scaling-to-potentially-500-tx-sec-through-mass-tx-validation/3477\r\n","\\0002-ckb\\0002-ckb.md":"---\r\nNumber: \"0002\"\r\nCategory: Informational\r\nStatus: Draft\r\nAuthor: Jan Xie\r\nOrganization: Nervos Foundation\r\nCreated: 2018-01-02\r\n---\r\n\r\n# Nervos CKB: A Common Knowledge Base for Crypto-Economy\r\n\r\n## Abstract\r\n\r\nNervos is a layered crypto-economy network. Nervos separates the infrastructure of a crypto-economy into two layers: a verification layer (layer 1) that serves as a trust root and smart custodian, and a generation layer (layer 2) for high-performance transactions and privacy protection.\r\n\r\nThis document provides an overview of the Nervos Common Knowledge Base (CKB), a public permissionless blockchain and layer 1 of Nervos. CKB generates trust and extends this trust to upper layers, making Nervos a trust network. It's also the value store of the Nervos network, providing public, secure and censorship-resistant custody services for assets, identities and other common knowledge created in the network.\r\n\r\n## Contents\r\n\r\n1. Motivation\r\n2. Overview\r\n3. Consensus\r\n4. Programming Model\r\n    1. State Generation and Verification\r\n    2. Cell\r\n    3. VM\r\n    4. Transaction\r\n5. Economic Model\r\n6. Network\r\n7. Summary\r\n8. References\r\n9. Appendix\r\n\r\n## 1. Motivation\r\n\r\nWe want a peer-to-peer crypto-economy network.\r\n\r\nIn such a network, people can not only collaborate but also have incentives to do so. We need the ability to define, issue, transfer, and own assets in a peer-to-peer network to create such incentives. Blockchain technology brings us the last piece of the puzzle.\r\n\r\nBitcoin[1] was the first public permissionless blockchain, designed to be used solely as peer-to-peer cash. Ethereum[2] extends the use case of blockchain to create a general purpose trust computing platform on which people have built all kinds of decentralized applications. The booming applications on the Bitcoin and Ethereum networks have proven the concept of the future crypto-economy. However, these networks also suffer from the notorious scalability problem, their transaction processing capability cannot scale with the number of participants in the network, which severely limits their potential.\r\n\r\nThe blockchain community has proposed many scalability solutions in recent years. In general, we can divide these solutions into two categories, on-chain scaling and off-chain scaling. On-chain scaling solutions are those that try to scale at the same layer where consensus runs. The consensus process is the core of a blockchain protocol, in which nodes exchange network messages and reach agreement eventually. A consensus is slow almost by definition, because message exchange on a public and open network is slow and uncertain, nodes must wait and retry to reach agreement in the consensus process. To scale at this layer, we can either \"scale up\" by increasing the processing ability and network bandwidth of nodes (but sacrifice decentralization due to high hardware and infrastructure costs), or \"scale out\" by sharding. The idea of sharding is to divide nodes into many small \"shards\", and ask each shard to process only a fraction of network transactions. Sharding is widely adopted by Internet giants, as they face the same scalability issues when serving millions of users. However, sharding is well known for the complexity of shard coordination and cross-shard transactions, which even in a trusted environment, leads to performance degradation as the number of shards grows.\r\n\r\nIn contrast, off-chain scaling solutions acknowledge the inherent complexity of the consensus process. They recognize that consensus within different scopes incur different costs, and the global consensus created by a public permissionless blockchain is the most expensive consensus. While it is hard to scale a global consensus, we can use it wisely. Most transactions between two or more parties don't need to be known by every node in the network, except when they are securely settled; in other words, when users want to turn their transactions into common knowledge of the network. This network scales by offloading most of the work to upper layers, with no limit on scalability. Processing transactions off-chain also brings additional benefits, such as lower latency and higher privacy.\r\n\r\nWhile we agree with the general ideas of off-chain scaling, we have found that there is no existing blockchain designed for it. For example, though the lightning network is one of the earliest explorations in off-chain scaling, it has taken years to launch its testnet and is still far from mass-adoption due to the limitations of the underlying Bitcoin protocol. Ethereum provides powerful programming ability, but its computation-oriented economic model doesn't fit well with off-chain scaling. Because off-chain participants handle most of the computation, what is required is a blockchain that can keep their assets in secure custody and move assets according to the final state of their computation. The computation-oriented design of Ethereum also makes it difficult to execute transactions in parallel, which is an impediment to scalability.\r\n\r\nThe economic models of current blockchains also face challenges. With more users and applications moving to blockchain platforms, the amount of data stored on blockchains also increases. Current blockchain solutions are concerned more with the cost of consensus and computation, and allow a user to pay once and have their data occupy full nodes’ storage forever. Cryptocurrency prices also are highly volatile, and users may find it difficult to pay high transaction fees as the price of a cryptocurrency increases.\r\n\r\nWe propose Nervos CKB, a public permissionless blockchain designed for a layered crypto-economy network.\r\n\r\n## 2. Overview\r\n\r\nNervos CKB (Common Knowledge Base) is a layer 1 blockchain, a decentralized and secure layer that provides common knowledge custody for the network. Common knowledge refers to states that are verified by global consensus. Crypto-assets are an example of common knowledge.\r\n\r\nIn Nervos, the CKB and all layer 2 protocols work together to serve the crypto-economy. CKB (or layer 1) is where state is stored and defined, and layer 2 is the generation layer (or computation layer, these two terms are interchangeable) that processes most transactions and generates new states. Layer 2 participants submit newly generated states to the CKB eventually at the time they deem necessary. If those states pass the corresponding verification performed by nodes in a global network, the CKB stores them in a peer-to-peer node securely.\r\n\r\nThe layered architecture separates state and computation, providing each layer more flexibility and scalability. For example, blockchains on the generation layer (layer 2) may use different consensus algorithms. CKB is the lowest layer with the broadest consensus and provides the most secure consensus in the Nervos network. However, different applications might prefer different consensus scopes and forcing all applications to use CKB’s consensus would be inefficient. Applications can choose the appropriate generation methods based on their particular needs. The only time these applications will need to submit states to CKB for broader agreement is when they need to make these states common knowledge that has been verified by the CKB's global consensus.\r\n\r\nPossible state generation methods include (but are not limited to) the following:\r\n\r\n- Local generators on the client: Generators run directly on the client’s devices. Developers can implement the generator in any programming language.\r\n- Web services: Users may use traditional web services to generate new states. All current web services may work with CKB in this way to gain more trust and liquidity for the generated states. For example, game companies may define in-game items as assets in CKB, the game itself functions as a web service that generates game data, which is then verified and stored in CKB.\r\n- State channels: Two or more users may use peer-to-peer communication to generate new states.\r\n- Generation chains: A generation chain is a blockchain that generates new states and stores them in CKB. Generation chains may be permissionless blockchains or permissioned blockchains. In each generation chain, nodes reach consensus in smaller scopes, providing better privacy and performance.\r\n\r\n![Figure 1. Layered Architecture](images/layered-architecture.png)\r\n*Figure 1. Layered Architecture*\r\n\r\nCKB consists of a Proof-of-Work based consensus, a RISC-V instruction set based virtual machine, a state model based on cells, a state-oriented economic model, and a peer-to-peer network. The Proof-of-Work based consensus makes the CKB a public and censorship-resistant service. The combination of CKB VM and the Cell model creates a stateful Turing-complete programming model for developers, making state generation (or layer 2) on CKB practical. The CKB economic model is designed for common knowledge custody and long-term sustainability. The CKB peer-to-peer network provides secure and optimal communication between different types of nodes.\r\n\r\n## 3. Consensus\r\n\r\nCKB consensus is an improved Nakamoto consensus based on Proof-of-Work, that aims to achieve openness, correctness and high performance in distributed environments with network delay and Byzantine node faults.\r\n\r\nPermissionless blockchains run in open networks where nodes can join and exit freely, with no liveness assumptions. These are severe problems for traditional BFT consensus algorithms to solve. Satoshi Nakamoto introduced economic incentives and probabilistic consensus to solve these problems. Nakamoto consensus in Bitcoin uses blocks as votes, which takes longer (up to 10 minutes to an hour) to confirm transactions and leads to an inferior user experience.\r\n\r\nCKB consensus is a Nakamoto consensus variant, which means it allows nodes to join and exit the network freely. Every node can participate in the consensus process either by mining (running a specific algorithm to find the Proof-of-Work) to produce new blocks, or by verifying new blocks are valid. CKB uses an ASIC-neutral Proof-of-Work function, with the goals of distributing tokens as evenly as possible and making the network as secure as possible.\r\n\r\nCorrectness includes eventual consistency, availability, and fairness. Eventual consistency guarantees every node sees an identical copy of state. Availability makes sure the network responds to users' requests within a reasonable time. Fairness ensures mining nodes get fair returns for their efforts to keep the network functioning securely.\r\n\r\nHigh performance includes transaction latency, the time between the submission of a request and the confirmation of its execution results, and transaction throughput, the number of transactions the system is capable of processing per second. Both of these measures depend on block time, which is the average time between two consecutive blocks.\r\n\r\nPlease check the CKB Consensus Paper for more details.\r\n\r\n## 4. Programming Model\r\n\r\nCKB provides a stateful Turing-complete programming model based on CKB VM and cell model.\r\n\r\n| | Bitcoin | Ethereum | CKB |\r\n|-|---------|----------|------------|\r\n|Instruction Set|Script|EVM|RISC-V|\r\n|Cryptographic Primitive|Opcode|Precompile|Assembly|\r\n|Stateful|No|Yes|Yes|\r\n|State Type|Ledger|General|General|\r\n|State Model|UTXO|Account|Cell|\r\n|State Verification|On-chain|On-chain|On-chain|\r\n|State Generation|Off-chain|On-chain|Off-chain|\r\n\r\n*Table 1. Comparison of Bitcoin, Ethereum and CKB Programming Model*\r\n\r\nThe CKB programming model consists of three parts:\r\n\r\n- state generation (off-chain)\r\n- state verification (CKB VM)\r\n- state storage (Cell model)\r\n\r\nIn this model, decentralized application logic is split into two parts (generation and verification), running in different places. State generation logic runs off-chain on the client side; new states are packaged into transactions and broadcasted to the entire network. CKB transactions have an inputs/outputs based structure like Bitcoin. Transaction inputs are references to previous outputs, along with proofs to unlock them. The client includes generated new states as transaction outputs, which are called cells in CKB. Cells are the primary state storage units in CKB and are assets owned by users that must follow associated application logic specified by scripts. CKB VM executes these scripts and verifies proofs included in inputs to make sure the user is permitted to use referenced cells and the state transition is valid under specified application logic. In this way, all nodes in the network verify that new states are valid and keep these states in custody.\r\n\r\nState in CKB is a first-class citizen, states are included in transactions and blocks and synchronized directly among nodes. Although the programming model is stateful, scripts running in CKB VM are pure functions with no internal state, which makes CKB scripts deterministic, conducive to parallel execution, and easy to compose.\r\n\r\n### 4.1 State Generation and Verification\r\n\r\nDecentralized applications on Nervos separate the generation and verification of state. While these processes occur in different places, CKB provides the additional flexibility to utilize different algorithms for state generation and verification.\r\n\r\nUtilizing the same algorithm on both generation and verification sides is a straightforward choice that works for general problems. In this model, the same algorithm has two implementations, one that runs off-chain in any execution environment targeted by the application, and the other one runs on-chain in CKB VM. New states are generated off-chain with this algorithm (based on previous states and user inputs), packaged as a transaction, and then broadcasted to the network. CKB nodes run this same algorithm on-chain, provide it the same previous states and user inputs, and then verify the result matches the transaction-specified outputs.\r\n\r\nThere are several advantages to this separation of state generation and validation:\r\n\r\n- Deterministic transactions: Certainty of transaction execution is one of the core pursuits of decentralized applications. If transactions include only user input and new states are the result of computation on nodes (as seen in Ethereum), the transaction creator cannot be certain about the on-chain computation context, which may lead to unexpected results. In CKB, users generate new states on the client side. They can confirm the new states before broadcasting their state transition to the network. The transaction outcome is certain: either the transaction passes on-chain verification and the new state is accepted, or the transaction is deemed invalid and no state change is made to CKB (Figure 1).\r\n\r\n- Parallelism: If transactions only include user inputs and new states are generated by nodes, then nodes will not know what state is going to be accessed by the verification process, and cannot determine dependencies between transactions. In CKB, because transactions explicitly include previous states and new states, nodes can see dependencies between transactions prior to verification, and can process transactions in parallel.\r\n\r\n- Higher resource utilization: As application logic is split and run in different places, the network can distribute computational workload more evenly across nodes and clients, and thus utilize system resources more efficiently.\r\n\r\n- Flexible state generation: Even when the same algorithms are used, developers can implement generation and validation in different ways. On the client side there is the flexibility to choose the programming language that provides for better performance and fast development.\r\n\r\nIn some scenarios, state verification can utilize a different (but associated) algorithm that is much more efficient than the one used for state generation. The most typical example is seen in Bitcoin transactions: Bitcoin transaction construction consists mainly of a searching process to identify appropriate UTXOs to use, while verification is the addition of numbers and simple comparison. Other interesting examples include sorting and searching algorithms: the computational complexity for quicksort, one of the best sorting algorithms for the average case, is O(Nlog(N)), but the algorithm to verify the result is just O(N). Searching for the index of an element in a sorted array is O(log(N)) with binary search, but its verification only takes O(1). The more complex the business rules, the higher probability that there can be asymmetric generation and validation algorithms with differing computational complexity.\r\n\r\nSystem throughput can be improved by utlizing asymmetry between state generation and validation. Moving details of computation to the client side is also valuable for algorithm protection and privacy. With the advancement of technologies such as zero-knowledge proofs, we may find efficient generation and verification solutions to general problems, and CKB is a natural fit for these types of solutions.\r\n\r\nWe refer to programs that generate new states and create new cells as Generators. Generators run locally on the client side (off-chain). They utilize user input and existing cells as program inputs, to create new cells with new states as outputs. The inputs that Generators use and the outputs they produce together form a transaction.\r\n\r\n![Figure 2. Separation of state generation and verification](images/separation-of-generation-verification.png)\r\n*Figure 2. Separation of state generation and verification*\r\n\r\n### 4.2 Cell\r\n\r\nCells are the primary state units in CKB, within them users can include arbitrary states. A cell has the following fields:\r\n\r\n- `capacity` - Size limit of the cell. A cell's size is the total size of all fields contained in it.\r\n- `data` - State data stored in this cell. It could be empty, however the total bytes used by a cell (including data), must always be less than or equal to its capacity.\r\n- `type`: State verification script.\r\n- `lock`: Script that represents the ownership of the cell. Owners of cells can transfer cells to others.\r\n\r\nA cell is an immutable object, no one can modify it after creation. Every cell can only be used once, it cannot be used as input for two different transactions. Cell ‘updates’ mark previous cells as history and create new cells with the same capacity to replace them. By constructing and sending transactions, users provide new cells with new states in them and invalidate previous cells that store old states atomically. The set of all current (or live) cells represents the latest version of all common knowledge in CKB, and the set of history (or dead) cells represents all historical versions of common knowledge.\r\n\r\nCKB allows users to transfer a cell's capacity all at once, or transfer only a fraction of a cell's capacity, which would in turn lead to more cells being created (e.g., a cell whose capacity is 10 bytes can become two cells whose capacity is 5 bytes each).\r\n\r\nTwo kinds of scripts (type and lock) are executed in CKB VM. CKB VM executes the `type` script when a cell is created in a transaction output, to guarantee the state in the cell is valid under specific rules. CKB VM executes the lock script, taking proofs as arguments, when the cell is referenced by a transaction input, to make sure the user has appropriate permissions to update or transfer the cell. If the execution of the lock script returns true, the user is allowed to transfer the cell or update its data according to validation rules that are specified by the `type` script.\r\n\r\nThis `type` and `lock` script pair allows all kinds of possibilities, for example:\r\n\r\n- Upgradable cryptography - Anyone can deploy useful cryptography libraries written in languages such as C or C++ and use them in `type` and `lock` scripts. In CKB VM, there are no hardcoded cryptographic primitives, users are free to choose any cryptographic signature scheme they'd like to use to sign transactions.\r\n- Multisig - Users can easily create M-of-N multisig or more complex `lock` scripts.\r\n- Lending - Cell owners can lend cells for others to use while still maintaining their ownership of the cells.\r\n\r\nThe Cell model is a more generic state model compared to the UTXO or Account model. Both the UTXO and the Account model can express relationships between assets and their owners. The UTXO model defines ownership of assets (with the lock script), while the Account model defines ownership of assets by owner (with the account balance). The UTXO model makes the ledger history more clear, but its lack of generic state storage makes its already inexpressive scripts harder to use. The Account model is easy to understand and can support authorizations and identities well, but it presents challenges to processing transactions in parallel. The Cell model with `lock` and `type` scripts takes the best of both models to provide a more generic state model.\r\n\r\n### 4.3 VM\r\n\r\nCKB VM is a RISC-V instruction set based VM for executing type and lock scripts. It uses only standard RISC-V instructions, to maintain a standard compliant RISC-V software implementation which can embrace the broadest industrial support. CKB implements cryptographic primitives as ordinary assembly running on its VM, instead of customized instructions. It supports syscall, by which scripts can read metadata such as current transaction and general blockchain information from CKB. CKB VM defines `cycles` for each instruction, and provides total cycles executed during transaction verification to help miners determine transaction fees.\r\n\r\nExisting blockchains hardcode cryptographic primitives in the protocol. For example, Bitcoin has special cryptographic opcodes such as `OP_CHECK*`, and Ethereum uses special 'precompiled' contracts located at a special address (e.g. `0000000000000000000000000000000000000001`) to support cryptographic operations such as `ecrecover`. To add new cryptographic primitives to these blockchains, we can only soft-fork (as Bitcoin re-uses opcodes to support new primitives) or hard-fork.\r\n\r\nCKB VM is a crypto-agnostic virtual machine. There are no special cryptographic instructions hardcoded in CKB VM. New cryptographic primitives can always be deployed and used by scripts like an ordinary library. Being a RISC-V standard compliant implementation means existing cryptographic libraries written in C or other languages can be easily ported to CKB VM and used by cell scripts. CKB even implements the default hash function and public-key cryptography used in transaction verification this way. Being crypto-agnostic allows decentralized application developers on Nervos to use any new cryptography (such as Schnorr signatures, BLS signatures, and zkSNARKs/zkSTARKs) they'd like without affecting other users, and allows CKB users to keep their assets secure even in the post-quantum era.\r\n\r\nCKB VM chooses a hardware targeting ISA because blockchain is hardware-like software. Though its creation is as easy as software, its upgrade is as difficult as hardware. As an ISA designed for chips, RISC-V is very stable, its core instruction set is implausible to change in the future. The ability to keep compatibility with the ecosystem without the need of a hard-fork is a key feature of a blockchain virtual machine like CKB VM. The simplicity of RISC-V also makes runtime cost modeling easy, which is crucial for transaction fee calculations.\r\n\r\nPlease check [RFC 0003](https://github.com/nervosnetwork/rfcs/blob/master/rfcs/0003-ckb-vm/0003-ckb-vm.md) for more details of CKB VM.\r\n\r\n### 4.4 Transaction\r\n\r\nTransactions express state transitions, resulting in cell transfer, update, or both. In a single transaction, users can update data in one or more cells or transfer their cells to other users. All state transitions in the transaction are atomic, they will either all succeed or all fail.\r\n\r\nA transaction includes the following:\r\n\r\n- `deps`: Dependent cell set, provides read-only cells required by transaction verification. These must be references to living cells.\r\n- `inputs`: Cell references and proofs. Cell references point to live cells that are transferred or updated in the transaction. Proofs (e.g., signature) prove that the transaction creator has the permission to transfer or update the referenced cells.\r\n- `outputs`: New cells created in this state transition.\r\n\r\nThe design of the CKB cell model and transactions is friendly to light clients. Since all the states are in blocks, block synchronization also accomplishes state synchronization. Light clients only need to synchronize blocks and do not need additional state synchronization or state transition computation. If only events were stored in blocks, full nodes would be required for state synchronization. State synchronization can be difficult across large networks because there are weak incentives to synchronize. This is different from block synchronization, in which miners are incentivized to broadcast blocks as widely as possible. With no need for extra state synchronization, the protocol makes light nodes and full nodes more equal peers, leading to a more robust and decentralized system.\r\n\r\n![Figure 3. Transaction Parallelism and Conflict Detection](images/transaction-parallelism.png)\r\n*Figure 3. Transaction Parallelism and Conflict Detection*\r\n\r\nThe `deps` and `inputs` in CKB transactions make it easier for nodes to determine transaction dependencies and perform parallel transaction processing (Figure 4). Different types of cells can be mixed and included in a single transaction to achieve atomic operation across types.\r\n\r\n## 5. Economic Model\r\n\r\nA well-designed economic model should incentivize all participants to contribute to the success of the crypto-economy and maximize the utility of the blockchain.\r\n\r\nThe CKB economic model is designed to motivate users, developers and node operators to work toward the common goal of common knowledge custody. The subject of the CKB economic model is state instead of computation, by using cell capacity and transaction fees as incentives for stakeholders.\r\n\r\n### 5.1 State Cost and Cell Capacity\r\n\r\nThe creation and storage of states on the CKB incur costs. The creation of new states needs to be verified by full nodes (which incur computational costs), and the storage of states requires full nodes to provide disk space on an ongoing basis. Current permissionless blockchains only charge one-time transaction fees, but allow states to be stored on all full nodes, occupying storage space indefinitely.\r\n\r\nIn CKB, cells are basic storage units of state. A cell owner can use the cell to store state himself or lend it out to others. Because a cell's capacity can only be utilized by one user at a time, an owner utilizing the capacity himself would give up the opportunity to earn interest by lending the capacity out (either to CKB or to other users). With this opportunity cost, users pay for storage with a cost that is proportional to both space and time - the larger the capacity and the longer time they occupy it, the higher opportunity cost they incur. The advantage of CKB's implicit state cost model, when compared to an upfront payment model (such as storage rent discussed in the Ethereum community), is that it avoids the problem that upfront payments could be used up and the system would have to recycle the state and break any applications or contracts depend on it.\r\n\r\nCell metadata (`capacity`, `type` and `lock`) are states, which will occupy users' cell capacity and incur a state cost as well. This meta cost would incentivize users to create fewer cells when possible, increasing capacity efficiency.\r\n\r\n### 5.2 Computation Cost and Transaction Fees\r\n\r\nUpdating a cell’s data or transferring cell ownership incurs transaction fees. Miners can set the transaction fee level that they are willing to accept based on CKB VM cycles used and state changes in transaction verification, allowing the market to determine transaction fees. With the programming model described above, cell owners can also pay transaction fees on behalf of their users.\r\n\r\nAs cell capacity is the only native asset in CKB, it is the most convenient asset users can use to pay transaction fees. However, users can also use any other user-defined assets as long as miners accept them; there is no hard-coded payment method in CKB transactions. This is allowed in CKB because its economic model and native asset do not center on computation, but states. Although cell capacity can be used as a means of paying transaction fees, its primary function is secure common knowledge storage, which can store state and hold it long-term. Payment method competition in the fee market does not compromise its value.\r\n\r\nRestricting the transaction fee payment method to a blockchain's native asset is a significant obstacle preventing blockchains' mass adoption. This requires users to acquire native assets before using any of the blockchain's services, raising the barrier of entry for new users. By allowing cell owners to pay fees on behalf of their users and allowing payment with any user-defined assets, CKB can provide a better experience to users and wider choices of business models for developers.\r\n\r\nPlease check the Nervos CKB Economic Paper ([RFC 0015](https://github.com/nervosnetwork/rfcs/blob/master/rfcs/0015-ckb-cryptoeconomics/0015-ckb-cryptoeconomics.md)) for details of the economic model.\r\n\r\n## 6. Network\r\n\r\nWe can categorize CKB nodes into three types:\r\n\r\n- Mining Node: They participate in the CKB consensus process. Mining nodes collect new transactions, package them into blocks and produce new blocks when they have found a Proof-of-Work. Mining nodes do not have to store the entire transaction history, only the current cell set.\r\n- Full Node: They verify new blocks and transactions, relay blocks and transactions, and select the chain fork on which they agree. Full nodes are the verifiers of the network.\r\n- Light Node: They trust full nodes, only subscribe and store a subset of cells that they are concerned with. They use minimal resources. Users increasingly rely on mobile devices and mobile apps to access the Internet, the light node is designed to run on mobile devices.\r\n\r\nUniform blockchain networks (in which each node has the same role and performs the same function) are currently facing severe challenges. Full nodes validate all blocks and transaction data, requiring minimum external trust, but they incur a higher cost and are inconvenient to run. Light clients trade minimal trust for a substantial cost reduction on transaction verification, leading to a much better user experience. In a mature crypto-economy network, the largest group of nodes would be light nodes, followed by full nodes and mining nodes. Because light nodes depend on full nodes for state and state verification, a large number of light nodes would require a large number of full nodes to serve them. With CKB's economic model, both computation and storage resources required by a full node can be kept at a reasonable level, and the barriers to running a full node low, leading to a large group of service providers for light nodes and a highly decentralized network.\r\n\r\n## 7. Summary\r\n\r\nWe envision a layered crypto-economy and CKB is its base layer. CKB is the decentralized trust root of this crypto-economy, it ensures the security of the trustless activities of the upper layers. It's a common knowledge custody network, in which states are verified by global consensus and stored in a highly available peer-to-peer network. CKB is designed from scratch to meet the needs of a layered architecture, and its design focuses on states rather than computation. In CKB, users and developers can define, issue, transfer and store crypto-assets, they can also create digital identities and utilize these identities in the crypto-economy. Only our imagination is the bounds of its use.\r\n\r\n## 8. References\r\n\r\n1. Satoshi Nakamoto, “Bitcoin A Peer-to-Peer Electronic Cash System”, 2008\r\n2. Vitalik Buterin, \"Ethereum A Next-Generation Smart Contract and Decentralized Application Platform\", 2014\r\n\r\n## 9. Appendix\r\n\r\nCommon Knowledge is the knowledge that’s accepted by everyone in a community. Participants in the community not only accept the knowledge themselves but know that others in the community also accept the knowledge.\r\n\r\nIn the past, common knowledge was scattered across individual's minds, and its formation required repeated communication and confirmation. Today, with the advancement of cryptography and distributed ledger technology, algorithms and machines are replacing humans as the medium for the formation and storage of common knowledge. Every piece of data in the blockchain, including digital assets and smart contracts, is a piece of common knowledge.\r\n\r\nBlockchains are common knowledge bases. Participating in a blockchain network implies accepting and helping validate the common knowledge contained in it. Blockchains store transactions with their proofs, users can trust the validity of these transactions and know other users trust it too.\r\n\r\n*The various ways in which the knowledge on which people base their plan is communicated to them is the crucial problem for any theory explaining the economic process, and the problem of what is the best way to utilizing knowledge initially dispersed among all the people is at least one of the main problems of economic policy - or of designing an efficient economic system.*\r\n\r\n*- The Use of Knowledge in Society, Friedrich A. Hayek, 1945*\r\n","\\0003-ckb-vm\\0003-ckb-vm.md":"---\r\nNumber: \"0003\"\r\nCategory: Informational\r\nStatus: Draft\r\nAuthor: Xuejie Xiao\r\nOrganization: Nervos Foundation\r\nCreated: 2018-08-01\r\n---\r\n\r\n# CKB-VM\r\n\r\n## Overview\r\n\r\nVM layer in CKB is used to perform a series of validation rules to determine if transaction is valid given transaction's inputs and outputs.\r\n\r\nCKB uses [RISC-V](https://riscv.org/) ISA to implement VM layer. To be more precise, CKB uses rv64imc architecture: it is based on core [RV64I](https://riscv.org/specifications/) ISA with M standard extension for integer multiplication and division, and C standard extension for RCV(RISC-V Compressed Instructions). Note that CKB doesn't support floating point instructions, a CKB script developer can choose to pack a softfloat implementation into the binary if needed.\r\n\r\nCKB relies on dynamic linking and syscalls to provide additional capabilities required by the blockchain, such as reading external cells or other crypto computations. Any compilers with RV64I support, such as [riscv-gcc](https://github.com/riscv/riscv-gcc), [riscv-llvm](https://github.com/lowRISC/riscv-llvm) or [Rust](https://github.com/rust-embedded/wg/issues/218) can be used to generate CKB compatible scripts.\r\n\r\n## RISC-V Runtime Model\r\n\r\nCKB leverages 64-bit RISC-V virtual machine to run contracts. We provide the core instructions in 64-bit address space, with additional integer multiplication/division extension instructions. CKB also supports RISC-V Compressed Instructions to reduce contract size. For maximum tooling and debugging support, CKB leverages Linux ELF format directly as contract format.\r\n\r\nEach contract has a maximum size of 10MB in uncompressed size, and 1MB in gzip size. CKB virtual machine has a maximum of 128 MB runtime memory for running contracts. VM's runtime memory provides space for executable code pages mapped from contracts, stack space, head space and mmapped pages of external cell.\r\n\r\nRunning a contract is almost the same as running an executable in single core Linux environment:\r\n\r\n```c\r\nint main(int argc, char* argv[]) {\r\n  uint64_t input_cell_length = 10000;\r\n  void *input_cell = malloc(input_cell_length);\r\n  ckb_load_cell(input_cell, &input_cell_length, 0, 0, CKB_SOURCE_INPUT);\r\n\r\n  uint64_t output_cell_length = 10000;\r\n  void *output_cell = malloc(output_cell_length);\r\n  ckb_load_cell(output_cell, &output_cell_length, 0, 0, CKB_SOURCE_OUTPUT);\r\n\r\n  // Consume input & output cell\r\n\r\n  return 0;\r\n}\r\n```\r\n\r\nContract starts from main function in the ELF formatted contract file, arguments are passed in via standard argc and argv. When main returns 0, the contract is treated as success. Note that due to space consideration, we might not store full inputs and outputs data in argv. Instead, we might just provide metadata in argv, and leverages additional libraries and syscalls to support input/output loading. This way the runtime cost can be minimized. CKB VM is a strict single-threaded model, contract can ship with coroutines of their own.\r\n\r\nFor simplicity and deterministic behavior, CKB doesn't support floating point numbers. We suggest a softfloat solution if floating point number is really needed. Since CKB runs in a single threaded environment, atomic instructions are not needed.\r\n\r\n## Libraries and bootloader\r\n\r\nCKB provides additional libraries in the form of VM libraries, and system cell. This is to make sure contract size can be reduced to bare minimum. Those libraries might include: libc, crypto libraries, IO libraries for reading/writing inputs/outputs, and additional tools for working with Cell. All those libraries would be implemented via dynamic linking to reduce contract size.\r\n\r\nIn addition, we will provide custom bootloader which might be used in compiler(gcc/llvm) linking phase to further reduce unnecessary cost.\r\n\r\nBased on current architecture, the following minimal C contract can be shrinked to 628 bytes uncompressed, and 313 bytes gzipped:\r\n\r\n```c\r\nint main()\r\n{\r\n  return 0;\r\n}\r\n```\r\n\r\nWe can think this as the intrinsic cost of RISC-V model.\r\n\r\n## Languages\r\n\r\nCKB only defines the low level virtual machine. In theory, any languages with RISC-V backend can be used for CKB contract development:\r\n\r\n* CKB can leverage standard riscv-gcc, riscv-llvm or even upstream gcc/llvm for C/C++ contract development. Executables emitted by those compilers can be directly used as CKB contracts.\r\n* C-based Bitcoin or Ethereum VM can also be compiled into RISC-V binaries as common cells, contracts can then load those common cells to run Bitcoin or Ethereum compatible contracts.\r\n* Higher-level language VMs, such as [duktape](http://duktape.org/) or [mruby](https://github.com/mruby/mruby) can also be compiled and loaded to run contracts running by JavaScript or Ruby\r\n* [Rust](https://github.com/riscv-rust/rust) can also be used to write contracts with recent development in this space\r\n\r\n## Runtime Cost\r\n\r\nCKB will leverage suitable open source RISC-V CPU implementation as the CPI(cycle per instruction) model. CPU cycles will be gathered while running each instruction of a contract. The total cycles accumulated when contract is completed will then be treated as the runtime cost of the contract.\r\n\r\nIn addition, we will also record running costs of reading/writing additional cells while running a contract.\r\n\r\n## Example\r\n\r\nHere a user defined token(UDT) issuing process will be used as an example. Note that the UDT implementation used here is simplified here:\r\n\r\n* 64-bit integer is used to store token number instead of 256-bit integer\r\n* Simple linear array is used instead of hashtable as account data structure. A strict upper bound is also used for simplicity\r\n* Alphabetical order is used to store accounts, so a simple memcmp can be used to determine data structure equality in exchange for slight performance penalty\r\n* Instead of a serialization step, C layout is used for storage\r\n\r\nIn production, the above assumptions won't be made in CKB\r\n\r\n### Data structure\r\n\r\nFollowing data structure is used to store token account information:\r\n\r\n```c\r\n#define ADDRESS_LENGTH 32\r\n#define MAX_BALANCES 100\r\n#define MAX_ALLOWED 100\r\n\r\ntypedef struct {\r\n  char address[ADDRESS_LENGTH];\r\n  int64_t tokens;\r\n} balance_t;\r\n\r\ntypedef struct {\r\n  char address[ADDRESS_LENGTH];\r\n  char spender[ADDRESS_LENGTH];\r\n  int64_t tokens;\r\n} allowed_t;\r\n\r\ntypedef struct {\r\n  balance_t balances[MAX_BALANCES];\r\n  int used_balance;\r\n  allowed_t allowed[MAX_ALLOWED];\r\n  int used_allowed;\r\n\r\n  char owner[ADDRESS_LENGTH];\r\n  char newOwner[ADDRESS_LENGTH];\r\n  int64_t total_supply;\r\n} data_t;\r\n```\r\n\r\nFollowing APIs are provided to work on the above data structures:\r\n\r\n```c\r\nint udt_initialize(data_t *data, char owner[ADDRESS_LENGTH], int64_t total_supply);\r\nint udt_total_supply(const data_t *data);\r\nint64_t udt_balance_of(data_t *data, const char address[ADDRESS_LENGTH]);\r\nint udt_transfer(data_t *data, const char from[ADDRESS_LENGTH], const char to[ADDRESS_LENGTH], int64_t tokens);\r\nint udt_approve(data_t *data, const char from[ADDRESS_LENGTH], const char spender[ADDRESS_LENGTH], int64_t tokens);\r\nint udt_transfer_from(data_t *data, const char from[ADDRESS_LENGTH], const char spender[ADDRESS_LENGTH], const char to[ADDRESS_LENGTH], int64_t tokens);\r\n```\r\n\r\nIt's both possible to compile implementations of those functions directly into the contract, or as dynamic linking cell code. Both solutions will be introduced below.\r\n\r\n### Issuing tokens\r\n\r\nAssume CKB has the following method for reading cell data:\r\n\r\n```c\r\nint ckb_read_cell_data(size_t index, size_t source, void** buffer, size_t* size);\r\n```\r\n\r\nGiven a cell ID, CKB VM will mmap cell content to address space of current virtual machine, and returns pointer to the content and size.\r\n\r\nFollowing contract can then be used for issuing tokens:\r\n\r\n```c\r\nint udt_initialize(data_t *data, char owner[ADDRESS_LENGTH], int64_t total_supply)\r\n{\r\n  memset(&data, 0, sizeof(data_t));\r\n  memcpy(data->owner, owner, ADDRESS_LENGTH);\r\n  memcpy(data->balances[0].address, owner, ADDRESS_LENGTH);\r\n\r\n  data->balances[0].tokens = total_supply;\r\n  data->used_balance = 1;\r\n  data->used_allowed = 0;\r\n  data->total_supply = total_supply;\r\n\r\n  return 0;\r\n}\r\n\r\nint main(int argc, char* argv[]) {\r\n  data_t data;\r\n  ret = udt_initialize(&data, \"<i am an owner>\", 10000000);\r\n  if (ret != 0) {\r\n    return ret;\r\n  }\r\n\r\n  data_t *output_data = NULL;\r\n  ret = ckb_read_cell(0, CKB_SOURCE_OUTPUT, (void **) &output_data, NULL);\r\n  if (ret != 0) {\r\n    return ret;\r\n  }\r\n\r\n  if (memcmp(&data, output_data, sizeof(data_t)) != 0) {\r\n    return -1;\r\n  }\r\n  return 0;\r\n}\r\n```\r\n\r\nIt ensures generated data is legit by validating that contents in output cell match contents generated in token initializing steps.\r\n\r\n### Transfer\r\n\r\nIn the above example, function implementation for validating cell is directly compiled into input contract script. It's also possible to reference and call code from external cell for validation.\r\n\r\nFirst, the following implementation can be provided for transfering UDT tokens:\r\n\r\n```c\r\nint udt_transfer(data_t *data, const char from[ADDRESS_LENGTH], const char to[ADDRESS_LENGTH], int64_t tokens)\r\n{\r\n  balance_t *from_balance = NULL, *to_balance = NULL;\r\n  int ret = _udt_find_balance(data, from, 1, &from_balance);\r\n  if (ret != 0) {\r\n    return ret;\r\n  }\r\n  ret = _udt_find_balance(data, to, 1, &to_balance);\r\n  if (ret != 0) {\r\n    return ret;\r\n  }\r\n  if (from_balance->tokens < tokens) {\r\n    return ERROR_NOT_SUFFICIENT_BALANCE;\r\n  }\r\n  int target = to_balance->tokens + tokens;\r\n  if (target < to_balance->tokens) {\r\n    return ERROR_OVERFLOW;\r\n  }\r\n  from_balance->tokens -= tokens;\r\n  to_balance->tokens = target;\r\n  return 0;\r\n}\r\n```\r\n\r\n`_udt_find_balance` here is used to locate `balance_t` data structure given an address, and also create an entry if the address doesn't already exist. Here we omit the full implementation for this function, please refer to CKB codebase for full example.\r\n\r\nFollowing binary code is compiled result of this function:\r\n\r\n```c\r\n00000000 <_udt_find_balance>:\r\n   0:   7179                    addi    sp,sp,-48\r\n   2:   d606                    sw      ra,44(sp)\r\n   4:   d422                    sw      s0,40(sp)\r\n   6:   1800                    addi    s0,sp,48\r\n   8:   fca42e23                sw      a0,-36(s0)\r\n   c:   fcb42c23                sw      a1,-40(s0)\r\n  10:   fcc42a23                sw      a2,-44(s0)\r\n  14:   fcd42823                sw      a3,-48(s0)\r\n  18:   fe042623                sw      zero,-20(s0)\r\n  1c:   57fd                    li      a5,-1\r\n  1e:   fef42423                sw      a5,-24(s0)\r\n  22:   a835                    j       5e <.L2>\r\n\r\n00000024 <.L5>:\r\n  24:   fec42703                lw      a4,-20(s0)\r\n  28:   87ba                    mv      a5,a4\r\n  2a:   078a                    slli    a5,a5,0x2\r\n  2c:   97ba                    add     a5,a5,a4\r\n  2e:   078e                    slli    a5,a5,0x3\r\n  30:   fdc42703                lw      a4,-36(s0)\r\n  34:   97ba                    add     a5,a5,a4\r\n  36:   02000613                li      a2,32\r\n\r\n<omitted ...>\r\n```\r\n\r\nTools will be provided by CKB to encode the binary code here as cell data. Following input contract script can then be used:\r\n\r\n```c\r\ntypedef int *transfer(data_t *, const char*, const char*, int64_t);\r\n\r\nint main(int argc, char* argv[]) {\r\n  data_t *input_data = NULL;\r\n  ret = ckb_read_cell(0, CKB_SOURCE_INPUT, (void **) &input_data, NULL);\r\n  if (ret != 0) {\r\n    return ret;\r\n  }\r\n\r\n  data_t *output_data = NULL;\r\n  ret = ckb_read_cell(1, CKB_SOURCE_OUTPUT, (void **) &output_data, NULL);\r\n  if (ret != 0) {\r\n    return ret;\r\n  }\r\n\r\n  transfer *f = (transfer *) ckb_mmap_cell(function_cell_id, 0, -1, PROT_EXEC);\r\n  ret = f(input_data, from, to, 100);\r\n  if (ret != 0) {\r\n    return ret;\r\n  }\r\n\r\n  if (memcmp(input_data, output_data, sizeof(data_t)) != 0) {\r\n    return -1;\r\n  }\r\n  return 0;\r\n}\r\n```\r\n\r\nWith mmap, we load a cell directly as a callable function, this function is then used to complete the transfer. This way we can ensure contract size stays minimal while reusing the same method across multiple transactions.\r\n\r\n## Multi-function support via dynamic linking\r\n\r\nEven though transfer method is stored as an external cell in the above example, one disadvantage here is that the memory address of the mmapped function is unknown at compile time. As a result, internal implementation within that method can only leverage local jumps. In addition, only one function is supported this way, there's no way to store multiple function in a single cell.\r\n\r\nDynamic linking is provide to solve this problem: assuming we have all UDT functions compiled as a shared library in one cell:\r\n\r\n```c\r\nint udt_initialize(data_t *data, char owner[ADDRESS_LENGTH], int64_t total_supply);\r\nint udt_total_supply(const data_t *data);\r\nint64_t udt_balance_of(data_t *data, const char address[ADDRESS_LENGTH]);\r\nint udt_transfer(data_t *data, const char from[ADDRESS_LENGTH], const char to[ADDRESS_LENGTH], int64_t tokens);\r\nint udt_approve(data_t *data, const char from[ADDRESS_LENGTH], const char spender[ADDRESS_LENGTH], int64_t tokens);\r\nint udt_transfer_from(data_t *data, const char from[ADDRESS_LENGTH], const char spender[ADDRESS_LENGTH], const char to[ADDRESS_LENGTH], int64_t tokens);\r\n```\r\n\r\nWith dynamic linking, following input script can be used:\r\n\r\n```c\r\nint main(int argc, char* argv[])\r\n{\r\n  data_t *input_data = NULL;\r\n  ret = ckb_read_cell(0, CKB_SOURCE_INPUT, (void **) &input_data, NULL);\r\n  if (ret != 0) {\r\n    return ret;\r\n  }\r\n\r\n  data_t *output_data = NULL;\r\n  ret = ckb_read_cell(0, CKB_SOURCE_OUTPUT, (void **) &output_data, NULL);\r\n  if (ret != 0) {\r\n    return ret;\r\n  }\r\n\r\n  if (strcmp(argv[4], \"initialize\") == 0) {\r\n    // processing initialize arguments\r\n    ret = udt_initialize(...);\r\n    if (ret != 0) {\r\n      return ret;\r\n    }\r\n  } else if (strcmp(argv[4], \"transfer\") == 0) {\r\n    // processing transfer arguments\r\n    ret = udt_transfer(input_data, ...);\r\n    if (ret != 0) {\r\n      return ret;\r\n    }\r\n  } else if (strcmp(argv[4], \"approve\") == 0) {\r\n    // processing approve arguments\r\n    ret = udt_approve(input_data, ...);\r\n    if (ret != 0) {\r\n      return ret;\r\n    }\r\n  }\r\n  // more commands here\r\n\r\n  if (memcmp(input_data, output_data, sizeof(data_t)) != 0) {\r\n    return -1;\r\n  }\r\n  return 0;\r\n}\r\n```\r\n\r\nHere all UDT functions are linked dynamically from external cells, current contract can be minimized in terms of size.\r\n","\\0003-ckb-vm\\0003-ckb-vm.zh.md":"---\r\nNumber: \"0003\"\r\nCategory: Informational\r\nStatus: Draft\r\nAuthor: Xuejie Xiao\r\nOrganization: Nervos Foundation\r\nCreated: 2018-08-01\r\n---\r\n\r\n# CKB-VM\r\n\r\n## 概述\r\n\r\nCKB 的 VM 层用于在给定 transaction 的 inputs 与 outputs 的情况下，执行一系列验证条件，以判断 transaction 是否合法并返回结果。\r\n\r\nCKB 使用 [RISC-V](https://riscv.org/) 指令集来实现虚拟机层。更精确的说，CKB 使用 rv64imc 指令集架构：基于 [RV64I](https://riscv.org/specifications/) 核心指令集，并添加 RV32M 整型乘除法扩展以及 RVC 指令压缩功能。注意 CKB 不支持浮点数运算，合约开发者如有需要，可以通过添加 softfloat 实现来完成相应功能。\r\n\r\nCKB 通过动态链接库的方式，依赖 syscall 来实现链上运算所需的其他功能，比如读取 Cell 的内容，或是其他与 block 相关的普通运算及加密运算。任何支持 RV64I 的编译器 (如 [riscv-gcc](https://github.com/riscv/riscv-gcc), [riscv-llvm](https://github.com/lowRISC/riscv-llvm), [Rust](https://github.com/rust-embedded/wg/issues/218)) 生成的可执行文件均可以作为 CKB VM 中的 script 来运行。\r\n\r\n## RISC-V 运行模型\r\n\r\nCKB 中使用 64 位的 RISC-V 虚拟机作为 VM 来执行合约。VM 运行在 64 位地址空间下，提供了 RV32I 定义的核心指令集，以及 RV64M 扩展中的整型乘除法的扩展指令。为减小生成的合约大小，CKB 还支持 RVC 指令压缩功能，尽可能减小指令的存储开销。合约会直接使用 Linux 的 ELF 可执行文件格式，以方便对接开源社区的工具及离线调试。\r\n\r\n每个合约在 gzip 后最大提供 1MB 的存储空间，解压后的原始合约最大限制为 10 MB。合约运行时，CKB 虚拟机会为合约提供 128 MB 的运行空间，其中包含合约可执行文件映射到虚拟机上的代码页，合约运行时需要的栈空间，堆空间以及外部的 Cell 通过 mmap 映射后的地址页。\r\n\r\n为保证合约运行的唯一性及安全性，CKB 虚拟机中的内存及所有寄存器在未被访问之前，均全部写入 0。\r\n\r\n合约的运行等同于 Linux 环境下一个可执行文件在单核 CPU 下的运行：\r\n\r\n```c\r\nint main(int argc, char* argv[]) {\r\n  uint64_t input_cell_length = 10000;\r\n  void *input_cell = malloc(input_cell_length);\r\n  ckb_load_cell(input_cell, &input_cell_length, 0, 0, CKB_SOURCE_INPUT);\r\n\r\n  uint64_t output_cell_length = 10000;\r\n  void *output_cell = malloc(output_cell_length);\r\n  ckb_load_cell(output_cell, &output_cell_length, 0, 0, CKB_SOURCE_OUTPUT);\r\n\r\n  // Consume input & output cell\r\n\r\n  return 0;\r\n}\r\n```\r\n\r\n合约运行从合约 ELF 文件中的 main 函数开始执行，通过 argc 与 argv 提供输入参数进行合约的执行，当 main 函数返回值为 0 时，认为合约执行成功，否则合约执行失败。注意这里的 argc 与 argv 并不保存完整的 inputs 以及 outputs 数据，而是只保留相应的 metadata，对 inputs 与 outputs 的读取则通过单独定义的库与 syscalls 来实现，以便减少不必要的开销。同时 CKB VM 仅为单线程模型，合约文件可以自行提供 coroutine 实现，但是在 VM 层不提供 threading。\r\n\r\n基于简化实现以及确定性的考虑，CKB 不提供浮点数运算。如果有对浮点数的需要，我们建议通过引入 softfloat 来实现需求。同时由于 CKB VM 仅为单线程模型，不提供对于原子性操作的支持。\r\n\r\n## 辅助库与 Bootloader\r\n\r\n为了尽可能减小合约本身的存储开销，CKB 会在 VM 层及 system cell 中提供合约运行所需的辅助库，包括但不限于：libc 中提供的函数，加密库，读写 inputs，outputs 以及其他 Cell 的工具库。所有这些库通过动态链接的形式提供，以确保不占用合约自身的空间。\r\n\r\n与此同时 CKB 会提供定制的简化版 bootloader 用于 gcc, llvm 等编译器的链接步骤，以确保省去不必要的开销。\r\n\r\n在目前的条件下，对于如下最简单的合约 C 代码：\r\n\r\n```c\r\nint main()\r\n{\r\n  return 0;\r\n}\r\n```\r\n\r\n编译后的合约代码大小为 628 字节，gzip 后为 313 字节。可以认为这 313 字节为 RISC-V 合约模型下的固定开销。\r\n\r\n## 开发语言\r\n\r\nCKB 核心只定义了底层的虚拟机模型，理论上任何提供了 RISC-V 后端的语言均可以用来开发 CKB 合约:\r\n\r\n* CKB 可以直接使用标准的 riscv-gcc 以及 riscv-llvm 以 C/C++ 语言来进行开发。编译后的可执行文件可以直接作为 CKB 的合约来使用\r\n* 与此相应的，可以将 C 实现的 Bitcoin 以及 Ethereum VM 编译成 RISC-V 二进制代码，保存在公共 Cell 中，然后在合约中引用公共 Cell 来运行 Bitcoin 或者 Ethereum 的合约\r\n* 其他的高级语言 VM 如 [duktape](http://duktape.org/) 及 [mruby](https://github.com/mruby/mruby) 在编译后，也可以用来相应的运行 JavaScript 或者 Ruby 编写的合约\r\n* 相应的也可以使用 [Rust](https://github.com/riscv-rust/rust) 作为实现语言来编写合约\r\n\r\n## Runtime Cost\r\n\r\nCKB 会选取合适的 RISC-V 开源实现作为运行模型。在执行合约时，可以收集每条指令执行所需的时钟周期。合约执行完毕后，累积的总时钟周期既可作为合约运行的开销。与此同时，我们还会针对读取 Cell 中内容的操作收取合适的运行开销。\r\n\r\n## 示例\r\n\r\n以下通过一个用户自定义代币(user defined token, or UDT)的发行过程来介绍 CKB 中虚拟机的执行过程。需要注意的是，为了简化说明，这里描述的 UDT 实现经过了一定程度的简化：\r\n\r\n* 使用 64 位整数，而不是 256 位整数来保存代币数目\r\n* 使用简化的线性数组与顺序查询的方式代替哈希数据结构存储代币发行情况。同时对代币最多能发给的账户数直接做上限限制\r\n* 同时这里假设所有的账户信息是按字典序顺序排列，于是判断两组数据结构是否相同就简化成了 memcmp 操作，不需要依次遍历数据结构来判断\r\n* 使用 C 的 struct layout 来直接保存数据，省去序列化的步骤\r\n\r\n注意，在生产环境 CKB 不会有以上的假设。\r\n\r\n### 数据结构\r\n\r\n代币信息保存在如下数据结构内：\r\n\r\n```c\r\n#define ADDRESS_LENGTH 32\r\n#define MAX_BALANCES 100\r\n#define MAX_ALLOWED 100\r\n\r\ntypedef struct {\r\n  char address[ADDRESS_LENGTH];\r\n  int64_t tokens;\r\n} balance_t;\r\n\r\ntypedef struct {\r\n  char address[ADDRESS_LENGTH];\r\n  char spender[ADDRESS_LENGTH];\r\n  int64_t tokens;\r\n} allowed_t;\r\n\r\ntypedef struct {\r\n  balance_t balances[MAX_BALANCES];\r\n  int used_balance;\r\n  allowed_t allowed[MAX_ALLOWED];\r\n  int used_allowed;\r\n\r\n  char owner[ADDRESS_LENGTH];\r\n  char newOwner[ADDRESS_LENGTH];\r\n  int64_t total_supply;\r\n} data_t;\r\n```\r\n\r\n对于数据结构有如下的 API 来提供各种操作：\r\n\r\n```c\r\nint udt_initialize(data_t *data, char owner[ADDRESS_LENGTH], int64_t total_supply);\r\nint udt_total_supply(const data_t *data);\r\nint64_t udt_balance_of(data_t *data, const char address[ADDRESS_LENGTH]);\r\nint udt_transfer(data_t *data, const char from[ADDRESS_LENGTH], const char to[ADDRESS_LENGTH], int64_t tokens);\r\nint udt_approve(data_t *data, const char from[ADDRESS_LENGTH], const char spender[ADDRESS_LENGTH], int64_t tokens);\r\nint udt_transfer_from(data_t *data, const char from[ADDRESS_LENGTH], const char spender[ADDRESS_LENGTH], const char to[ADDRESS_LENGTH], int64_t tokens);\r\n```\r\n\r\n这些方法的实现既可以直接编译到合约中，也可以保存在 Cell 中，通过动态链接的方式来提供。以下会分别介绍两种使用方式。\r\n\r\n### 代币发行\r\n\r\n假设 CKB 提供如下的方法用来读取 Cell 中的内容：\r\n\r\n```c\r\nint ckb_read_cell_data(size_t index, size_t source, void** buffer, size_t* size);\r\n```\r\n\r\n即给定 Cell ID，CKB 的虚拟机读取 Cell 中的内容，并映射到当前虚拟机的地址空间中，返回相应的指针，与 Cell 的大小。\r\n\r\n这样就可以通过如下的合约来发行代币：\r\n\r\n```c\r\nint udt_initialize(data_t *data, char owner[ADDRESS_LENGTH], int64_t total_supply)\r\n{\r\n  memset(&data, 0, sizeof(data_t));\r\n  memcpy(data->owner, owner, ADDRESS_LENGTH);\r\n  memcpy(data->balances[0].address, owner, ADDRESS_LENGTH);\r\n\r\n  data->balances[0].tokens = total_supply;\r\n  data->used_balance = 1;\r\n  data->used_allowed = 0;\r\n  data->total_supply = total_supply;\r\n\r\n  return 0;\r\n}\r\n\r\nint main(int argc, char* argv[]) {\r\n  data_t data;\r\n  ret = udt_initialize(&data, \"<i am an owner>\", 10000000);\r\n  if (ret != 0) {\r\n    return ret;\r\n  }\r\n\r\n  data_t *output_data = NULL;\r\n  ret = ckb_read_cell(0, CKB_SOURCE_OUTPUT, (void **) &output_data, NULL);\r\n  if (ret != 0) {\r\n    return ret;\r\n  }\r\n\r\n  if (memcmp(&data, output_data, sizeof(data_t)) != 0) {\r\n    return -1;\r\n  }\r\n  return 0;\r\n}\r\n```\r\n\r\n通过验证 Output Cell 中的数据与自行初始化后的 UDT 代币数据是否一致，这里可以确保当前合约及生成数据均是正确的。\r\n\r\n### 转账\r\n\r\n上述发行代币模型中，验证 Cell 的脚本直接保存在了 input script 中。这里其实也可以通过引用外部 Cell 的方式，调用外部代码来实现验证 Cell 的方法。\r\n\r\n考虑 UDT 代币的转账模型，首先有如下基于 C 的实现：\r\n\r\n```c\r\nint udt_transfer(data_t *data, const char from[ADDRESS_LENGTH], const char to[ADDRESS_LENGTH], int64_t tokens)\r\n{\r\n  balance_t *from_balance = NULL, *to_balance = NULL;\r\n  int ret = _udt_find_balance(data, from, 1, &from_balance);\r\n  if (ret != 0) {\r\n    return ret;\r\n  }\r\n  ret = _udt_find_balance(data, to, 1, &to_balance);\r\n  if (ret != 0) {\r\n    return ret;\r\n  }\r\n  if (from_balance->tokens < tokens) {\r\n    return ERROR_NOT_SUFFICIENT_BALANCE;\r\n  }\r\n  int target = to_balance->tokens + tokens;\r\n  if (target < to_balance->tokens) {\r\n    return ERROR_OVERFLOW;\r\n  }\r\n  from_balance->tokens -= tokens;\r\n  to_balance->tokens = target;\r\n  return 0;\r\n}\r\n```\r\n\r\n其中 `_udt_find_balance` 的作用是给定地址，从当前代币数据结构中找到该地址对应的 `balance_t` 数据结构。如果该地址不存在的话，则在数据结构中创建该地址的条目。在这里我们略去实现，完整的例子可以参考 CKB 代码库。\r\n\r\n可以将该函数编译，得到对应的二进制代码：\r\n\r\n```c\r\n00000000 <_udt_find_balance>:\r\n   0:   7179                    addi    sp,sp,-48\r\n   2:   d606                    sw      ra,44(sp)\r\n   4:   d422                    sw      s0,40(sp)\r\n   6:   1800                    addi    s0,sp,48\r\n   8:   fca42e23                sw      a0,-36(s0)\r\n   c:   fcb42c23                sw      a1,-40(s0)\r\n  10:   fcc42a23                sw      a2,-44(s0)\r\n  14:   fcd42823                sw      a3,-48(s0)\r\n  18:   fe042623                sw      zero,-20(s0)\r\n  1c:   57fd                    li      a5,-1\r\n  1e:   fef42423                sw      a5,-24(s0)\r\n  22:   a835                    j       5e <.L2>\r\n\r\n00000024 <.L5>:\r\n  24:   fec42703                lw      a4,-20(s0)\r\n  28:   87ba                    mv      a5,a4\r\n  2a:   078a                    slli    a5,a5,0x2\r\n  2c:   97ba                    add     a5,a5,a4\r\n  2e:   078e                    slli    a5,a5,0x3\r\n  30:   fdc42703                lw      a4,-36(s0)\r\n  34:   97ba                    add     a5,a5,a4\r\n  36:   02000613                li      a2,32\r\n\r\n<omitted ...>\r\n```\r\n\r\nCKB 会提供工具链，可以将这里的二进制代码直接作为数据生成 Cell，于是可以有如下的 input script:\r\n\r\n```c\r\ntypedef int *transfer(data_t *, const char*, const char*, int64_t);\r\n\r\nint main(int argc, char* argv[]) {\r\n  data_t *input_data = NULL;\r\n  ret = ckb_read_cell(0, CKB_SOURCE_INPUT, (void **) &input_data, NULL);\r\n  if (ret != 0) {\r\n    return ret;\r\n  }\r\n\r\n  data_t *output_data = NULL;\r\n  ret = ckb_read_cell(0, CKB_SOURCE_OUTPUT, (void **) &output_data, NULL);\r\n  if (ret != 0) {\r\n    return ret;\r\n  }\r\n\r\n  transfer *f = (transfer *) ckb_mmap_cell(function_cell_id, 0, -1, PROT_EXEC);\r\n  ret = f(input_data, from, to, 100);\r\n  if (ret != 0) {\r\n    return ret;\r\n  }\r\n\r\n  if (memcmp(input_data, output_data, sizeof(data_t)) != 0) {\r\n    return -1;\r\n  }\r\n  return 0;\r\n}\r\n```\r\n\r\n这里通过 mmap 的方式将一个 Cell 中的内容映射为可以调用的方法，然后调用这个方法来完成转账的目的。这样可以保证方法得到重用，同时也可以减小合约的大小。\r\n\r\n### 多方法支持\r\n\r\n上面的示例中，虽然转账方法放在了 Cell 中，但是这里的验证方法仍然有一个问题：由于方法是直接 mmap 到内存中，在编译期并不知道 mmap 之后方法所处的内存地址，所以方法的内部实现只能使用局部跳转，无法使用全局跳转。同时在一段内存空间内也只能放入一个验证方法，没有办法支持有多个方法的调用库。\r\n\r\n这里我们也可以通过动态链接的方式来使用外部 Cell 提供的辅助库。假设在某一个 Cell 中已经提供了 UDT 代币的所有实现:\r\n\r\n```c\r\nint udt_initialize(data_t *data, char owner[ADDRESS_LENGTH], int64_t total_supply);\r\nint udt_total_supply(const data_t *data);\r\nint64_t udt_balance_of(data_t *data, const char address[ADDRESS_LENGTH]);\r\nint udt_transfer(data_t *data, const char from[ADDRESS_LENGTH], const char to[ADDRESS_LENGTH], int64_t tokens);\r\nint udt_approve(data_t *data, const char from[ADDRESS_LENGTH], const char spender[ADDRESS_LENGTH], int64_t tokens);\r\nint udt_transfer_from(data_t *data, const char from[ADDRESS_LENGTH], const char spender[ADDRESS_LENGTH], const char to[ADDRESS_LENGTH], int64_t tokens);\r\n```\r\n\r\n于是可以在编译期时直接指定链接方式为动态链接，这样便可以有如下的 input script:\r\n\r\n```c\r\nint main(int argc, char* argv[])\r\n{\r\n  data_t *input_data = NULL;\r\n  ret = ckb_read_cell(0, CKB_SOURCE_INPUT, (void **) &input_data, NULL);\r\n  if (ret != 0) {\r\n    return ret;\r\n  }\r\n\r\n  data_t *output_data = NULL;\r\n  ret = ckb_read_cell(0, CKB_SOURCE_OUTPUT, (void **) &output_data, NULL);\r\n  if (ret != 0) {\r\n    return ret;\r\n  }\r\n\r\n  if (strcmp(argv[4], \"initialize\") == 0) {\r\n    // processing initialize arguments\r\n    ret = udt_initialize(...);\r\n    if (ret != 0) {\r\n      return ret;\r\n    }\r\n  } else if (strcmp(argv[4], \"transfer\") == 0) {\r\n    // processing transfer arguments\r\n    ret = udt_transfer(input_data, ...);\r\n    if (ret != 0) {\r\n      return ret;\r\n    }\r\n  } else if (strcmp(argv[4], \"approve\") == 0) {\r\n    // processing approve arguments\r\n    ret = udt_approve(input_data, ...);\r\n    if (ret != 0) {\r\n      return ret;\r\n    }\r\n  }\r\n  // more commands here\r\n\r\n  if (memcmp(input_data, output_data, sizeof(data_t)) != 0) {\r\n    return -1;\r\n  }\r\n  return 0;\r\n}\r\n```\r\n\r\n这里所有的 UDT 函数均通过动态链接的方式引用其他 Cell 里的内容，不占用当前 Cell 的空间。\r\n","\\0004-ckb-block-sync\\0004-ckb-block-sync.md":"---\r\nNumber: \"0004\"\r\nCategory: Standards Track\r\nStatus: Proposal\r\nAuthor: Ian Yang\r\nOrganization: Nervos Foundation\r\nCreated: 2018-07-25\r\n---\r\n\r\n# CKB Block Synchronization Protocol\r\n\r\nGlossary of Terms\r\n\r\n- Chain: a list of blocks starting with genesis block and consisted of successive blocks.\r\n- Best Chain: a chain with the most accumulated PoW, and starting with a common genesis block which nodes agree with the consensus.\r\n- Best Header Chain: a chain with the most PoW and consisted only of blocks in the status of Connected, Downloaded or Accepted. Please refer to block status part for more details.\r\n- Tip: the latest block of a chain and Tip can be used to determine a specific chain. \r\n- Best Chain Tip: the tip of Best Chain.\r\n\r\n## Abstract\r\n\r\nBlock synchronization **must** be performed in stages with [Bitcoin Headers First](https://bitcoin.org/en/glossary/headers-first-sync) style. Block is downloaded in parts in each stage and is validated using the obtained parts.\r\n\r\n1. Connecting Header: Get block header, and validate format and PoW.\r\n2. Downloading Block: Get and validate the complete block. Transactions in ancestor blocks are not required.\r\n3. Accepting Block: Validate the block in the context of the chain. \r\n\r\nThe purpose of stage execution is trying to preclude most of the attacks with the least cost. For example, in the first step, header connecting only accounts for 5% workload while there would be 95% possibility to say the block is valid. \r\n\r\nAccording to the execution stages, there is 5 status of blocks:\r\n\r\n1. Unknown: the status of a block is unknown before header connecting.\r\n2. Invalid: A block and all of its descendant blocks are marked as 'Invalid' if any above steps failed.\r\n3. Connected: A block succeeds in stage Connecting Header, and all its ancestor blocks are in a status of connected, downloaded or accepted.\r\n4. Downloaded: A block succeeds in stage Downloading Block, and all its ancestor blocks are in a status of downloaded or accepted.\r\n5. Accept: A block succeeds in stage Accepting Block, and all its ancestor blocks are in the status of accepted.\r\n\r\nBlock status is propagated from the previous block to the later ones. Using the list index number above, the status number of a block is always less than or equal to its parent block. Here are conditions, if a block is invalid, all of its descendant blocks must be invalid. The cost of every step for synchronization is higher than the previous one and every step may fail. In this scenario, work will be wasted if a child block enters a later status before its parent block, and parent block is approved to be Invalid later.\r\n\r\nInitially, Genesis block is in status Accepted and the rest is in status Unknown.\r\n\r\nBelow figures are used to indicate blocks in different status later on. \r\n\r\n![](images/block-status.jpg \"Block Status\")\r\n\r\nGenesis block of the nodes synchronizing **must be** the same, and all blocks can be constructed as a tree with the genesis block being the root. Blocks will be removed if they cannot connect to the root eventually. \r\n\r\nEvery participating node forms its local status tree where the chain consisting of Accepted blocks with the most PoW is considered as Best Chain. The chain that consists of blocks in the status of connected, downloaded or accepted with the most PoW is Best Header Chain. \r\n\r\nThe graph below is an example of Status Tree formed by Alice and blocks signed with name Alice is this node's current Best Chain Tip. \r\n\r\n![](images/status-tree.jpg \"Status Tree by Alice\")\r\n \r\n## Connecting Header\r\n\r\nSynchronizing headers first helps to validate PoW with the least cost. Since it costs the same work to construct PoW whether the included transactions are valid or not, attackers may use other more efficient ways. It means it's highly possible to regard the whole block as valid when the PoW is valid. This is why headers synchronization first would avoid resource-wasting on invalid blocks.\r\n\r\nBecause of the low cost, Headers synchronization can be processed in parallel with all peers and construct a highly reliable global graph. In this way, block downloading can be scheduled in the most efficient way to avoid wasting resource on lower PoW branch.\r\n\r\nThe goal of connecting header is demonstrated using the following example. When Alice connects to Bob, Alice asks Bob to send all block headers in Bob's Best Chain but not in Alice's **Best Header Chain** and then validate them to decide the blocks status are either Connected or Invalid. \r\n\r\nWhen Alice connects header, keeping Best Header Chain Tip updated could help to decrease numbers of receiving headers already existed.\r\n\r\n![](images/seq-connect-headers.jpg)\r\n\r\nThe graph above instructs the process of connecting headers. After a round of connecting headers, nodes are supposed to keep up-to-date using new block notification.\r\n\r\nTake Alice and Bob above as an example, firstly Alice samples blocks from her Best Header Chain and sent the hashes to Bob. The basic principle of sampling is that later blocks are more possible to be selected than early blocks. For example, choose latest 10 blocks from the chain, then sample other blocks backward with 2's exponential increased intervals, a.k.a, 2, 4, 8, and etc. The list of hashes of the sampled blocks is called a Locator. In the following figure, the undimmed blocks are sampled. The genesis block should be always in the Locator. \r\n\r\n![](images/locator.jpg)\r\n\r\nBob can get the latest common block between these two chains according to Locator and his own Best Chain. Because the genesis block is identical, there must be such kind of block. Bob will send all block headers from the common block to his Best Chain Tip to Alice. \r\n\r\n![](images/connect-header-conditions.jpg)\r\n\r\nIn the figure above, blocks with undimmed color should be sent from Bob to Alice, and golden bordered one is the latest common block. There are three possible cases in the process:\r\n\r\n1. If Bob's Best Chain Tip is in Alice's Best Header Chain, the latest common block will be Bob's Best Chain Tip and there are no block headers for Bob to send.\r\n2. If Alice's Best Header Chain Tip is in Bob's Best Chain but is not the Tip, the latest common block will be Alice's Best Header Chain Tip.\r\n3. If Alice's Best Header Chain and Bob's Best Chain fork, the latest common block will be the one before the fork occurs.\r\n\r\nIf there are too many blocks to send, pagination is required. Bob sends the first page, Alice will ask Bob for the next page if she finds out that there are more block headers. A simple pagination solution is to limit the maximum number of block headers returned each time, 2000 for example. If the number of block headers returned is equal to 2000, it means there may be other blocks could be returned. If the last block of a certain page is the ancestor of Best Chain Tip or Best Header Chain Tip, it can be optimized to get next page starting with the corresponding tip.\r\n\r\nAlice could observe Bob's present Best Chain Tip, which is the last block received during each round of synchronization. If Alice's Best Header Chain Tip is exactly Bob's Best Chain Tip, Alice couldn't observe Bob's present Best Chain because Bob has no block headers to send. Therefore, it should start building from the parent block of Best Header Chain Tip when sending the first request in each round.\r\n\r\nIn the following cases, a new round of connection block header synchronization must be performed. \r\n\r\n- Received a new block notification from the others, but the parent block status of the new block is Unknown.\r\n\r\nThe following exceptions may occur when connecting a block header: \r\n\r\n- Alice observed that Bob's Best Chain Tip has not been updated for a long time, or its timestamp is old. In this case, Bob does not provide valuable data. When the number of connections reaches a limit, this peer could be disconnected first.\r\n- Alice observed that the status of Bob's Best Chain Tip is Invalid. This can be found in any page without waiting for the end of a round of Connect Head. There, Bob is on an invalid branch, Alice can stop synchronizing with Bob and add Bob to the blacklist.\r\n- There are two possibilities if the block headers Alice received are all on her own Best Header Chain. One is that Bob sends them deliberately. The other is that Best Chain changes when Alice wants to Connect Head. In this case, those block headers can only be ignored because they are difficult to distinguish. However, the proportion of received blocks already in Best Header Chain would be recorded. If the proportion is above a certain threshold value, Bob may be added to the blacklist.\r\n\r\nUpon receiving the block header message, the format should be verified first.\r\n\r\n- The blocks in the message are continuous.\r\n- The status of all blocks and the parent block of the first block are not Invalid in the local Status Tree. \r\n- The status of the parent block of the first block is not Unknown in the local Status Tree, which means Orphan Block will not be processed in synchronizing.\r\n\r\nIn this stage, verification includes checking if block header satisfies the consensus rules and if Pow is valid or not. Since Orphan Blocks are not processed, difficulty adjustment can be verified as well. \r\n\r\n![](images/connect-header-status.jpg)\r\n\r\nThe figure above is the Status Tree of Alice after synchronized with Bob, Charlie, Davis, Elsa. The observed Best Chain Tip of each peer is also annotated in the figure.\r\n\r\nIf the Unknown status block is considered not on the Status Tree, new blocks in the status of Connected or Invalid will be extended to the leaves of the Status Tree during Connecting Header. As a result, Connecting Header stage explores and extends the status tree.\r\n\r\n## Downloading Block\r\n\r\nAfter Connecting Header is completed, the branch of some observed Best Chain Tip ends with one or more Connected block, a.k.a., Connected Chain. Downloading Block stage should start to request complete blocks from peers and perform verification.\r\n\r\nWith the status tree, synchronization can be scheduled to avoid useless work. An effective optimization is to download the block only if the Best Chain of the observed peer is better than the local Best Chain's. And priority can be ordered that the connected chain with more accumulated PoW should be processed first. Only when a branch is approved to be invalid, or the download times out, the branch with lower PoW can be tried.\r\n\r\nWhen downloading a branch, earlier blocks should be downloaded firstly due to the dependency of blocks; and should be downloaded concurrently from different peers to utilize full bandwidth. A sliding window can be applied to solve the problem.\r\n\r\nAssume that the number of the first Connected status block to be downloaded is M and the length of the sliding window is N, then only the blocks numbered M to M+N-1 can be downloaded. After the block M is downloaded and verified, the sliding window moves to the next Connected block. If verification of block M fails, then the remaining blocks of this branch are all Invalid, and there is no need to continue downloading. If the window does not move towards the right for a long time, it is considered as time out. The node should try again later, or waits until the branch has new connected blocks.\r\n\r\n![](images/sliding-window.jpg)\r\n\r\nThe figure above is an example of an 8 length sliding window. In the beginning, the downloadable block range from 3 to 10. After block 3 is downloaded,  the window will move to block 5 because block 4 has already been downloaded in advance (as the figure above illustrated).\r\n\r\nThe Best Chains of peers are already known in stage Connecting Header, it is assumed that the peer has a block if it is in the peer's Best Chain and that peer is a full node. During the downloading, blocks in the sliding window can be split into several small stripes and those stripes could be scheduled among peers who have the blocks.\r\n\r\nThe downloaded transactions in a block may be mismatched with the Merkle Hash Root in the header, or the list contains duplicated txid. It doesn't mean that the block is invalid since it can only approve the downloaded block is incorrect. The block content provider could be added to the blacklist, but the block status should not be marked as invalid. Otherwise, the malicious nodes may pollute the nodes' Status Tree by sending the wrong block contents.\r\n\r\nVerification of transaction lists and block header matching is required in this stage, but any validation that relies on the transaction contents in the ancestor block is not required, which will be placed in the next stage.\r\n\r\nSeveral validations can be checked in this phase, for example, Merkle Hash validation, transaction txid cannot be repeated, transaction list cannot be empty, inputs and outputs cannot be blank at the same time, or only the first transaction can be generation transaction, etc.\r\n\r\nDownloading Block will update the status of blocks in the best Connected Chain, from Connected to Downloaded or Invalid.\r\n\r\n## Accepting Block\r\n\r\nIn the previous stage, there will be some chains which ended with one or more Downloaded status, hereinafter referred to as Downloaded Chain. If those chains' cumulative work is more than Best Chain Tip's, the complete validation in the chain context should be performed in this stage. If there are more than one chains satisfied, the chain with the most work should be performed first.\r\n\r\nAll the verification must be completed in this stage, including all rules that depend on historical transactions.\r\n\r\nBecause it involves UTXO (unspent transaction outputs) indexes, the cost of verification is huge in this phase. One set of UTXO indexes is sufficient in this simple solution. First rollback local Best Chain Tip necessarily. After that, verify blocks in the candidate best Downloaded Chain and add them to Best Chain one by one. If there is an invalid block during verification, the remain blocks in Downloaded Chain are also considered as Invalid. If so, Best Chain Tip would even have lower work than the previous Tip. It can be resolved in several different ways:\r\n\r\n- If the work of Best Chain before rollback is more than present Tip, then restore the previous Best Chain.\r\n- If the work of other Downloaded Chains is more than Best Chain that before rollback, try rollback and relocate to that chain.\r\n\r\nThe process of Accepting Block will change the status of blocks in the Downloaded chain, from Downloaded to Accepted or Invalid. The verified Downloaded Chain which has the most work will become the new local Best Chain.\r\n\r\n## New block announcement\r\n\r\nWhen the local Best Chain Tip changes, the node should push an announcement to peers. The best header with most cumulative work sent to each peer should be recorded, to avoid sending duplicate blocks in the announcement and sending blocks only peer doesn't know. This does not only record headers sent for new blocks, but also the ones sent as the responses in stage Connecting Header.\r\n\r\nIt is assumed that the peers already know the Best Sent Header and its ancestors, so these blocks can be excluded when sending new block announcements.\r\n\r\n![](images/best-sent-header.jpg \"Best Sent Header\")\r\n\r\nFrom the above example, Alice's Best Chain Tip is annotated with her name. The best header sent to Bob is annotated as \"Best Sent To Bob\". The undimmed blocks are the ones Alice should send to Bob as new blocks announcement. Following is the detailed description for each step:\r\n\r\n1. In the beginning, Alice only has Best Chain Tip to send\r\n2. Another new block is added to the best chain before Alice has a chance to send the headers. In this case, the last two blocks of Best Chain need to be sent.\r\n3. Alice sends the last two blocks to Bob and updates Best Sent to Bob.\r\n4. Alice's Best Chain relocates to another fork. Only blocks after the last common block should be sent to Bob.\r\n\r\nHow to send the announcement is determined by connection negotiated parameters and the number of new blocks to be announced:\r\n\r\n- If there is only one block and the peer prefers Compact Block [^1], then use Compact Block.\r\n- In other cases, just send block header list with an upper limit on the number of blocks to send. For example, if the limit is 8 and there are 8 or more blocks need to be announced, only the latest 7 blocks will be announced.\r\n\r\nWhen receiving a new block announcement, there may be a situation the parent block's status is Unknown, also called Orphan Block. If so, a new round of Connecting Header is required immediately. When a Compact Block is received, and its parent block is the local Best Chain Tip, then the full block may be recovered from the transaction pool. If the recovery succeeds, the work of these three stages can be compacted into one. Otherwise, it falls back to a header-only announcement.\r\n\r\n## Synchronization Status \r\n\r\n### Configuration\r\n- `GENESIS_HASH`: hash of genesis block\r\n- `MAX_HEADERS_RESULTS`: the max number of block headers can be sent in a single message\r\n- `MAX_BLOCKS_TO_ANNOUNCE`: the max number of new blocks to be announced\r\n- `BLOCK_DOWNLOAD_WINDOW`: the size of the download window\r\n\r\n### Storage\r\n- Block Status Tree\r\n- Best Chain Tip, decide whether to download blocks and accept blocks\r\n- Best Header Chain Tip, used in Connecting Header to construct the Locator of the first request in each round.\r\n\r\nEach connection peer should store:\r\n- Observed Best Chain Tip\r\n- The block header hash with the most work sent last time —— Best Sent Header\r\n\r\n### Message Definition\r\n\r\nOnly related message and fields are listed here. See completed definition and documentation in the reference implementation.\r\n\r\nThe message passing is completely asynchronous. For example, sending `getheaders` does not block other requests. Also, there is no need to guarantee the order relationship between the requests and the responses. For example, node A sends `getheaders` and `getdata` to B, and B can replies `block` firstly, and then `headers` to A.\r\n\r\nCompact Block [^1] messages `cmpctblock` and `getblocktxn` will be described in related Compact Block documentation.\r\n\r\n### getheaders\r\n\r\nIt is used to request a block header from a peer in stage Connecting Header. The first-page request, and subsequent pages request can share the same getheaders message format. The difference between them is that the first page requests generate a Locator from the parent block of the local Best Header Chain Tip, and the subsequent page request generates the Locator using the last block in the last received page.\r\n\r\n- `locator`: Sampled hashes of the already known blocks\r\n\r\n### headers\r\n\r\nIt is used to reply `getheaders` and announce new blocks. There is no difference in processing logic, but if an Orphan Block is founded when the number of block headers is less than `MAX_BLOCKS_TO_ANNOUNCE`, a new round of Connecting Header is required. If the number of block `headers` received equals is equal to `MAX_HEADERS_RESULTS`, it indicates that there are more blocks to request.\r\n\r\n- `headers`：block headers list\r\n\r\n### getdata\r\n\r\nIt is used in Downloading Block stage.\r\n\r\n- `inventory`:  object lists for download, with following fields in each list element:\r\n    - `type`: type of the object, only \"block\" here\r\n    - `hash`: hash of the object as identity\r\n\r\n### block\r\n\r\nIt is used to reply block downloading request of `getdata` \r\n\r\n- `header` block header\r\n- `transactions` transaction list\r\n\r\n[^1]: Compact Block is a technique for compressing and transferring complete blocks. It is based on the fact that when a new block is propagated, the transactions should already be in the pool of other nodes. Under this circumstances, Compact Block only contains the list of transaction txid list and complete transactions which are predicated unknown to the peers. The receiver can recover the complete block using the transaction pool. Please refer to Compact Block RFC (TODO: link to RFC) and related Bitcoin [BIP](https://github.com/bitcoin/bips/blob/master/bip-0152.mediawiki) for details.\r\n","\\0004-ckb-block-sync\\0004-ckb-block-sync.zh.md":"---\r\nNumber: \"0004\"\r\nCategory: Standards Track\r\nStatus: Proposal\r\nAuthor: Ian Yang\r\nOrganization: Nervos Foundation\r\nCreated: 2018-07-25\r\n---\r\n\r\n# 链同步协议\r\n\r\n术语说明\r\n\r\n- Chain: 创世块开头，由连续的块组成的链。\r\n- Best Chain: 节点之间要达成最终一致的、满足共识验证条件的、PoW 累积工作量最高的、以共识的创世块开始的 Chain。\r\n- Best Header Chain: 累积工作量最高，由状态是 Connected, Downloaded 或者 Accepted 的块组成的 Chain。详见下面块状态的说明。\r\n- Tip: Chain 最后一个块。Tip 可以唯一确定 Chain。\r\n- Best Chain Tip: Best Chain 的最后一个块。\r\n\r\n## 同步概览\r\n\r\n块同步**必须**分阶段进行，采用 [Bitcoin Headers First](https://bitcoin.org/en/glossary/headers-first-sync) 的方式。每一阶段获得一部分块的信息，或者基于已有的块信息进行验证，或者两者同时进行。\r\n\r\n1.  连接块头 (Connect Header): 获得块头，验证块头格式正确且 PoW 工作量有效\r\n2.  下载块 (Download Block): 获得块内容，验证完整的块，但是不依赖祖先块中的交易信息。\r\n3.  采用块 (Accept Block): 在链上下文中验证块，会使用到祖先块中的交易信息。\r\n\r\n分阶段执行的主要目的是先用比较小的代价排除最大作恶的可能性。举例来说，第一步连接块头的步骤在整个同步中的工作量可能只有 5%，但是完成后能有 95% 的可信度认为块头对应的块是有效的。\r\n\r\n按照已经执行的阶段，块可以处于以下 5 种状态：\r\n\r\n1.  Unknown: 在连接块头执行之前，块的状态是未知的。\r\n2.  Invalid：任意一步失败，块的状态是无效的，且当一个块标记为 Invalid，它的所有子孙节点也都标记为 Invalid。\r\n3.  Connected: 连接块头成功，且该块到创世块的所有祖先块都必须是 Connected, Downloaded 或 Accepted 的状态。\r\n4.  Downloaded: 下载块成功，且该块到创世块的所有祖先块都必须是 Downloaded 或者 Accepted 的状态。\r\n5.  Accepted: 采用块成功，且该块到创世块的所有祖先块都必须是 Accepted 的状态。\r\n\r\n块的状态是会沿着依赖传递的。按照上面的编号，子块的状态编号一定不会大于父块的状态编号。首先，如果某个块是无效的，那依赖它的子孙块自然也是无效的。另外，同步的每一步代价都远远高于前一步，且每一步都可能失败。如果子节点先于父节点进入下一阶段，而父节点被验证为无效，那子节点上的工作量就浪费了。而且，子块验证是要依赖父块的信息的。\r\n\r\n初始时创世块状态为 Accepted，其它所有块为 Unknown。\r\n\r\n之后会使用以下图示表示不同状态的块：\r\n\r\n![](images/block-status.jpg \"Block Status\")\r\n\r\n参与同步的节点创世块**必须**相同，所有的块必然是组成由创世块为根的一颗树。如果块无法最终连接到创世块，这些块都可以丢弃不做处理。\r\n\r\n参与节点都会在本地构造这颗状态树，其中全部由 Accepted 块组成的累积工作量最大的链就是 Best Chain。而由状态可以是 Connected, Downloaded 或 Accepted 块组成的累积工作量最大的链就是 Best Header Chain.\r\n\r\n下图是节点 Alice 构造的状态树的示例，其中标记为 Alice 的块是该节点当前的 Best Chain Tip。\r\n\r\n![](images/status-tree.jpg \"Status Tree by Alice\")\r\n\r\n## 连接块头\r\n\r\n先同步 Headers 可以用最小的代价验证 PoW 有效。构造 PoW 时，不管放入无效的交易还是放入有效的交易都需要付出相同的代价，那么攻击者会选择其它更高性价比的方式进行攻击。可见，当 PoW 有效时整个块都是有效的概率非常高。所以先同步 Headers 能避免浪费资源去下载和验证无效块。\r\n\r\n因为代价小，同步 Headers 可以和所有的节点同时进行，在本地能构建出可信度非常高的、当前网络中所有分叉的全局图。这样可以对块下载进行规划，避免浪费资源在工作量低的分支上。\r\n\r\n连接块头这一步的目标是，当节点 Alice 连接到节点 Bob 之后，Alice 让 Bob 发送所有在 Bob 的 Best Chain 上但不在 Alice 的 **Best Header Chain** 上的块头，进行验证并确定这些块的状态是 Connected 还是 Invalid。\r\n\r\nAlice 在连接块头时，需要保持 Best Header Chain Tip 的更新，这样能减少收到已有块头的数量。\r\n\r\n![](images/seq-connect-headers.jpg)\r\n\r\n上图是一轮连接块头的流程。完成了一轮连接块头后，节点之间应该通过新块通知保持之后的同步。\r\n\r\n以上图 Alice 从 Bob 同步为例，首先 Alice 将自己 Best Header Chain 中的块进行采样，将选中块的哈希作为消息内容发给 Bob。采样的基本原则是最近的块采样越密，越早的块越稀疏。比如可以取最后的 10 个块，然后从倒数第十个块开始按 2, 4, 8, … 等以 2 的指数增长的步长进行取样。采样得到的块的哈希列表被称为 Locator。下图中淡色处理的是没有被采样的块，创世块应该始终包含在 Locator 当中。\r\n\r\n![](images/locator.jpg)\r\n\r\nBob 根据 Locator 和自己的 Best Chain 可以找出两条链的最后一个共同块。因为创世块相同，所以一定存在这样一个块。Bob 把共同块之后一个开始到 Best Chain Tip 为止的所有块头发给 Alice。\r\n\r\n![](images/connect-header-conditions.jpg)\r\n\r\n上图中未淡出的块是 Bob 要发送给 Alice 的块头，金色高亮边框的是最后共同块。下面列举了同步会碰到的三种情况：\r\n\r\n1.  Bob 的 Best Chain Tip 在 Alice 的 Best Header Chain 中，最后共同块就是 Bob 的 Best Chain Tip，Bob 没有块头可以发送。\r\n2.  Alice 的 Best Header Chain Tip 在 Bob 的 Best Chain 中并且不等于 Tip，最后共同块就是 Alice 的 Best Header Chain Tip。\r\n3.  Alice 的 Best Header Chain 和 Bob 的 Best Chain 出现了分叉，最后共同块是发生发叉前的块。\r\n\r\n如果要发送的块很多，需要做分页处理。Bob 先发送第一页，Alice 通过返回结果发现还有更多的块头就继续向 Bob 请求接下来的页。一个简单的分页方案是限制每次返回块头的最大数量，比如 2000。如果返回块头数量等于 2000，说明可能还有块可以返回，就接着请求之后的块头。如果某页最后一个块是 Best Header Chain Tip 或者 Best Chain Tip 的祖先，可以优化成用对应的 Tip 生成 Locator 发送请求，减少收到已有块头的数量。\r\n\r\n在同步的同时，Alice 可以观察到 Bob 当前的 Best Chain Tip，即在每轮同步时最后收到的块。如果 Alice 的 Best Header Chain Tip 就是 Bob 的 Best Chain Tip ，因为 Bob 没有块头可发，Alice 就无法观测到 Bob 目前的 Best Chain。所以在每轮连接块头同步的第一个请求时，**应该**从 Best Header Chain Tip 的父块开始构建，而不包含 Tip。\r\n\r\n在下面的情况下**必须**做新一轮的连接块头同步。\r\n\r\n- 收到对方的新块通知，但是新块的父块状态时 Unknown\r\n\r\n连接块头时可能会出现以下一些异常情况：\r\n\r\n- Alice 观察到的 Bob Best Chain Tip 很长一段时间没有更新，或者时间很老。这种情况 Bob 无法提供有价值的数据，当连接数达到限制时，可以优先断开该节点的连接。\r\n- Alice 观察到的 Bob Best Chain Tip 状态是 Invalid。这个判断不需要等到一轮 Connect Head 结束，任何一个分页发现有 Invalid 的块就可以停止接受剩下的分页了。因为 Bob 在一个无效的分支上，Alice 可以停止和 Bob 的同步，并将 Bob 加入到黑名单中。\r\n- Alice 收到块头全部都在自己的 Best Header Chain 里，这有两种可能，一是 Bob 故意发送，二是 Alice 在 Connect Head 时 Best Chain 发生了变化，由于无法区分只能忽略，但是可以统计发送的块已经在本地 Best Header Chain 上的比例，高于一定阈值可以将对方加入到黑名单中。\r\n\r\n在收到块头消息时可以先做以下格式验证：\r\n\r\n- 消息中的块是连续的\r\n- 所有块和第一个块的父块在本地状态树中的状态不是 Invalid\r\n- 第一个块的父块在本地状态树中的状态不是 Unknown，即同步时不处理 Orphan Block。\r\n\r\n这一步的验证包括检查块头是否满足共识规则，PoW 是否有效。因为不处理 Orphan Block，难度调整也可以在这里进行验证。\r\n\r\n![](images/connect-header-status.jpg)\r\n\r\n上图是 Alice 和 Bob, Charlie, Davis, Elsa 等节点同步后的状态树情况和观测到的其它节点的 Best Chain Tip。\r\n\r\n如果认为 Unknown 状态块是不在状态树上的话，在连接块头阶段，会在状态树的末端新增一些 Connected 或者 Invalid 状态的节点。所以可以把连接块头看作是拓展状态树，是探路的阶段。\r\n\r\n## 下载块\r\n\r\n完成连接块头后，一些观测到的邻居节点的 Best Chain Tip 在状态树上的分支是以一个或者多个 Connected 块结尾的，即 Connected Chain，这时可以进入下载块流程，向邻居节点请求完整的块，并进行必要的验证。\r\n\r\n因为有了状态树，可以对同步进行规划，避免做无用工作。一个有效的优化就是只有当观测到的邻居节点的 Best Chain 的累积工作量大于本地的 Best Chain 的累积工作量才进行下载块。而且可以按照 Connected Chain 累积工作量为优先级排序，优先下载累积工作量更高的分支，只有被验证为 Invalid 或者因为下载超时无法进行时才去下载优先级较低的分支。\r\n\r\n下载某个分支时，因为块的依赖性，应该优先下载更早的块；同时应该从不同的节点去并发下载，充分利用带宽。这可以使用滑动窗口解决。\r\n\r\n假设分支第一个要下载的 Connected 状态块号是 M，滑动窗口长度是 N，那么只去下载 M 到 M + N - 1 这 N 个块。在块 M 下载并验证后，窗口往右移动到下一个 Connected 状态的块。如果块 M 验证失败，则分支剩余的块也就都是 Invalid 状态，不需要继续下载。如果窗口长时间没有向右移动，则可以判定为下载超时，可以在尝试其它分支之后再进行尝试，或者该分支上有新增的 Connected 块时再尝试。\r\n\r\n![](images/sliding-window.jpg)\r\n\r\n上图是一个长度为 8 的滑动窗口的例子。开始时可下载的块是从 3 到 10。块 3 下载后，因为 4 已经先下载好了，所以窗口直接滑动到从 5 开始。\r\n\r\n因为通过连接块头已经观测到了邻居节点的 Best Chain，如果在对方 Best Chain 中且对方是一个全节点，可以认为对方是能够提供块的下载的。在下载的时候可以把滑动窗口中的块分成小块的任务加到任务队列中，在能提供下载的节点之间进行任务调度。\r\n\r\n下载块如果出现交易对不上 Merkle Hash Root 的情况，或者能对上但是有重复的交易 txid 的情况，并不能说明块是无效，只是没有下载到正确的块内容。可以将对方加入黑名单，但是不能标记块的状态为 Invalid，否则恶意节点可以通过发送错误的块内容来污染节点的状态树。\r\n\r\n这一阶段需要验证交易列表和块头匹配，但是不需要做任何依赖祖先块中交易内容的验证，这些验证会放在下一阶段进行。\r\n\r\n可以进行的验证比如 Merkel Hash 验证、交易 txid 不能重复、交易列表不能为空、所有交易不能 inputs outputs 同时为空、只有第一个交易可以是 generation transaction 等等。\r\n\r\n下载块会把状态树中工作量更高的 Connected Chain 中的 Connected 块变成 Downloaded 或者 Invalid。\r\n\r\n## 采用块\r\n\r\n在上一阶段中会产生一些以一个或多个 Downloaded 状态的块结尾的链，以下简称为 Downloaded Chain。如果这些链的累积工作量大于 Best Chain Tip， 就可以对这条链进行该阶段完整的合法性验证。如果有多个这样的链，选取累积工作量最高的。\r\n\r\n这一阶段需要完成所有剩余的验证，包括所有依赖于历史交易内容的规则。\r\n\r\n因为涉及到 UTXO (未消耗掉的交易 outputs) 的索引，这一步的验证开销是非常大的。为了简化系统，可以只保留一套 UTXO 索引，尝试将本地的 Best Chain Tip 进行必要回退，然后将 Downloaded Chain 上的块进行一次验证，再添加到 Best Chain 上。如果中间有块验证失败则 Downloaded Chain 上剩余的块也就都是 Invalid 状态不需要再继续。这时 Best Chain Tip 甚至会低于之前的 Tip，如果遇到可以采取以下的方案处理：\r\n\r\n- 如果回退之前的 Best Chain 工作量比当前 Tip 更高，恢复之前的 Best Chain\r\n- 如果有其它 Downloaded Chain 比回退之前的 Best Chain 工作量更高，可以继续使用下一个 Downloaded Chain 进行采用块的步骤。\r\n\r\n采用块会将工作量更高的 Downloaded Chain 中的 Downloaded 状态块变成 Accepted 或者 Invalid，而累积工作量最高的 Downloaded Chain 应该成为本地的 Best Chain。\r\n\r\n## 新块通知\r\n\r\n当节点的 Best Chain Tip 发生变化时，应该通过推送的方式主动去通知邻居节点。为了避免通知重复的块，和尽量一次性发送邻居节点没有的块，可以记录给对方发送过的累积工作量最高的块头 (Best Sent Header)。发送过不但指发送过新块通知，也包括发送过在连接块头时给对方的块头的回复。\r\n\r\n因为可以认为对方节点已经知道 Best Sent Header，及其祖先节点，所以发送新块通知时可以排除掉这些块。\r\n\r\n![](images/best-sent-header.jpg \"Best Sent Header\")\r\n\r\n上面的例子中标记为 Alice 的块是节点 Alice 的 Best Chain Tip。标记为 Best Sent to Bob 是记录的发送给 Bob 工作量最高的块头。其中未淡化的块是 Alice 需要通知给 Bob 的新块。数字对应的每一步说明如下：\r\n\r\n1. 开始时 Alice 只有 Best Chain Tip 需要发送\r\n2. Alice 还没有来得及发送，就又多了一个新块，这时需要发送 Best Chain 最后两个块头\r\n3. Alice 将最后两个块头发送给了 Bob 并同时更新了 Best Sent to Bob\r\n4. Alice 的 Best Chain 发生了分支切换，只需要发送和 Best Sent to Bob 最后共同块之后的块。\r\n\r\n基于连接的协商参数和要通知的新块数量：\r\n\r\n- 数量为 1 且对方偏好使用 Compact Block [^1]，则使用 Compact Block\r\n- 其它情况直接发送块头列表，但要限制发送块的数量不超过某个阈值，比如 8，如果有 8 个或更多的块要通知，只通知最新的 7 个块。\r\n\r\n当收到新块通知时，会出现父块状态是 Unknown 的情况，即 Orphan Block，这个时候需要立即做一轮连接块头的同步。收到 Compact Block 且父块就是本地的 Best Chain Tip 的时候可以尝试用交易池直接恢复，如果恢复成功，直接可以将三阶段的工作合并进行，否则就当作收到的只是块头。\r\n\r\n## 同步状态\r\n\r\n### 配置\r\n\r\n- `GENESIS_HASH`: 创世块哈希\r\n- `MAX_HEADERS_RESULTS`: 一条消息里可以发送块头的最大数量\r\n- `MAX_BLOCKS_TO_ANNOUNCE`: 新块通知数量不可超过该阈值\r\n- `BLOCK_DOWNLOAD_WINDOW`: 下载滑动窗口大小\r\n\r\n### 存储\r\n\r\n- 块状态树\r\n- Best Chain Tip，决定是否要下载块和采用块。\r\n- Best Header Chain Tip，连接块头时用来构建每轮第一个请求的 Locator\r\n\r\n每个连接节点需要单独存储的\r\n\r\n- 观测到的对方的 Best Chain Tip\r\n- 上一次发送过的工作量最高的块头哈希 Best Sent Header\r\n\r\n## 消息定义\r\n\r\n具体消息定义见参考实现，这里只列出同步涉及到的消息和必要的一些字段和描述。\r\n\r\n消息的发送是完全异步的，比如发送 `getheaders` 并不需要等待对方回复 `headers` 再发送其它请求，也不需要保证请求和回复的顺序关系，比如节点 A 发送了 `getheaders` 和 `getdata` 给 B，B 可以先发送 `block`，然后再发送 `headers` 给 A。\r\n\r\nCompact Block [^1] 需要使用到的消息 `cmpctblock` 和 `getblocktxn` 会在 Compact Block 相关文档中说明。\r\n\r\n### getheaders\r\n\r\n用于连接块头时向邻居节点请求块头。请求第一页，和收到后续页使用相同的 getheaders 消息，区别是第一页是给本地的 Best Header Chain Tip 的父块生成 Locator，而后续页是使用上一页的最后一个块生成 Locator。\r\n\r\n- `locator`: 对 Chain 上块采样，得到的哈希列表\r\n\r\n### headers\r\n\r\n用于回复 `getheaders` 和通知新块，处理逻辑没有区别，只是当块头数量小于 `MAX_BLOCKS_TO_ANNOUNCE` 时如果发现有 Orphan Block，因为可能是新块通知，所以需要做一次连接块同步。收到 `headers` 如果块头数量等于 `MAX_HEADERS_RESULTS` 表示还有更多的块需要请求。\r\n\r\n- `headers`：块头列表\r\n\r\n### getdata\r\n\r\n用于下载块阶段\r\n\r\n- `inventory`: 要下载对象列表，每个成员包含字段\r\n\t- `type`: 下载对象的类型，这里只用到块\r\n\t- `hash`: 使用对象哈希做标识符\r\n\r\n### block\r\n\r\n回复 `getdata` 的块下载请求\r\n\r\n- `header` 块头\r\n- `transactions` 交易列表\r\n\r\n\r\n[^1]:\tCompact Block 是种压缩传输完整块的技术。它基于在传播新块时，其中的交易应该都已经在对方节点的交易池中。这时只需要包含 交易 txid 列表，和预测对方可能没有的交易的完整信息，接收方就能基于交易池恢复出完整的交易。详细请查阅 Compact Block RFC (TODO: link to rfc) 和 Bitcoin 相关 [BIP](https://github.com/bitcoin/bips/blob/master/bip-0152.mediawiki)。\r\n\r\n","\\0005-priviledged-mode\\0005-priviledged-mode.md":"---\r\nNumber: \"0005\"\r\nCategory: Informational\r\nStatus: Draft\r\nAuthor: Xuejie Xiao\r\nOrganization: Nervos Foundation\r\nCreated: 2018-11-26\r\n---\r\n\r\n# Privileged architecture support for CKB VM\r\n\r\n## Abstract\r\n\r\nThis RFC aims to introduce privileged architecture support for CKB VM. While CKB VM doesn't require a privileged model since it only runs one contract at a time, privileged model can help bring MMU support, which can be quite useful in the following cases:\r\n\r\n* Implementing sophisticated contracts that require dynamic memory allocation, MMU can be used here to prevent invalid memory access for better security.\r\n* Beginners can leverage MMU to trade some cycles for better security.\r\n\r\nSpecifically, we plan to add the following features to CKB VM:\r\n\r\n* Just enough CSR(control and status register) instructions and VM changes to support a) privilege mode switching and b) page fault function installation.\r\n* A [TLB](https://en.wikipedia.org/wiki/Translation_lookaside_buffer) structure\r\n\r\nNotice privileged architecture here is an opt-in feature that is closed by default: while CKB VM will always have this feature, it's up to contract writers to decide if they need it. Contracts optimized purely for minimum cycles should have no problem completely ignoring privileged mode.\r\n\r\n## Privileged mode support via CSR instructions\r\n\r\nTo ensure maximum compatibility, we will use the exact instructions and workflows defined in the [RISC-V spec](https://riscv.org/specifications/privileged-isa/) to implement privilege mode support here:\r\n\r\n* First, CSR instructions as defined in RISC-V will be implemented in CKB VM to implement read/write on control and status registers(CSR).\r\n* For simplicity reasons, we might not implement every control and status register as defined in RISC-V spec. For now, we are planning to implement `Supervisor Trap Vector Base Address Register(stvec)` and any other register that might be used in the trap phase. As documented in the spec, reading/writing other registers will result in illegal instruction exception, it's up to contract writer how they want to handle this.\r\n* For now, CKB VM will only use 2 privileged modes: `machine` privileged mode and `user` privileged mode. In machine mode, the contract is free to do anything, in user mode, on the other hand, the operations will be limited.\r\n\r\nThe trap function installed in `stvec` is nothing but a normal RISC-V function except that it runs with machine privileged mode. As a result, we will also add proper permission checkings to prevent certain operations in user mode, which might include but are not limited to:\r\n\r\n* CSR instructions\r\n* Accessing memory pages belonging to machine privileged mode\r\n* Accessing memory pages without correct permissions, for example, it's forbidden to execute a memory page which doesn't have `EXECUTE` permission\r\n\r\nNote that when CKB VM first loads, it will be in machine privileged mode, hence contracts that don't need privileged mode support can act as if privileged mode doesn't exist. Contracts that do leverage privileged mode, however, can first setup metadata, then switch to user privileged mode by leveraging RISC-V standard `mret` instruction.\r\n\r\n## TLB\r\n\r\nTo help with MMU, a [Transaction lookaside buffer](https://en.wikipedia.org/wiki/Translation_lookaside_buffer) (TLB) structure will also be included in CKB VM. For simplicity, we will implement a TLB now with the following characteristics:\r\n\r\n* The TLB entry will have 64 entries, each entry is 4KB(exactly 1 memory page).\r\n* The TLB implemented will be one-way associative, meaning if 2 memory pages have the same value for the last 6 bits, they will evict each other.\r\n* Whenever we are switching between different privileged levels, the TLB will be fully flushed.\r\n\r\nNotice TLB will only be instantiated when CKB VM is generating the first page fault trap, that means if a contract keeps running in machine mode, the contract might never interact with the TLB.\r\n\r\nAfter a TLB is instantiated, there's no way to turn it down in current CKB VM's lifecycle.\r\n","\\0005-priviledged-mode\\0005-priviledged-mode.zh.md":"---\r\nNumber: \"0005\"\r\nCategory: Informational\r\nStatus: Draft\r\nAuthor: Xuejie Xiao\r\nOrganization: Nervos Foundation\r\nCreated: 2018-11-26\r\n---\r\n\r\n# CKB VM 中的特权架构支持\r\n\r\n## 概要\r\n\r\n本 RFC 的目标是为 CKB VM 添加特权架构支持。虽然由于 CKB VM 每次只运行一个合约，特权模式在 CKB VM 本身的运行中并不需要，但特权模式对添加 MMU 的支持是很有帮助的，MMU 的存在有利于以下几个场景：\r\n\r\n* 实现需要动态内存分配的复杂合约时，MMU 可以帮助避免内存越界错误，增加安全性\r\n* MMU 可以帮助初学者在消耗一定 cycle 的情况下增加安全性\r\n\r\n具体来说，我们提议为 CKB VM 增加如下部分：\r\n\r\n* 为支持特权模式切换功能，以及指定 page fault 函数功能添加刚刚好足够的 CSR(控制与状态寄存器，control and status register) 指令以及 VM 修改\r\n* [TLB](https://en.wikipedia.org/wiki/Translation_lookaside_buffer) 结构\r\n\r\n注意这里实现的特权架构是一个默认关闭，可选开启的功能：虽然这个功能在 CKB VM 中一直存在，但是合约设计者可以自由决定是否使用这一功能。为最小 cycle 使用数优化的合约可以完全忽略这一功能。\r\n\r\n## 基于 CSR 指令的特权模式支持\r\n\r\n为尽最大可能确保兼容性，我们会用 [RISC-V 标准](https://riscv.org/specifications/privileged-isa/) 中定义的指令以及流程来实现特权指令支持：\r\n\r\n* 首先，我们会实现 RISC-V 标准中定义的 CSR 指令，用于读写控制与状态寄存器 (CSR)。\r\n* 出于简化实现的考虑，我们不会实现 RISC-V 中定义的每一个控制与状态寄存器。目前为止，我们只计划实现 `Supervisor Trap Vector Base Address Register(stvec)` 以及其他在 trap 阶段会被用到的寄存器。在 CKB VM 中读写其他寄存器会参照 spec 中的定义，抛出违法指令的异常，合约开发者可以自行决定如何处理异常。\r\n* 目前 CKB VM 只用到了两个特权模式级别：`machine` 特权模式以及 `user` 特权模式，在 machine 特权模式中，合约可以自由做任何操作，相应的在 user 特权模式中，合约只可以进行允许的操作。\r\n\r\n`stvec` 中指定的 trap 方法 其实就是一个普通的 RISC-V 函数，他与其他普通函数的唯一区别在于它运行在 machine 特权模式上。相对应的，我们也会在 user 特权模式中禁止某些操作，这包括但不限于：\r\n\r\n* CSR 指令\r\n* 访问属于 machine 特权级别的内存页\r\n* 用错误的权限访问内存页，如执行没有执行权限内存页上的代码\r\n\r\n注意 CKB VM 加载时首先会进入 machine 特权模式，因此不需要特权模式支持的合约可以假装特权模式不存在而继续运行。需要特权模式的合约则可以先进行初始化操作，然后通过 RISC-V 的标准指令 `mret` 切换到 user 特权模式。\r\n\r\n## TLB\r\n\r\nCKB VM 会添加 [Transaction lookaside buffer](https://en.wikipedia.org/wiki/Translation_lookaside_buffer) (TLB) 结构辅助 MMU 实现。出于简化实现的考虑，我们会实现具有如下特性的 TLB：\r\n\r\n* TLB 中有 64 个条目，每个条目为 4KB (即正好一个内存页)\r\n* TLB 为单路组相联，即两个末尾 6 个 bit 相同的内存页会相互竞争一个条目位置\r\n* 切换特权级别时，整个 TLB 会被全部清空\r\n\r\n注意 TLB 只会在 CKB VM 第一次生成 page fault trap 操作时才被初始化。这意味着如果一个合约一直在 machine 特权模式下运行的话，该合约可能永远也不会与 TLB 交互。\r\n\r\nTLB 成功初始化之后，在当前 CKB VM 运行期间会持续存在，无法在初始化之后关闭 TLB。\r\n","\\0006-merkle-tree\\0006-merkle-tree.md":"---\r\nNumber: \"0006\"\r\nCategory: Standards Track\r\nStatus: Proposal\r\nAuthor: Ke Wang\r\nOrganization: Nervos Foundation\r\nCreated: 2018-12-01\r\n---\r\n\r\n# Merkle Tree for Static Data\r\n\r\n## Complete Binary Merkle Tree\r\n\r\nCKB uses Complete Binary Merkle Tree(CBMT) to generate *Merkle Root*  and *Merkle Proof* for a static list of items. Currently, CBMT is used to calculate *Transactions Root*. Basically, CBMT is a ***complete binary tree***, in which every level, except possibly the last, is completely filled, and all nodes are as far left as possible. And it is also a ***full binary tree***, in which every node other than the leaves has two children. Compare with other Merkle trees, the hash computation of CBMT is minimal, as well as the proof size.\r\n\r\n## Nodes Organization\r\n\r\nFor the sake of illustration, we order the tree nodes from ***top to bottom*** and ***left to right*** starting at zero. In CBMT with *n* items, root is the *first* node, and the first item's hash is *node 0*, second is *node n+1*, etc. We choose this nodes organization because it is easy to calculate the node order for an item.\r\n\r\nFor example, CBMT with 6 items(suppose the hashes are `[T0, T1, T2, T3, T4, T5]`) and CBMT with 7 items(suppose the hashes are `[T0, T1, T2, T3, T4, T5, T6]`) is shown below:\r\n\r\n```\r\n        with 6 items                       with 7 items\r\n\r\n              B0 -- node 0                       B0 -- node 0\r\n             /  \\                               /  \\\r\n           /      \\                           /      \\\r\n         /          \\                       /          \\\r\n       /              \\                   /              \\\r\n      B1 -- node 1    B2 -- node 2       B1 -- node 1    B2 -- node 2\r\n     /  \\            /  \\               /  \\            /  \\\r\n    /    \\          /    \\             /    \\          /    \\\r\n   /      \\        /      \\           /      \\        /      \\\r\n  B3(3)   B4(4)  TO(5)    T1(6)      B3(3)   B4(4)   B5(5)   T0(6)\r\n /  \\    /  \\                       /  \\    /  \\    /  \\\r\nT2  T3  T4  T5                     T1  T2  T3  T4  T5  T6\r\n(7) (8) (9) (10)                   (7) (8) (9)(10)(11) (12)\r\n```\r\n\r\nSpecially, the tree with 0 item is empty(0 node) and its root is `H256::zero`.\r\n\r\n## Tree Struct\r\n\r\nCBMT can be represented in a very space-efficient way, using an array alone. Nodes in the array are presented in ascending order.\r\n\r\nFor example, the two trees above can be represented as:\r\n\r\n```\r\n// an array with 11 elements, the first element is node 0(BO), second is node 1, etc.\r\n[B0, B1, B2, B3, B4, T0, T1, T2, T3, T4, T5]\r\n\r\n// an array with 13 elements, the first element is node 0(BO), second is node 1, etc.\r\n[B0, B1, B2, B3, B4, B5, T0, T1, T2, T3, T4, T5, T6]\r\n```\r\n\r\nSuppose a CBMT with *n* items, the size of the array would be *2n-1*, the index of item i(start at 0) is *i+n-1*. For node at *i*, the index of its parent is *(i-1)/2*, the index of its sibling is *(i+1)^1-1*(*^* is xor) and the indexes of its children are *[2i+1, 2i+2]*.\r\n\r\n## Merkle Proof\r\n\r\nMerkle Proof can provide a proof for existence of one or more items. Only sibling of the nodes along the path that form leaves to root, excluding the nodes already in the path, should be included in the proof. We also specify that ***the nodes in the proof is presented in descending order***(with this, algorithms of proof's generation and verification could be much simple). Indexes of item that need to prove are essential to complete the root calculation, since the index is not the inner feature of item, so the indexes are also included in the proof, and in order to get the correct correspondence, we specify that the indexes are ***presented in ascending order by corresponding hash***. For example, if we want to show that `[T1, T4]` is in the list of 6 items above, only nodes `[T5, T0, B3]` and indexes `[9, 6]` should be included in the proof.\r\n\r\n### Proof Structure\r\n\r\nThe schema of proof struct is:\r\n\r\n```\r\ntable Proof {\r\n  // indexes of items\r\n  indexes: [uint32];\r\n  // nodes on the path which can not be calculated, in descending order by index\r\n  nodes: [H256];\r\n}\r\n```\r\n\r\n### Algorithm of proof generation\r\n\r\n```c++\r\nProof gen_proof(Hash tree[], U32 indexes[]) {\r\n  Hash nodes[];\r\n  U32 tree_indexes[];\r\n  Queue queue;\r\n\r\n  int size = len(tree) >> 1 + 1;\r\n  indexes.desending_sort();\r\n\r\n  for index in indexes {\r\n    queue.push_back(index + size - 1);\r\n  }\r\n\r\n  while(queue is not empty) {\r\n    int index = queue.pop_front();\r\n    int sibling = calculate_sibling(index);\r\n\r\n    if(sibling == queue.front()) {\r\n      queue.pop_front();\r\n    } else {\r\n      nodes.push_back(tree[sibling]);\r\n    }\r\n\r\n    int parent = calculate_parent(index);\r\n    if(parent != 0) {\r\n      queue.push_back(parent);\r\n    }\r\n  }\r\n\r\n  add (size-1) for every index in indexes;\r\n  sort indexes in ascending order by corresponding hash;\r\n\r\n  return Proof::new(indexes, nodes);\r\n}\r\n```\r\n\r\n### Algorithm of validation\r\n\r\n```c++\r\nbool validate_proof(Proof proof, Hash root, Item items[]) {\r\n  Queue queue;\r\n  ascending_sort_by_item_hash(items);\r\n\r\n  for (index,item) in (proof.indexes, items) {\r\n    queue.push_back((item.hash(), index));\r\n  }\r\n\r\n  descending_sort_by_index(queue);\r\n\r\n  int i = 0;\r\n  while(queue is not empty) {\r\n    Hash hash, hash1, hash2;\r\n    int index1, index2;\r\n\r\n    (hash1, index1) = queue.pop_front();\r\n    (hash2, index2) = queue.front();\r\n    int sibling = calculate_sibling(index1);\r\n\r\n    if(sibling == index2) {\r\n      queue.pop_front();\r\n      hash = merge(hash2, hash1);\r\n    } else {\r\n      hash2 = proof.nodes[i++];\r\n\r\n      if(is_left_node(index1)) {\r\n        hash = merge(hash1, hash2);\r\n      } else {\r\n        hash = merge(hash2, hash1);\r\n      }\r\n    }\r\n\r\n    int parent = calculate_parent(index);\r\n    if(parent == 0) {\r\n      return root == hash;\r\n    }\r\n    queue.push_back((hash, parent))\r\n  }\r\n\r\n  return false;\r\n}\r\n```\r\n","\\0006-merkle-tree\\0006-merkle-tree.zh.md":"---\r\nNumber: \"0006\"\r\nCategory: Standards Track\r\nStatus: Proposal\r\nAuthor: Ke Wang\r\nOrganization: Nervos Foundation\r\nCreated: 2018-12-01\r\n---\r\n\r\n# 静态 Merkle Tree\r\n\r\n## Complete Binary Merkle Tree\r\n\r\nCKB 使用 ***Complete Binary Merkle Tree(CBMT)*** 来为静态数据生成 *Merkle Root* 及 *Merkle Proof*，目前 CBMT 被用于 *Transactions Root* 的计算中。它是一棵完全二叉树，同时也是一棵满二叉树，相比于其它的 Merkle Tree，***Complete Binary Merkle Tree*** 具有最少的 Hash 计算量及最小的 proof size。\r\n\r\n## 节点组织形式\r\n\r\n规定 CBMT 中节点的排列顺序为从上到下、从左到右（从零开始标号），在一棵由 *n* 个 item 生成的 CBMT 中，下标为 *0* 的节点为 *Merkle Root*，下标为 *n* 的节点为第 *1* 个 item 的 hash，下标 *n+1* 的节点为第 2 个 item 的 hash，以此类推。之所以采用这种排列方式，是因为从 item 的位置很容易计算出其在 CBMT 中节点对应的位置。\r\n\r\n举例来说，6 个 item (假设 item 的 Hash 为 `[T0, T1, T2, T3, T4, T5]`)与 7 个 item (假设 item 的 hash 为 `[T0, T1, T2, T3, T4, T5, T6]`)生成的 Tree 的结构如下所示：\r\n\r\n```\r\n        with 6 items                       with 7 items\r\n\r\n              B0 -- node 0                       B0 -- node 0\r\n             /  \\                               /  \\\r\n           /      \\                           /      \\\r\n         /          \\                       /          \\\r\n       /              \\                   /              \\\r\n      B1 -- node 1    B2 -- node 2       B1 -- node 1    B2 -- node 2\r\n     /  \\            /  \\               /  \\            /  \\\r\n    /    \\          /    \\             /    \\          /    \\\r\n   /      \\        /      \\           /      \\        /      \\\r\n  B3(3)   B4(4)  TO(5)    T1(6)      B3(3)   B4(4)   B5(5)   T0(6)\r\n /  \\    /  \\                       /  \\    /  \\    /  \\\r\nT2  T3  T4  T5                     T1  T2  T3  T4  T5  T6\r\n(7) (8) (9) (10)                   (7) (8) (9)(10)(11) (12)\r\n```\r\n\r\n此外，我们规定对于只有 0 个 item 的情况，生成的 tree 只有 0 个 node，其 root 为 `H256::zero`。\r\n\r\n## 数据结构\r\n\r\nCBMT 可以用一个数组来表示，节点按照升序存放在数组中，上面的两棵 tree 用数组表示分别为：\r\n\r\n```\r\n// 11 个元素的数组，数组第一个位置放 node0, 第二个位置放 node1，以此类推。\r\n[B0, B1, B2, B3, B4, T0, T1, T2, T3, T4, T5]\r\n// 13 个元素的数组，数组第一个位置放 node0, 第二个位置放 node1，以此类推。\r\n[B0, B1, B2, B3, B4, B5, T0, T1, T2, T3, T4, T5, T6]\r\n```\r\n\r\n在一个由 n 个 item 生成的 CBMT 中，其数组的大小为 *2n-1*，*item i* 在数组中的下标为（下标从 0 开始）*i+n-1*。对于下标为 *i* 的节点，其父节点下标为 *(i-1)/2*，兄弟节点下标为 *(i+1)^1-1*（^为异或），子节点的下标为 *2i+1*、*2i+2*。\r\n\r\n## Merkle Proof\r\n\r\nMerkle Proof 能为一个或多个 item 提供存在性证明，Proof 中应只包含从叶子节点到根节点路径中无法直接计算出的节点，并且我们规定这些节点按照降序排列，采用降序排列的原因是这与节点的生成顺序相符且 *proof* 的生成及校验算法也会变得非常简单。此外，计算 root 时还需要知道要证明的 item 的 index，因此这些 index 也应包含在 Proof 中，且为了能够使这些 index 能够正确的对应到 item，因此规定这些 index 按对应的 item 的 hash 升序排列，如在 6 个 item 的 Merkle Tree 中为 `[T1, T4]` 生成的 Proof 中应只包含 `[T5, T0, B3]` 和 `[9,6]`。\r\n\r\n### Proof 结构\r\n\r\nProof 结构体的 schema 形式为：\r\n\r\n```\r\ntable Proof {\r\n  // indexes of items\r\n  indexes: [uint32];\r\n  // nodes on the path which can not be calculated, in descending order by index\r\n  nodes: [H256];\r\n}\r\n```\r\n\r\n### Proof 生成算法\r\n\r\n```c++\r\nProof gen_proof(Hash tree[], U32 indexes[]) {\r\n  Hash nodes[];\r\n  U32 tree_indexes[];\r\n  Queue queue;\r\n\r\n  int size = len(tree) >> 1 + 1;\r\n  indexes.desending_sort();\r\n\r\n  for index in indexes {\r\n    queue.push_back(index + size - 1);\r\n  }\r\n\r\n  while(queue is not empty) {\r\n    int index = queue.pop_front();\r\n    int sibling = calculate_sibling(index);\r\n\r\n    if(sibling == queue.front()) {\r\n      queue.pop_front();\r\n    } else {\r\n      nodes.push_back(tree[sibling]);\r\n    }\r\n\r\n    int parent = calculate_parent(index);\r\n    if(parent != 0) {\r\n      queue.push_back(parent);\r\n    }\r\n  }\r\n\r\n  add (size-1) for every index in indexes;\r\n  sort indexes in ascending order by corresponding hash;\r\n\r\n  return Proof::new(indexes, nodes);\r\n}\r\n```\r\n\r\n### Proof 校验算法\r\n\r\n```c++\r\nbool validate_proof(Proof proof, Hash root, Item items[]) {\r\n  Queue queue;\r\n  ascending_sort_by_item_hash(items);\r\n\r\n  for (index,item) in (proof.indexes, items) {\r\n    queue.push_back((item.hash(), index));\r\n  }\r\n\r\n  descending_sort_by_index(queue);\r\n\r\n  int i = 0;\r\n  while(queue is not empty) {\r\n    Hash hash, hash1, hash2;\r\n    int index1, index2;\r\n\r\n    (hash1, index1) = queue.pop_front();\r\n    (hash2, index2) = queue.front();\r\n    int sibling = calculate_sibling(index1);\r\n\r\n    if(sibling == index2) {\r\n      queue.pop_front();\r\n      hash = merge(hash2, hash1);\r\n    } else {\r\n      hash2 = proof.nodes[i++];\r\n\r\n      if(is_left_node(index1)) {\r\n        hash = merge(hash1, hash2);\r\n      } else {\r\n        hash = merge(hash2, hash1);\r\n      }\r\n    }\r\n\r\n    int parent = calculate_parent(index);\r\n    if(parent == 0) {\r\n      return root == hash;\r\n    }\r\n    queue.push_back((hash, parent))\r\n  }\r\n\r\n  return false;\r\n}\r\n```\r\n","\\0007-scoring-system-and-network-security\\0007-scoring-system-and-network-security.md":"---\r\nNumber: \"0007\"\r\nCategory: Standards Track\r\nStatus: Proposal\r\nAuthor: Jinyang Jiang\r\nOrganization: Nervos Foundation\r\nCreated: 2018-10-02\r\n---\r\n\r\n# P2P Scoring System And Network Security\r\n\r\n## Abstract\r\n\r\nThis document describes the scoring system of CKB P2P Networking layer and several networking security strategies based on it.\r\n\r\n\r\n## Motivation\r\n\r\nCKB network is designed as an open peer-to-peer network and any node can join the network without permission. This openness, however, also makes it possible for malicious nodes to join and attack the peer-to-peer network.\r\n\r\nThere were \"Eclipse Attack\" security issues in both Bitcoin network and Ethereum network, which also designed as the open peer-to-peer network.\r\nThe principle of Eclipse Attack is that the attacker would occupy all Peers connection slots of the victim node by manipulating malicious nodes, then filter the victim's view of the blockchain network.\r\n\r\nVia \"Eclipse Attack\" the attacker can take down a victim node with low cost. After that, the attacker could control the victim's mining power for its nefarious purposes, or cheat this victim node to launch a double spent attack.\r\n\r\nReference paper -- [Eclipse Attacks on Bitcoin’s Peer-to-Peer Network][2] \r\n\r\nThere are several strategies to prevent \"Eclipse attack\" introduced in this paper and parts of them have already been implemented in the Bitcoin network. That is to say, this document will describe how to deploy these strategies to CKB network.\r\n\r\nIn addition, this document also describes the scoring system of CKB P2P Networking layer and we want to handle more generalized network security cases by combining it with more sophisticated security strategies from the Bitcoin network.\r\n\r\nBased on the scoring system, we can follow several rules below to handle malicious peers:\r\n\r\n1. Nodes should store peers information as much as possible.\r\n2. Nodes need to score Peers' good and bad behavior continuously.\r\n3. Nodes should retain good (high-score) peers and evict bad (low-score) peers out.\r\n\r\nCKB client should implement the scoring system and following security strategies.\r\n\r\n\r\n## Specification\r\n\r\n### Terminology\r\n\r\n* `Node`\r\n* `Peer` - Other nodes connected through the network\r\n* `PeerInfo` - A data struct used for describing information of `Peer`\r\n* `PeerStore` - A component used to store `PeerInfo`\r\n* `outbound peer` - describe a peer which initiates a connection.\r\n* `inbound peer` - describe a peer which accepts a connection.\r\n* `max_outbound` - Max number of outbound peers.\r\n* `max_inbound` - Max number of inbound peers.\r\n* `network group` - A concept which used when to evict out peers, calculating from the peer's IP address(prefix 16 bits of IPv4 and prefix 32 bits of IPv6).\r\n\r\n### Peer Store and Peer Info\r\n\r\nPeerStore should be persistent storage and store PeerInfos as more as possible.\r\n\r\nPeerInfo should include fields below at least:\r\n\r\n```\r\nPeerInfo { \r\n  NodeId,\r\n  ConnectedIP,\r\n  Direction,  // Inbound or Outbound\r\n  LastConnectedAt, // The time of the last connection \r\n  Score\r\n}\r\n```\r\n\r\n### Scoring System\r\n\r\nParameters below are required in Scoring System:\r\n\r\n* `PEER_INIT_SCORE` - the initial score of peers\r\n* `BEHAVIOURS` - a set of peer's possible behaviors, such as: `UNEXPECTED_DISCONNECT`, `TIMEOUT`, `CONNECTED`\r\n* `SCORING_SCHEMA` - describe different scores corresponding to different behaviors, such as: `{\"TIMEOUT\": -10, \"CONNECTED\": 10}`\r\n* `BAN_SCORE` - a peer will be banned when its score is lower than this value.\r\n\r\nNetwork layer should provide the scoring interface, allow upper sub-protocols (such as: `sync`, `relay`) to report behaviors of a peer, and update peer's score based on `SCORING_SCHEMA`.\r\n\r\n``` ruby\r\npeer.score += SCOREING_SCHEMA[BEHAVIOUR]\r\n```\r\n\r\nPeer's behaviors can be distinguished into three categories:\r\n\r\n1. Correct behaviors which follow the specification:\r\n    * For example, a node downloads a new block from a peer; a node connects to a peer successfully. Considering a bad peer may pretend like a good one before launching an attack, we should give the peer a relatively low positive score instead of giving a high score at once to encourage the peer to accumulate his credit by performing good behaviors for a long time.\r\n2. Incorrect behaviors which may be caused by network exception:\r\n    * For example, a peer disconnect unexpectedly; a node failed to connect to a peer; ping timeout. Since we can't tell whether these behaviors are intentional bad behavior or caused by the network,  we should give the peer a little negative score to keep tolerant.\r\n3. Incorrect behaviors which violate the protocol:\r\n    * For example, a peer sends an illegal encoded content; a peer sends an invalid block; a peer sends an invalid transaction. We should give a peer a negative score when we can be pretty sure its behavior is malicious, and when a peer's score is lower than `BAN_SCORE`, this peer should be banned.\r\n\r\nExamples:\r\n\r\n* Peer 1 connected successfully. A node reported this peer's `CONNECTED` behavior and peer 1 got a 10 score rewarded.\r\n* Peer 2 gets a connection timeout. A node reports `TIMEOUT` behavior and peer 2 get a -10 score as punishment.\r\n* Peer 1 sent repetitive `GET_BLOCK` messages. A node reported `DUPLICATED_REQUEST_BLOCK` behavior and peer 1 got a -50 score as punishment.\r\n* Peer 1's score is lower than `BAN_SCORE`, node disconnect with peer 1 then ban the peer.\r\n\r\nParameters like `BEHAVIOURS`, `SCORING_SCHEMA` are not a part of consensus protocol, so CKB client should tune these parameters according to the actual situation of the network.\r\n\r\n### Outbound peers selection\r\n\r\nThe \"Eclipse Attack\" paper describes a critical security issue during Bitcoin node restarting process:\r\n\r\n1. The attacker tries to fit the victim node's addrman(Bitcoin's peer store) with attacker's bad nodes' addresses.\r\n2. The attacker waits the victim node to restart (or use several methods to force it).\r\n3. After the restart, the victim node will select some address from addrman to connect.\r\n4. The attack successes if all outbound connections of the victim node are connected to the attacker's bad nodes.\r\n\r\nCKB should avoid this problem when initialize the network.\r\n\r\n#### The process of initializing outbound peers\r\n\r\nRequired parameters:\r\n\r\n* `TRY_SCORE` - We only try to connect a peer when its score is higher than this value.\r\n* `ANCHOR_PEERS` - the number of anchor peers should be less than `max_outbound`, such as `2`\r\n\r\nRequired variables:\r\n\r\n* `try_new_outbound_peer` - network component checks this variable to decide whether to connect to extra outbound peers or not.\r\n\r\nThe process of choosing an outbound peer:\r\n\r\n1. Execute step 2 if currently connected outbound peers less than `ANCHOR_PEERS`, otherwise execute step 3.\r\n2. Choose an \"anchor peer\":\r\n    1. Choose recently connected outbound peers from peer store(can select by `LastConnectedAt` field of peer info).\r\n    2. Execute step 3, if `recent_peers` is empty; otherwise, we choose the peer which has got the highest score from `recent_peers` and return it as the new outbound peer.\r\n3. Choose peer info randomly which must have a higher score than `TRY_SCORE` and have different `network group` from all currently connected outbound peers from PeerStore, return it as the new outbound peer and if we can't find anyone, then execute step 5.\r\n4. Choose peer info randomly from boot nodes.\r\n\r\nIn step 1, we choose an anchor peer if the node has zero or only a few connected outbound peers. This behavior refers to \"Anchor Connection\" strategy which described in the [Eclipse Attack][2] paper.\r\n\r\nPseudocode:\r\n\r\n``` ruby\r\n# return our new outbound peer\r\ndef find_outbound_peer\r\n  connected_outbound_peers = connected_peers.select{|peer| peer.outbound? && !peer.feeler? }\r\n  # step 1\r\n  if connected_outbound_peers.length < ANCHOR_PEERS\r\n    find_anchor_peer() || find_random_peer() || random_boot_node()\r\n  else\r\n    find_random_peer() || random_boot_node()\r\n  end\r\nend\r\n\r\n# step 2\r\ndef find_anchor_peer\r\n  last_connected_peers = peer_store.sort_by{|peer| -peer.last_connected_at}.take(max_outbound)\r\n  # return the higest scored peer info\r\n  last_connected_peers.sort_by(&:score).last\r\nend\r\n\r\n# step 3\r\ndef find_random_peer\r\n  connected_outbound_peers = connected_peers.select{|peer| peer.outbound? && !peer.feeler? }\r\n  exists_network_groups = connected_outbound_peers.map(&:network_group)\r\n  candidate_peers = peer_store.select do |peer| \r\n    peer.score >= TRY_SCORE && !exists_network_groups.include?(peer.network_group)\r\n  end\r\n  candidate_peers.sample\r\nend\r\n\r\n# step 4\r\ndef random_boot_node\r\n  boot_nodes.sample\r\nend\r\n```\r\n\r\nThe node should repeat this process until the number of connected outbound peers is equal to or greater than  `max_outbound` and `try_new_outbound_peer` is `false`.\r\n\r\n``` ruby\r\ncheck_outbound_peers_interval = 15\r\n# continually check the number of outbound peers\r\nloop do\r\n  sleep(check_outbound_peers_interval)\r\n  connected_outbound_peers = connected_peers.select{|peer| peer.outbound? && !peer.feeler? }\r\n  if connected_outbound_peers.length >= max_outbound && !try_new_outbound_peer \r\n    next\r\n  end\r\n  new_outbound_peer = find_outbound_peer()\r\n  connect_peer(new_outbound_peer)\r\nend\r\n```\r\n\r\n`try_new_outbound_peer` variable is used for some situation where a node can't get any useful messages in a duration time. Then we will set `try_new_outbound_peer` to `true` and allow the node to connect to more extra outbound peers. This strategy would be introduced later.\r\n\r\nUnder this strategy, the attacker must achieve the following conditions to apply an eclipse attack:\r\n\r\n1. The attacker needs to have `n` malicious peers (`n == ANCHOR_PEERS`) to be the victim node's outbound peers and these peers must have the highest scores.\r\n2. The attacker needs to prepare at least `max_outbound - ANCHOR_PEERS` bad peers' addresses in PeerStore. At the same time, the attacker must make sure that the randomly selected `max_outbound - ANCHOR_PEERS` outbound peers are all camouflage nodes of the attacker.\r\n\r\n\r\n#### Extra outbound peers and eviction\r\n\r\nNetwork component should check the main protocol (for example: `sync` protocol in CKB) status every few minutes.\r\n\r\n``` ruby\r\ndef sync_maybe_stale\r\n  now = Time.now\r\n  # use block product time to detect network status\r\n  # we consider network maybe stale if block not produced within a predicted time\r\n  last_tip_updated_at < now - block_produce_interval * n\r\nend\r\n```\r\n\r\nThe network component should set `try_new_outbound_peer` to `true` when `sync` protocol doesn't work and set back to `false` when `sync` protocol puts back.\r\n\r\n``` ruby\r\ncheck_sync_stale_at = Time.now\r\nloop_interval = 30\r\ncheck_sync_stale_interval = 15 * 60 # 15 minutes\r\n\r\nloop do\r\n  sleep(loop_interval)\r\n  # try evict\r\n  evict_extra_outbound_peers()\r\n  now = Time.now\r\n  if check_sync_stale_at >= now\r\n    # update try_new_outbound_peer\r\n    set_try_new_outbound_peer(sync_maybe_stale())\r\n    check_sync_stale_at = now + check_sync_stale_interval\r\n  end\r\nend\r\n```\r\n\r\nCKB network will try to connect to extra outbound peers continually when `try_new_outbound_peer` is `true`, and try to evict useless extra peers every few minutes to prevent too many connections.\r\n\r\n``` ruby\r\n# eviction logic\r\ndef evict_extra_outbound_peers\r\n  connected_outbound_peers = connected_peers.select{|peer| peer.outbound? && !peer.feeler? }\r\n  if connected_outbound_peers.length <= max_outbound\r\n    return\r\n  end\r\n  now = Time.now\r\n  # here use last_block_anoncement_at to evict peers, we assume the oldest one is useless for us\r\n  evict_target = connected_outbound_peers.sort_by do |peer|\r\n    peer.last_block_announcement_at\r\n  end.first\r\n  if evict_target\r\n    if now - evict_target.last_connected_at > MINIMUM_CONNECT_TIME && !is_downloading?(evict_target)\r\n      disconnect_peer(evict_target)\r\n      # prevent connect to too many peers\r\n      set_try_new_outbound_peer(false)\r\n    end\r\n  end\r\nend\r\n```\r\n\r\n### The process of accepting inbound peers\r\n\r\nIn Bitcoin, a node will try to evict connected inbound peers if the number of connected inbound peers reaches `max_inbound` and another new inbound connection tries to connect. (check [Bitcoin source code][1] for details)\r\n\r\nThis eviction behavior is intended to keep high-quality peers and evict low-quality peers.\r\n\r\nCKB refers to Bitcoin's eviction test and steps are as follows:\r\n\r\n1. Consider currently connected inbound peers as `candidate_peers`.\r\n2. Protect peers(`N` represent the number of peers to protect in each step):\r\n    1. Delete `N` peers from `candidate_peers` which has the highest score.\r\n    2. Delete `N` peers from `candidate_peers` which has the lowest ping.\r\n    3. Delete `N` peers from `candidate_peers` which most recently sent us messages.\r\n    4. Delete `candidate_peers.size / 2` peers from `candidate_peers` which have the longest connection time.\r\n3. Group `candidate_peers` according to `network group` field.\r\n4. Find out the group which contains the most peers.\r\n5. Evict the lowest scored peer from the group found in step 4 if it is not empty. Otherwise, reject the connection from the new peer.\r\n\r\nWe protect some peers from eviction based on characteristics that an attacker is hard to simulate or manipulate, to enhence the security of the network.\r\n\r\n### Feeler Connection\r\n\r\nFeeler Connection is intended to test a peer is connectable or not.\r\n\r\nNode will start a feeler connection every few minutes after outbound peers reach `max_outbound` limit.\r\n\r\n1. Pick out peer info from PeerStore randomly which we never connected to\r\n2. Connect to this peer\r\n3. Run handshake protocol\r\n4. Disconnect\r\n\r\nFeeler peers would be assumed to disconnect soon.\r\n\r\n### Delete peer info from PeerStore\r\n\r\nRequired parameters:\r\n\r\n* `PEER_STORE_LIMIT` - max number of PeerInfo in PeerStore\r\n* `PEER_NOT_SEEN_TIMEOUT` - used for protecting peers which recently connected. Only peer info over `last_connected_to` would be deleted. \r\n\r\nWhen the number of peer info reaches `PEER_STORE_LIMIT`:\r\n\r\n1. Group all PeerInfos in PeerStore according to `network group` field\r\n2. Find out the group which contains the most peer infos\r\n3. Search peers have not been connected recently from this group: `peer.last_connected_at < Time.now - PEER_NOT_SEEN_TIMEOUT`\r\n4. Find out the lowest scored peer info as `candidate_peer_info`\r\n5. if `candidate_peer_info.score < new_peer_info.score` then we delete `candidate_peer_info` and add `new_peer_info`, otherwise we do not accept `new_peer_info`\r\n\r\n## References\r\n\r\n1. [Bitcoin source code][1]\r\n2. [Eclipse Attacks on Bitcoin’s Peer-to-Peer Network][2]\r\n\r\n[1]: https://github.com/bitcoin/bitcoin\r\n[2]: https://eprint.iacr.org/2015/263.pdf\r\n\r\n","\\0007-scoring-system-and-network-security\\0007-scoring-system-and-network-security.zh.md":"---\r\nNumber: \"0007\"\r\nCategory: Standards Track\r\nStatus: Proposal\r\nAuthor: Jinyang Jiang\r\nOrganization: Nervos Foundation\r\nCreated: 2018-10-02\r\n---\r\n\r\n# P2P 评分系统和网络安全\r\n\r\n## 简介\r\n\r\n本篇 RFC 描述了 CKB P2P 网络层的评分系统，以及基于评分的网络安全策略。\r\n\r\n\r\n## 目标\r\n\r\nCKB 网络被设计为开放的 P2P 网络，任何节点都能无需许可的加入网络，但网络的开放性同时使得恶意节点也能够加入并对 P2P 网络进行攻击。\r\n\r\n同样采用开放性 P2P 网络的比特币和以太坊中都曾有「日蚀攻击」的安全问题。\r\n日蚀攻击的原理是攻击者通过操纵恶意节点占领受害者节点所有的 Peers 连接，以此控制受害者节点可见的网络。\r\n\r\n攻击者可以用极少成本实施日蚀攻击，攻击成功后可以操纵受害节点的算力做些恶意行为, 或欺骗受害节点进行双花交易。\r\n\r\n参考论文 -- [Eclipse Attacks on Bitcoin’s Peer-to-Peer Network][2]\r\n\r\n论文中同时提出了几种防范手段, 其中部分已经在比特币主网应用，\r\n本 RFC 参考比特币网络的实现，描述如何在 CKB 网络中正确应用这些措施。\r\n\r\nRFC 同时描述了 CKB P2P 网络的评分机制，\r\n结合 CKB 的评分机制，可以使用比特币中成熟的安全措施来处理更加通用的攻击场景。\r\n\r\n基于 CKB 的评分机制，我们遵循几条规则来处理恶意 Peers：\r\n\r\n1. 节点应尽可能的存储已知的 Peers 信息\r\n2. 节点需要不断对 Peer 的好行为和坏行为进行评分\r\n3. 节点应保留好的(分数高的) Peer，驱逐坏的(分数低) Peer\r\n\r\nRFC 描述了客户端应该实现的打分系统和下文的几种安全策略。\r\n\r\n\r\n## Specification\r\n\r\n### 术语\r\n\r\n* `Node` - 节点\r\n* `Peer` - 网络上的其他节点\r\n* `PeerInfo` - 描述 Peer 信息的数据结构\r\n* `PeerStore` - 用于存储 PeerInfo 的组件\r\n* `outbound peer` - 主动发起连接的节点\r\n* `inbound peer` - 被动接受连接的节点\r\n* `max_outbound` - 节点主动连接的 Peers 上限\r\n* `max_inbound` - 节点被动接受的 Peers 上限\r\n* `network group` - 驱逐节点时用到的概念，对 Peer 连接时的 IP 计算，IPv4 取前 16 位，Ipv6 取前 32 位\r\n\r\n\r\n### PeerStore 和 PeerInfo\r\n\r\nPeerStore 应该做到持久化存储, 并尽可能多的储存已知的 PeerInfo\r\n\r\nPeerInfo 至少包含以下内容\r\n\r\n```\r\nPeerInfo { \r\n  NodeId, // Peer 的 NodeId\r\n  ConnectedIP,  // 连接时的 IP\r\n  Direction,  // Inbound or Outbound\r\n  LastConnectedAt, // 最后一次连接的时间\r\n  Score // 分数\r\n}\r\n```\r\n\r\n### 评分系统\r\n\r\n评分系统需要以下参数\r\n\r\n* `PEER_INIT_SCORE` - Peers 的初始分数\r\n* `BEHAVIOURS` - 节点的行为, 如 `UNEXPECTED_DISCONNECT`, `TIMEOUT`, `CONNECTED` 等\r\n* `SCORING_SCHEMA` - 描述不同行为对应的分数, 如 `{\"TIMEOUT\": -10, \"CONNECTED\": 10}`\r\n* `BAN_SCORE` - Peer 评分低于此值时会被加入黑名单\r\n\r\n网络层应该提供评分接口，允许 `sync`, `relay` 等上层子协议报告 peer 行为，\r\n并根据 peer 行为和 `SCORING_SCHEMA` 调整 peer 的评分。\r\n\r\n``` ruby\r\npeer.score += SCOREING_SCHEMA[BEHAVIOUR]\r\n```\r\n\r\nPeer 的评分是 CKB P2P 网络安全的重要部分，peer 的行为可以分为如下三种：\r\n\r\n1. 符合协议的行为:\r\n    * 如: 从 peer 获取了新的 block、节点成功连接上 peer 。 当 peer 作出符合协议的行为时，节点应上调对 peer 评分，\r\n考虑恶意 Peer 有可能在攻击前进行伪装，\r\n对好行为奖励的分数不应一次性奖励太多，\r\n而是鼓励 peer 长期进行好的行为来积累信用。\r\n\r\n2. 可能由于网络异常导致的行为:\r\n    * 如: peer 异常断开、连接 peer 失败、ping timeout。\r\n对这些行为我们采用宽容性的惩罚，下调对 peer 的评分，但不会一次性下调太多。\r\n\r\n3. 明显违反协议的行为:\r\n    * 如: peer 发送无法解码的内容、peer 发送 invalid block, peer 发送 invalid transaction。\r\n当我们可以确定 peer 存在明显的恶意行为时，对 peer 打低分，如果 peer 评分低于 `BAN_SCORE` ，将 peer 加入黑名单并禁止连接。\r\n\r\n例子:\r\n* peer 1 连接成功，节点报告 peer1 `CONNECTED` 行为，peer 1 加 10 分\r\n* peer 2 连接超时，节点报告 peer2 `TIMEOUT` 行为，peer 2 减 10 分\r\n* peer 1 通过 `sync` 协议发送重复的请求，节点报告 peer 1 `DUPLICATED_REQUEST_BLOCK` 行为，peer 1 减 50 分\r\n* peer 1 被扣分直至低于 `BAN_SCORE`, 被断开连接并加入黑名单\r\n\r\n`BEHAVIOURS`、 `SCORING_SCHEMA` 等参数不属于共识协议的一部分，CKB 实现应该根据网络实际的情况对参数调整。\r\n\r\n\r\n### 节点 outbound peers 的选择策略\r\n\r\n[日蚀攻击论文][2]中提到了比特币节点重启时的安全问题：\r\n\r\n1. 攻击者事先利用比特币的节点发现规则填充受害节点的地址列表\r\n2. 攻击者等待或诱发受害者节点重启\r\n3. 重启后，受害者节点会从 addrman (类似 peer store) 中选择一些地址连接\r\n3. 受害节点的所有对外的连接都连接到了恶意 peers 则攻击者攻击成功\r\n\r\nCKB 在初始化网络时应该避免这些问题\r\n\r\n#### Outbound peers 连接流程\r\n\r\n参数说明: \r\n* `TRY_SCORE` - 设置一个分数，仅当 PeerInfo 分数高于 `TRY_SCORE` 时节点才会去尝试连接\r\n* `ANCHOR_PEERS` - 锚点 peer 的数量，值应该小于 `max_outbound` 如 `2`\r\n\r\n变量:\r\n* `try_new_outbound_peer` - 设置节点是否该继续发起新的 Outbound 连接\r\n\r\n选择一个 outbound peer 的流程:\r\n\r\n1. 如果当前连接的 outbound peers 小于 `ANCHOR_PEERS` 执行 2， 否则执行 3\r\n2. 选择一个锚点 peer:\r\n    1. 从 PeerStore 挑选最后连接过的 `max_bound` 个 outbound peers 作为 `recent_peers`\r\n    2. 如果 `recent_peers` 为空则执行 3，否则从 `recent_peers` 中选择分数最高的节点作为 outbound peer 返回\r\n3. 在 PeerStore 中随机选择一个分数大于 `TRY_SCORE` 且 `NetworkGroup` 和当前连接的 outbound peers 都不相同的 peer info，如果找不到这样的 peer info 则执行 5，否则将这个 peer info 返回\r\n4. 从 `boot_nodes` 中随机选择一个返回\r\n\r\n伪代码\r\n\r\n``` ruby\r\n# 找到一个 outbound peer 候选\r\ndef find_outbound_peer\r\n  connected_outbound_peers = connected_peers.select{|peer| peer.outbound? && !peer.feeler? }\r\n  if connected_outbound_peers.length < ANCHOR_PEERS\r\n    find_anchor_peer() || find_random_peer() || random_boot_node()\r\n  else\r\n    find_random_peer() || random_boot_node()\r\n  end\r\nend\r\n\r\ndef find_anchor_peer\r\n  last_connected_peers = peer_store.sort_by{|peer| -peer.last_connected_at}.take(max_bound)\r\n  # 返回最高分的 peer info\r\n  last_connected_peers.sort_by(&:score).last\r\nend\r\n\r\ndef find_random_peer\r\n  connected_outbound_peers = connected_peers.select{|peer| peer.outbound? && !peer.feeler? }\r\n  exists_network_groups = connected_outbound_peers.map(&:network_group)\r\n  candidate_peers = peer_store.select do |peer| \r\n    peer.score >= TRY_SCORE && !exists_network_groups.include?(peer.network_group)\r\n  end\r\n  candidate_peers.sample\r\nend\r\n\r\ndef random_boot_node\r\n  boot_nodes.sample\r\nend\r\n```\r\n\r\n\r\n节点应该重复以上过程，直到节点正在连接的 outbound peers 数量大于等于 `max_outbound` 并且 `try_new_outbound_peer` 为 `false`。\r\n\r\n``` ruby\r\ncheck_outbound_peers_interval = 15\r\n# 每隔几分钟检查 outbound peers 数量\r\nloop do\r\n  sleep(check_outbound_peers_interval)\r\n  connected_outbound_peers = connected_peers.select{|peer| peer.outbound? && !peer.feeler? }\r\n  if connected_outbound_peers.length >= max_outbound && !try_new_outbound_peer \r\n    next\r\n  end\r\n  new_outbound_peer = find_outbound_peer()\r\n  connect_peer(new_outbound_peer)\r\nend\r\n```\r\n\r\n`try_new_outbound_peer` 的作用是在一定时间内无法发现有效消息时，允许节点连接更多的 outbound peers，这个机制在后文介绍。\r\n\r\n该策略在节点没有 Peers 时会强制从最近连接过的 outbound peers 中选择，这个行为参考了[日蚀攻击论文][2]中的 Anchor Connection 策略。\r\n\r\n攻击者需要做到以下条件才可以成功实施日蚀攻击\r\n\r\n1. 攻击者有 `n` 个伪装节点(`n == ANCHOR_PEERS`) 成为受害者节点的 outbound peers，这些伪装节点同时要拥有最高得分\r\n2. 攻击者需要准备至少 `max_outbound - ANCHOR_PEERS` 个伪装节点地址在受害者节点的 PeerStore，并且受害者节点的随机挑选的 `max_outbound - ANCHOR_PEERS` 个 outbound peers 全部是攻击者的伪装节点。\r\n\r\n#### 额外的 outbound peers 连接和驱逐\r\n\r\n网络组件应该每隔几分钟检测子协议中的主要协议如 `sync` 协议是否工作\r\n\r\n``` ruby\r\ndef sync_maybe_stale\r\n  now = Time.now\r\n  # 可以通过上次 Tip 更新时间，出块间隔和当前时间判断 sync 是否正常工作\r\n  last_tip_updated_at < now - block_produce_interval * n\r\nend\r\n```\r\n\r\n当我们发现 `sync` 协议无法正常工作时，应该设置 `try_new_outbound_peer` 变量为 `true`，当发现 `sync` 协议恢复正常时设置 `try_new_outbound_peer` 为 `false`\r\n\r\n``` ruby\r\ncheck_sync_stale_at = Time.now\r\nloop_interval = 30\r\ncheck_sync_stale_interval = 15 * 60 #(15 minutes)\r\n\r\nloop do\r\n  sleep(loop_interval)\r\n  # try evict\r\n  evict_extra_outbound_peers()\r\n  now = Time.now\r\n  if check_sync_stale_at >= now\r\n    set_try_new_outbound_peer(sync_maybe_stale())\r\n    check_sync_stale_at = now + check_sync_stale_interval\r\n  end\r\nend\r\n```\r\n\r\n当 `try_new_outbound_peer` 为 `true` 时 CKB 网络将会持续的尝试连接额外的 outbound peers，并每隔几分钟尝试逐出没有用的额外 outbound peers，这个行为防止节点有过多的连接。\r\n\r\n``` ruby\r\ndef evict_extra_outbound_peers\r\n  connected_outbound_peers = connected_peers.select{|peer| peer.outbound? && !peer.feeler? }\r\n  if connected_outbound_peers.length <= max_outbound\r\n    return\r\n  end\r\n  now = Time.now\r\n  # 找出连接的 outbound peers 中 last_block_announcement_at 最老的 peer\r\n  evict_target = connected_outbound_peers.sort_by do |peer|\r\n    peer.last_block_announcement_at\r\n  end.first\r\n  if evict_target\r\n    # 至少连接上这个 peer 一段时间，且当前没有从这个 peer 下载块\r\n    if now - evict_target.last_connected_at > MINIMUM_CONNECT_TIME && !is_downloading?(evict_target)\r\n      disconnect_peer(evict_target)\r\n      # 防止连接过多的 outbound peer\r\n      set_try_new_outbound_peer(false)\r\n    end\r\n  end\r\nend\r\n```\r\n\r\n\r\n### 节点 inbound peers 接受机制\r\n\r\n比特币中当节点的被动 peers 连满同时又有新 peer 尝试连接时，节点会对已有 peers 进行驱逐测试(详细请参考 [Bitcoin 源码][1])。\r\n\r\n驱逐测试的目的在于节点保留高质量 peer 的同时，驱逐低质量的 peer。\r\n\r\nCKB 参考了比特币的驱逐测试，步骤如下:\r\n\r\n1. 找出当前连接的所有 inbound peers 作为 `candidate_peers`\r\n2. 保护 peers (`N` 代表每一步中我们想要保护的 peers 数量):\r\n    1. 从 `candidate_peers` 找出 `N` 个分数最高的 peers 删除\r\n    2. 从 `candidate_peers` 找出 `N` 个 ping 最小的 peers 删除\r\n    3. 从 `candidate_peers` 找出 `N` 个最近发送消息给我们的 peers 删除\r\n    4. 从 `candidate_peers` 找出 `candidate_peers.size / 2` 个连接时间最久的 peers 删除\r\n3. 按照 `network group` 对剩余的 `candidate_peers` 分组\r\n4. 找出包含最多 peers 的组\r\n5. 驱逐组中分数最低的 peer，找不到 peer 驱逐时则拒绝新 peer 的连接\r\n\r\n我们基于攻击者难以模拟或操纵的特征来保护一些 peers 免受驱逐，以增强网络的安全性。\r\n\r\n### Feeler Connection\r\n\r\nFeeler Connection 机制的目的在于测试 Peer 是否可以连接。\r\n\r\n当节点的 outbound peers 数量达到 `max_outbound` 限制时，\r\n节点会每隔一段时间(一般是几分钟)主动发起 feeler connection：\r\n\r\n1. 从 PeerStore 中随机选出一个未连接过的 peer info\r\n2. 连接该 peer\r\n3. 执行握手协议\r\n4. 断开连接\r\n\r\nFeeler peer 会被假设为很快断开连接\r\n\r\n### PeerStore 清理\r\n\r\n设置一些参数：\r\n`PEER_STORE_LIMIT` - PeerStore 最多可以存储的 PeerInfo 数量\r\n`PEER_NOT_SEEN_TIMEOUT` - 用于判断 peer info 是否该被清理，如该值设为 15 天，则表示最近 15 天内连接过的 peer 不会被清理\r\n\r\nPeerStore 中存储的 PeerInfo 数量达到 `PEER_STORE_LIMIT` 时需要清理，过程如下：\r\n\r\n1. 按照 `network group` 给 PeerStore 中的 PeerInfo 分组\r\n2. 找出包含最多节点的组\r\n3. 在组中搜索最近没有连接过的 peers `peer.last_connected_at < Time.now - PEER_NOT_SEEN_TIMEOUT`\r\n4. 在该集合中找到分数最低的 PeerInfo `candidate_peer_info`\r\n5. 如果 `candidate_peer_info.score < new_peer_info.score` 则删掉 `candidate_peer_info` 并插入 `new_peer_info`，否则不接受 `new_peer_info`\r\n\r\n\r\n## 参考\r\n\r\n1. [Bitcoin source code][1]\r\n2. [Eclipse Attacks on Bitcoin’s Peer-to-Peer Network][2]\r\n\r\n[1]: https://github.com/bitcoin/bitcoin\r\n[2]: https://eprint.iacr.org/2015/263.pdf\r\n","\\0008-serialization\\0008-serialization.md":"---\r\nNumber: \"0008\"\r\nCategory: Standards Track\r\nStatus: Proposal\r\nAuthor: Ian Yang\r\nOrganization: Nervos Foundation\r\nCreated: 2018-12-17\r\n---\r\n\r\n# Serialization\r\n\r\nCKB use two major serialization format, CFB and JSON.\r\n\r\n[CFB][cfb] (Canonical FlatBuffers) is a restricted variant of FlatBuffers for producing unequivocal transfer syntax. Since CFB generated binary is still valid FlatBuffers, any FlatBuffers reader can parse the serialized messages, but only CFB builder can serialize messages into valid binary.\r\n\r\nCFB is in the proposal stage, and is not ready yet. Now plain FlatBuffers is\r\nused to serialize P2P messages.\r\n\r\n[JSON][json] is used in node RPC service via [JSON-RPC][jsonrpc].\r\n\r\n[cfb]: https://github.com/nervosnetwork/rfcs/pull/47\r\n[json]: https://www.json.org\r\n[jsonrpc]: https://www.jsonrpc.org/specification\r\n","\\0009-vm-syscalls\\0009-vm-syscalls.md":"---\r\nNumber: \"0009\"\r\nCategory: Standards Track\r\nStatus: Proposal\r\nAuthor: Xuejie Xiao\r\nOrganization: Nervos Foundation\r\nCreated: 2018-12-14\r\n---\r\n\r\n# VM Syscalls\r\n\r\n## Abstract\r\n\r\nThis document describes all the RISC-V VM syscalls implemented in CKB so far.\r\n\r\n## Introduction\r\n\r\nCKB VM syscalls are used to implement communications between the RISC-V based CKB VM, and the main CKB process, allowing scripts running in the VM to read current transaction information as well as general blockchain information from CKB. Leveraging syscalls instead of custom instructions allow us to maintain a standard compliant RISC-V implementation which can embrace the broadest industrial support.\r\n\r\n## Specification\r\n\r\nIn CKB we use RISC-V's standard syscall solution: each syscall accepts 6 arguments stored in register `A0` through `A5`. Each argument here is of register word size so it can store either regular integers or pointers. The syscall number is stored in `A7`. After all the arguments and syscall number are set, `ecall` instruction is used to trigger syscall execution, CKB VM then transfers controls from the VM to the actual syscall implementation beneath. For example, the following RISC-V assembly would trigger *Exit* syscall with a return code of 10:\r\n\r\n```\r\nli a0, 10\r\nli a7, 93\r\necall\r\n```\r\n\r\nAs shown in the example, not all syscalls use all the 6 arguments. In this case the caller side can only fill in the needed arguments.\r\n\r\nSyscalls can respond to the VM in 2 ways:\r\n\r\n* A return value is put in `A0` if exists.\r\n* Syscalls can also write data in memory location pointed by certain syscall arguments, so upon syscall completion, normal VM instructions can read the data prepared by the syscall.\r\n\r\nFor convenience, we could wrap the logic of calling a syscall in a C function:\r\n\r\n```c\r\nstatic inline long\r\n__internal_syscall(long n, long _a0, long _a1, long _a2, long _a3, long _a4, long _a5)\r\n{\r\n  register long a0 asm(\"a0\") = _a0;\r\n  register long a1 asm(\"a1\") = _a1;\r\n  register long a2 asm(\"a2\") = _a2;\r\n  register long a3 asm(\"a3\") = _a3;\r\n  register long a4 asm(\"a4\") = _a4;\r\n  register long a5 asm(\"a5\") = _a5;\r\n\r\n  register long syscall_id asm(\"a7\") = n;\r\n\r\n  asm volatile (\"scall\"\r\n\t\t: \"+r\"(a0) : \"r\"(a1), \"r\"(a2), \"r\"(a3), \"r\"(a4), \"r\"(a5), \"r\"(syscall_id));\r\n\r\n  return a0;\r\n}\r\n\r\n#define syscall(n, a, b, c, d, e, f) \\\r\n        __internal_syscall(n, (long)(a), (long)(b), (long)(c), (long)(d), (long)(e), (long)(f))\r\n```\r\n\r\n(NOTE: this is adapted from [riscv-newlib](https://github.com/riscv/riscv-newlib/blob/77e11e1800f57cac7f5468b2bd064100a44755d4/libgloss/riscv/internal_syscall.h#L25))\r\n\r\nNow we can trigger the same *Exit* syscall more easily in C code:\r\n\r\n```c\r\nsyscall(93, 10, 0, 0, 0, 0, 0);\r\n```\r\n\r\nNote that even though *Exit* syscall only needs one argument, our C wrapper requires us to fill in all 6 arguments. We can initialize other unused arguments as all 0. Below we would illustrate each syscall with a C function signature to demonstrate each syscall's accepted arguments. Also for clarifying reason, all the code shown in this RFC is assumed to be written in pure C.\r\n\r\n- [Exit]\r\n- [Load Transaction Hash]\r\n- [Load Script Hash]\r\n- [Load Cell]\r\n- [Load Cell By Field]\r\n- [Load Input]\r\n- [Load Input By Field]\r\n- [Load Header]\r\n- [Debug]\r\n\r\n### Exit\r\n[exit]: #exit\r\n\r\nAs shown above, *Exit* syscall has a signature like following:\r\n\r\n```c\r\nvoid exit(int8_t code)\r\n{\r\n  syscall(93, code, 0, 0, 0, 0, 0);\r\n}\r\n```\r\n\r\n*Exit* syscall don't need a return value since CKB VM is not supposed to return from this function. Upon receiving this syscall, CKB VM would terminate execution with the specified return code. This is the only way of correctly exiting a script in CKB VM.\r\n\r\n### Load Transaction Hash\r\n[load transaction hash]: #load-transaction-hash\r\n\r\n*Load Transaction Hash* syscall has a signature like following:\r\n\r\n```c\r\nint ckb_load_tx_hash(void* addr, uint64_t* len, size_t offset)\r\n{\r\n  return syscall(2061, addr, len, offset, 0, 0, 0);\r\n}\r\n```\r\n\r\nThe arguments used here are:\r\n\r\n* `addr`: a pointer to a buffer in VM memory space denoting where we would load the serialized transaction data.\r\n* `len`: a pointer to a 64-bit unsigned integer in VM memory space, when calling the syscall, this memory location should store the length of the buffer specified by `addr`, when returning from the syscall, CKB VM would fill in `len` with the actual length of the buffer. We would explain the exact logic below.\r\n* `offset`: an offset specifying from which offset we should start loading the serialized transaction data.\r\n\r\nThis syscall would calculate the hash of current transaction and copy it to VM memory space.\r\n\r\nThe result is fed into VM via the steps below. For ease of reference, we refer the result as `data`, and the length of `data` as `data_length`.\r\n\r\n1. A memory read operation is executed to read the value in `len` pointer from VM memory space, we call the read result `size` here.\r\n2. `full_size` is calculated as `data_length - offset`.\r\n3. `real_size` is calculated as the minimal value of `size` and `full_size`\r\n4. The serialized value starting from `&data[offset]` till `&data[offset + real_size]` is written into VM memory space location starting from `addr`.\r\n5. `full_size` is written into `len` pointer\r\n6. `0` is returned from the syscall denoting execution success.\r\n\r\nThe whole point of this process, is providing VM side a way to do partial reading when the available memory is not enough to support reading the whole data altogether.\r\n\r\nOne trick here, is that by providing `NULL` as `addr`, and a `uint64_t` pointer with 0 value as `len`, this syscall can be used to fetch the length of the serialized data part without reading any actual data.\r\n\r\n### Load Script Hash\r\n[load script hash]: #load-script-hash\r\n\r\n*Load Script Hash* syscall has a signature like following:\r\n\r\n```c\r\nint ckb_load_script_hash(void* addr, uint64_t* len, size_t offset)\r\n{\r\n  return syscall(2062, addr, len, offset, 0, 0, 0);\r\n}\r\n```\r\n\r\nThe arguments used here are:\r\n\r\n* `addr`: the exact same `addr` pointer as used in *Load Transaction Hash* syscall.\r\n* `len`: the exact same `len` pointer as used in *Load Transaction Hash* syscall.\r\n* `offset`: the exact same `offset` value as used in *Load Transaction Hash* syscall.\r\n\r\nThis syscall would calculate the hash of current running script and copy it to VM memory space. This is the only part a script can load on its own cell.\r\n\r\n### Load Cell\r\n[load cell]: #load-cell\r\n\r\n*Load Cell* syscall has a signature like following:\r\n\r\n```c\r\nint ckb_load_cell(void* addr, uint64_t* len, size_t offset, size_t index, size_t source)\r\n{\r\n  return syscall(2071, addr, len, offset, index, source, 0);\r\n}\r\n```\r\n\r\nThe arguments used here are:\r\n\r\n* `addr`: the exact same `addr` pointer as used in *Load Transaction Hash* syscall.\r\n* `len`: the exact same `len` pointer as used in *Load Transaction Hash* syscall.\r\n* `offset`: the exact same `offset` value as used in *Load Transaction Hash* syscall.\r\n* `index`: an index value denoting the index of cells to read.\r\n* `source`: a flag denoting the source of cells to locate, possible values include:\r\n    + 1: input cells.\r\n    + 2: output cells.\r\n    + 3: dep cells.\r\n\r\nThis syscall would locate a single cell in the current transaction based on `source` and `index` value, serialize the whole cell into the CFB Encoding [1] format, then use the same step as documented in *Load Transaction Hash* syscall to feed the serialized value into VM.\r\n\r\nSpecifying an invalid source value here would immediately trigger a VM error, specifying an invalid index value here, however, would result in `1` as return value, denoting the index used is out of bound. Otherwise the syscall would return `0` denoting success state.\r\n\r\nNote this syscall is only provided for advanced usage that requires hashing the whole cell in a future proof way. In practice this is a very expensive syscall since it requires serializing the whole cell, in the case of a large cell with huge data, this would mean a lot of memory copying. Hence CKB should charge much higher cycles for this syscall and encourage using *Load Cell By Field* syscall below.\r\n\r\n### Load Cell By Field\r\n[load cell by field]: #load-cell-by-field\r\n\r\n*Load Cell By Field* syscall has a signature like following:\r\n\r\n```c\r\nint ckb_load_cell_by_field(void* addr, uint64_t* len, size_t offset,\r\n                           size_t index, size_t source, size_t field)\r\n{\r\n  return syscall(2081, addr, len, offset, index, source, field);\r\n}\r\n```\r\n\r\nThe arguments used here are:\r\n\r\n* `addr`: the exact same `addr` pointer as used in *Load Transaction Hash* syscall.\r\n* `len`: the exact same `len` pointer as used in *Load Transaction Hash* syscall.\r\n* `offset`: the exact same `offset` value as used in *Load Transaction Hash* syscall.\r\n* `index`: an index value denoting the index of cells to read.\r\n* `source`: a flag denoting the source of cells to locate, possible values include:\r\n    + 1: input cells.\r\n    + 2: output cells.\r\n    + 3: dep cells.\r\n* `field`: a flag denoting the field of the cell to read, possible values include:\r\n    + 0: capacity.\r\n    + 1: data.\r\n    + 2: data hash.\r\n    + 3: lock.\r\n    + 4: lock hash.\r\n    + 5: type.\r\n    + 6: type hash.\r\n\r\nThis syscall would locate a single cell in current transaction just like *Load Cell* syscall, but what's different, is that this syscall would only extract a single field in the specified cell based on `field`, then serialize the field into binary format with the following rules:\r\n\r\n* `capacity`: capacity is serialized into 8 little endian bytes, this is also how CFB Encoding [1] handles 64-bit unsigned integers.\r\n* `data`: data field is already in binary format, we can just use it directly, there's no need for further serialization\r\n* `data hash`: 32 raw bytes are extracted from `H256` structure by serializing data field\r\n* `lock`: lock script is serialized into the CFB Encoding [1] format\r\n* `lock hash`: 32 raw bytes are extracted from `H256` structure and used directly\r\n* `type`: type script is serialized into the CFB Encoding [1] format\r\n* `type hash`: 32 raw bytes are extracted from `H256` structure and used directly\r\n\r\nWith the binary result converted from different rules, the syscall then applies the same steps as documented in *Load Transaction Hash* syscall to feed data into CKB VM.\r\n\r\nSpecifying an invalid source value here would immediately trigger a VM error, specifying an invalid index value here, would result in `1` as return value, denoting index is out of bound. Specifying any invalid field will also trigger VM error immediately. Otherwise the syscall would return `0` denoting success state. Specifying a field that doesn't not exist(such as type on a cell without type script) would result in `2` as return value, denoting the item is missing.\r\n\r\n### Load Input\r\n[load input]: #load-input\r\n\r\n*Load Input* syscall has a signature like following:\r\n\r\n```c\r\nint ckb_load_input(void* addr, uint64_t* len, size_t offset,\r\n                   size_t index, size_t source)\r\n{\r\n  return syscall(2073, addr, len, offset, index, source, 0);\r\n}\r\n```\r\n\r\nThe arguments used here are:\r\n\r\n* `addr`: the exact same `addr` pointer as used in *Load Transaction Hash* syscall.\r\n* `len`: the exact same `len` pointer as used in *Load Transaction Hash* syscall.\r\n* `offset`: the exact same `offset` value as used in *Load Transaction Hash* syscall.\r\n* `index`: an index value denoting the index of inputs to read.\r\n* `source`: a flag denoting the source of inputs to locate, possible values include:\r\n    + 1: inputs.\r\n    + 2: outputs, note this is here to maintain compatibility of `source` flag, when this value is used in *Load Input By Field* syscall, the syscall would always return `2` since output doesn't have any input fields.\r\n    + 3: deps, when this value is used, the syscall will also always return `2` since dep doesn't have input fields.\r\n\r\nThis syscall would locate a single input field in the current transaction based on `source` and `index` value, serialize the whole input into the CFB Encoding [1] format, then use the same step as documented in *Load Transaction Hash* syscall to feed the serialized value into VM.\r\n\r\nSpecifying an invalid source value here would immediately trigger a VM error. Specifying a valid source that is not input, such as an output, would result in `2` as return value, denoting the item is missing. Specifying an invalid index value here would result in `1` as return value, denoting the index used is out of bound. Otherwise the syscall would return `0` denoting success state.\r\n\r\nNote this syscall is only provided for advanced usage that requires hashing the whole input in a future proof way. In practice this might be a very expensive syscall since it requires serializing the whole input, in the case of a large input with huge arguments, this would mean a lot of memory copying. Hence CKB should charge much higher cycles for this syscall and encourage using *Load Input By Field* syscall below.\r\n\r\n### Load Input By Field\r\n[load input by field]: #load-input-by-field\r\n\r\n*Load Input By Field* syscall has a signature like following:\r\n\r\n```c\r\nint ckb_load_input_by_field(void* addr, uint64_t* len, size_t offset,\r\n                            size_t index, size_t source, size_t field)\r\n{\r\n  return syscall(2083, addr, len, offset, index, source, field);\r\n}\r\n```\r\n\r\nThe arguments used here are:\r\n\r\n* `addr`: the exact same `addr` pointer as used in *Load Transaction Hash* syscall.\r\n* `len`: the exact same `len` pointer as used in *Load Transaction Hash* syscall.\r\n* `offset`: the exact same `offset` value as used in *Load Transaction Hash* syscall.\r\n* `index`: an index value denoting the index of inputs to read.\r\n* `source`: a flag denoting the source of inputs to locate, possible values include:\r\n    + 1: inputs.\r\n    + 2: outputs, note this is here to maintain compatibility of `source` flag, when this value is used in *Load Input By Field* syscall, the syscall would always return `2` since output doesn't have any input fields.\r\n    + 3: deps, when this value is used, the syscall will also always return `2` since dep doesn't have input fields.\r\n* `field`: a flag denoting the field of the input to read, possible values include:\r\n    + 0: args.\r\n    + 1: out_point.\r\n    + 2: since.\r\n\r\nThis syscall would first locate an input in current transaction via `source` and `index` value, it then extract the field (and serialize it if it's `args` or `out_point` into CFB format), then use the same steps as documented in *Load Transaction Hash* syscall to feed data into VM.\r\n\r\nSpecifying an invalid source value here would immediately trigger a VM error, specifying an invalid index value here, however, would result in `1` as return value, denoting the index used is out of bound. Specifying any invalid field will also trigger VM error immediately. Otherwise the syscall would return `0` denoting success state.\r\n\r\nNOTE there is one quirk when requesting `args` part in `deps`: since CFB doesn't allow using a vector as the root type, we have to wrap `args` in a `CellInput` table, and provide the `CellInput` table as the CFB root type instead.\r\n\r\n### Load Header\r\n[load header]: #load-header\r\n\r\n*Load Header* syscall has a signature like following:\r\n\r\n```c\r\nint ckb_load_header(void* addr, uint64_t* len, size_t offset, size_t index, size_t source)\r\n{\r\n  return syscall(2072, addr, len, offset, index, source, 0);\r\n}\r\n```\r\n\r\nThe arguments used here are:\r\n\r\n* `addr`: the exact same `addr` pointer as used in *Load Transaction Hash* syscall.\r\n* `len`: the exact same `len` pointer as used in *Load Transaction Hash* syscall.\r\n* `offset`: the exact same `offset` value as used in *Load Transaction Hash* syscall.\r\n* `index`: an index value denoting the index of cells to read.\r\n* `source`: a flag denoting the source of cells to locate, possible values include:\r\n    + 1: input cells.\r\n    + 2: output cells.\r\n    + 3: dep cells.\r\n\r\nThis syscall would locate the header associated with an input or a dep OutPoint based on `source` and `index` value, serialize the whole header into CFB Encoding [1] format, then use the same step as documented in *Load Transaction Hash* syscall to feed the serialized value into VM.\r\n\r\nSpecifying an invalid source value here would immediately trigger a VM error. Specifying `output` as source field would result in `2` as return value, denoting item missing state. Specifying an invalid index value here, however, would result in `1` as return value, denoting the index used is out of bound. Otherwise the syscall would return `0` denoting success state.\r\n\r\n### Debug\r\n[debug]: #debug\r\n\r\n*Debug* syscall has a signature like following:\r\n\r\n```c\r\nvoid ckb_debug(const char* s)\r\n{\r\n  syscall(2177, s, 0, 0, 0, 0, 0);\r\n}\r\n```\r\n\r\nThis syscall accepts a null terminated string and prints it out as debug log in CKB. It can be used as a handy way to debug scripts in CKB. This syscall has no return value.\r\n\r\n# Reference\r\n\r\n* [1]: CFB Encoding, *citation link pending*\r\n","\\0010-eaglesong\\0010-eaglesong.md":"---\r\nNumber: \"0010\"\r\nCategory: Standards Track\r\nStatus: Proposal\r\nAuthor: Alan Szepieniec\r\nOrganization: Nervos Foundation\r\nCreated: 2019-07-18\r\n---\r\n\r\n# Eaglesong (Proof-of-Work Function for Nervos CKB)\r\n\r\nThis document specifies the Eaglesong hash function as it is to be used in the context of Nervos CKB proof-of-work.\r\n\r\n * [Notation](#Notation)\r\n * [Design Strategies](#Desgin-Strategies)\r\n   * [Sponge Construction](#Sponge-Construction)\r\n   * [ARX](#ARX)\r\n * [Round Function](#Round-Function)\r\n   * [Bit Matrix](#Bit-Matrix)\r\n   * [Circulant Multiplication](#Circulant-Multiplication)\r\n   * [Injection of Constants](#Injection-of-Constants)\r\n   * [Addition-Rotation-Addition](#Addition-Rotation-Addition)\r\n   * [Full Permutation](#Full-Permutation)\r\n * [Eaglesong Hash](#Eaglesong-Hash)\r\n * [Reference Implementations](#Reference-Implementations)\r\n\r\n\r\n<a name=\"Notation\"></a>\r\n## Notation\r\nIn the pseudocode snippets below the following notation is used:\r\n\r\n - `//` -- denotes a comment\r\n - `||` -- denotes concatenation (of bit strings)\r\n - `length` -- returns the length (in bits) of the argument\r\n - `%` -- modulo operator, computes the remainder of left-hand-side argument after division by the right-hand-side\r\n - `zeros` -- produces a string of zero-bits of the specified length\r\n - `xor` -- denotes the exclusive-or operation of equal-length bit strings\r\n - `[]` -- python-style array indexing, with indexation starting (as it should) from zero; and when an upper-bound is included the range until but not including that upper bound is denoted; when not juxtaposed to an array, the expression `[a:b]` denotes the list of integers `{a, a+1, ..., b-1}`.\r\n\r\n<a name=\"Design-Strategies\"></a>\r\n# Design Strategies\r\n\r\n<a name=\"Sponge-Construction\"></a>\r\n## Sponge Construction\r\n\r\nEaglesong is a [sponge construction](https://en.wikipedia.org/wiki/Sponge_function). This means that what is specified is a permutation `F` along with three variables:\r\n\r\n - `r`, the rate : a positive integer;\r\n - `c`, the capacity : another positive integer;\r\n - `d`, the delimiter : a byte fixed to a nonzero value.\r\n\r\nThe permutation maps `r+c` bits to `r+c` bits. After appending the delimiter byte to variable-length input to the hash function, this input is chunked into pieces `chunk[i]`, each of `r` bits along with the last one padded with zeros as necessary. The sponge construction defines a state, which consists of `r+c` bits initialized to zeros. The absorbing phase iterates over all chunks and xors the current chunk into the top `r` bits of the state before applying the permutation. In the squeezing phase, the output buffer is initialized to the empty string. Until this buffer is of the right size, the permutation is applied to the state and the top `r` bits are read out and appended to the output buffer. If the output buffer is too long, it is truncated. The next pseudocode illustrates this operation.\r\n\r\n```\r\nfunction sponge( input, output_length ):\r\n    // append delimiter and pad as necessary\r\n    input = input || d\r\n    while length(input) % r =/= 0 do:\r\n        input = input || 0\r\n\r\n    // split\r\n    for i in [0, length(input) / r] do:\r\n        chunks[i] = input[i*r : (i+1)*r]\r\n\r\n    // initialize state\r\n    state = zeros(r+c)\r\n\r\n    // absorb\r\n    for chunk in chunks do:\r\n        state[0:r] = state[0:r] xor chunk\r\n        state = F(state)\r\n\r\n    // squeeze\r\n    output = \"\"\r\n    while length(output) <= output_length do:\r\n        output = output || state[0:r]\r\n        state = F(state)\r\n\r\n    return output[0:output_length]\r\n```\r\n\r\n<a name=\"ARX\"></a>\r\n## ARX\r\n\r\nEaglesong is member of a family of ciphers that use only three operations: addition, rotation, and xor. The operands are always 32-bit integers. This motivates the following notational overload:\r\n\r\n - `+`: addition modulo 2^32;\r\n - `<<<`: left rotation, specifically `a <<< r` is equivalent to `((a << r) xor (a >> (32-r))) and 0xffffffff`, where `<<` and `>>` denote left and right shift, respectively.\r\n\r\n<a name=\"Round-Function\"></a>\r\n# Round Function\r\n\r\nThe Eaglesong permutation F is obtained by iterating a round function f for `N = 43` times. The round function consists of four steps.\r\n\r\nThe state of the Eaglesong permutation consists of 16 integers.\r\n\r\n<a name=\"Bit-Matrix\"></a>\r\n## Bit Matrix\r\n\r\nThe state vector is mapped to a new state vector determined via (row-)vector-matrix multiplication, where the coefficients of the matrix are either 0 or 1. Every row of this matrix represents an indication of which state elements to xor together.\r\n\r\nThe matrix is given below. A multiplication procedure follows.\r\n\r\n```\r\nbit_matrix = [\r\n    [1 1 1 1 0 1 0 1 1 1 1 1 0 0 0 1]\r\n    [0 1 1 1 1 0 1 0 1 1 1 1 1 0 0 1]\r\n    [0 0 1 1 1 1 0 1 0 1 1 1 1 1 0 1]\r\n    [0 0 0 1 1 1 1 0 1 0 1 1 1 1 1 1]\r\n    [1 1 1 1 1 0 1 0 1 0 1 0 1 1 1 0]\r\n    [1 0 0 0 1 0 0 0 1 0 1 0 0 1 1 1]\r\n    [1 0 1 1 0 0 0 1 1 0 1 0 0 0 1 0]\r\n    [1 0 1 0 1 1 0 1 0 0 1 0 0 0 0 1]\r\n    [0 1 0 1 0 1 1 0 1 0 0 1 0 0 0 1]\r\n    [0 0 1 0 1 0 1 1 0 1 0 0 1 0 0 1]\r\n    [0 0 0 1 0 1 0 1 1 0 1 0 0 1 0 1]\r\n    [0 0 0 0 1 0 1 0 1 1 0 1 0 0 1 1]\r\n    [1 1 1 1 0 0 0 0 1 0 0 1 1 0 0 0]\r\n    [0 1 1 1 1 0 0 0 0 1 0 0 1 1 0 0]\r\n    [0 0 1 1 1 1 0 0 0 0 1 0 0 1 1 0]\r\n    [1 1 1 0 1 0 1 1 1 1 1 0 0 0 1 1]\r\n]\r\n\r\nfunction ApplyBitMatrix( state_vector ):\r\n    for j in range[0:16] do:\r\n        output_vector[j] = 0\r\n        for k in range[0:16] do:\r\n            output_vector[j] = output_vector[j] xor (bit_matrix[j,k] * state_vector[k])\r\n    return output_vector\r\n```\r\n\r\nNote that the matrix multiplication needs involve only ARX operations. Since the coefficients of the matrix are known, the loops can be unrolled and the entire matrix multiplication can be described in terms of the xor operation alone. The use of the multiplication operator `*` is merely a notational convenience to stress the link with matrix multiplication.\r\n\r\n<a name=\"Circulant-Multiplication\"></a>\r\n## Circulant Multiplication\r\n\r\nEvery element of the state vector undergoes a map involving xor and rotation. Specifically, `state_vector[i] = state_vector[i] xor (state_vector[i] <<< a[i]) xor (state_vector[i] <<< b[i])`. The coefficients `a` and `b` are different for every component. They are given below, with a procedure for computing the entire step.\r\n\r\n```\r\ncoefficients = [\r\n    [0, 2, 4],\r\n    [0, 13, 22],\r\n    [0, 4, 19],\r\n    [0, 3, 14],\r\n    [0, 27, 31],\r\n    [0, 3, 8],\r\n    [0, 17, 26],\r\n    [0, 3, 12],\r\n    [0, 18, 22],\r\n    [0, 12, 18],\r\n    [0, 4, 7],\r\n    [0, 4, 31],\r\n    [0, 12, 27],\r\n    [0, 7, 17],\r\n    [0, 7, 8],\r\n    [0, 1, 13]\r\n]\r\n\r\nfunction ApplyCirculantMultiplication( state_vector ):\r\n    for i in [0:16] do:\r\n        output_vector[i] = 0\r\n        for k in [0:3] do:\r\n            output_vector[i] = output_vector[i] xor (state_vector[i] <<< coefficients[i][k])\r\n    return output_vector\r\n```\r\n\r\n<a name=\"Injection-of-Constants\"></a>\r\n## Injection of Constants\r\n\r\nA given constant is xored into every component of the state vector. The coefficients differ per component and per round. They are determined from using Keccak to expand the ASCII string \r\n\r\n```\r\nThe various ways in which the knowledge on which people base their plan is communicated to them is the crucial problem for any theory explaining the economic process, and the problem of what is the best way to utilizing knowledge initially dispersed among all the people is at least one of the main problems of economic policy - or of designing an efficient economic system.\r\n```\r\n\r\nThe constants are given below, along with a procedure to compute the entire step. Attached to this RFC is a [python script](constants.py) ([and one for keccak](CompactFIPS202.py)) to compute the constants.\r\n\r\n```\r\ninjection_constants = [\r\n0x6e9e40ae ,  0x71927c02 ,  0x9a13d3b1 ,  0xdaec32ad ,  0x3d8951cf ,  0xe1c9fe9a ,  0xb806b54c ,  0xacbbf417 ,  \r\n0xd3622b3b ,  0xa082762a ,  0x9edcf1c0 ,  0xa9bada77 ,  0x7f91e46c ,  0xcb0f6e4f ,  0x265d9241 ,  0xb7bdeab0 ,  \r\n0x6260c9e6 ,  0xff50dd2a ,  0x9036aa71 ,  0xce161879 ,  0xd1307cdf ,  0x89e456df ,  0xf83133e2 ,  0x65f55c3d ,  \r\n0x94871b01 ,  0xb5d204cd ,  0x583a3264 ,  0x5e165957 ,  0x4cbda964 ,  0x675fca47 ,  0xf4a3033e ,  0x2a417322 ,  \r\n0x3b61432f ,  0x7f5532f2 ,  0xb609973b ,  0x1a795239 ,  0x31b477c9 ,  0xd2949d28 ,  0x78969712 ,  0x0eb87b6e ,  \r\n0x7e11d22d ,  0xccee88bd ,  0xeed07eb8 ,  0xe5563a81 ,  0xe7cb6bcf ,  0x25de953e ,  0x4d05653a ,  0x0b831557 ,  \r\n0x94b9cd77 ,  0x13f01579 ,  0x794b4a4a ,  0x67e7c7dc ,  0xc456d8d4 ,  0x59689c9b ,  0x668456d7 ,  0x22d2a2e1 ,  \r\n0x38b3a828 ,  0x0315ac3c ,  0x438d681e ,  0xab7109c5 ,  0x97ee19a8 ,  0xde062b2e ,  0x2c76c47b ,  0x0084456f ,  \r\n0x908f0fd3 ,  0xa646551f ,  0x3e826725 ,  0xd521788e ,  0x9f01c2b0 ,  0x93180cdc ,  0x92ea1df8 ,  0x431a9aae ,  \r\n0x7c2ea356 ,  0xda33ad03 ,  0x46926893 ,  0x66bde7d7 ,  0xb501cc75 ,  0x1f6e8a41 ,  0x685250f4 ,  0x3bb1f318 ,  \r\n0xaf238c04 ,  0x974ed2ec ,  0x5b159e49 ,  0xd526f8bf ,  0x12085626 ,  0x3e2432a9 ,  0x6bd20c48 ,  0x1f1d59da ,  \r\n0x18ab1068 ,  0x80f83cf8 ,  0x2c8c11c0 ,  0x7d548035 ,  0x0ff675c3 ,  0xfed160bf ,  0x74bbbb24 ,  0xd98e006b ,  \r\n0xdeaa47eb ,  0x05f2179e ,  0x437b0b71 ,  0xa7c95f8f ,  0x00a99d3b ,  0x3fc3c444 ,  0x72686f8e ,  0x00fd01a9 ,  \r\n0xdedc0787 ,  0xc6af7626 ,  0x7012fe76 ,  0xf2a5f7ce ,  0x9a7b2eda ,  0x5e57fcf2 ,  0x4da0d4ad ,  0x5c63b155 ,  \r\n0x34117375 ,  0xd4134c11 ,  0x2ea77435 ,  0x5278b6de ,  0xab522c4c ,  0xbc8fc702 ,  0xc94a09e4 ,  0xebb93a9e ,  \r\n0x91ecb65e ,  0x4c52ecc6 ,  0x8703bb52 ,  0xcb2d60aa ,  0x30a0538a ,  0x1514f10b ,  0x157f6329 ,  0x3429dc3d ,  \r\n0x5db73eb2 ,  0xa7a1a969 ,  0x7286bd24 ,  0x0df6881e ,  0x3785ba5f ,  0xcd04623a ,  0x02758170 ,  0xd827f556 ,  \r\n0x99d95191 ,  0x84457eb1 ,  0x58a7fb22 ,  0xd2967c5f ,  0x4f0c33f6 ,  0x4a02099a ,  0xe0904821 ,  0x94124036 ,  \r\n0x496a031b ,  0x780b69c4 ,  0xcf1a4927 ,  0x87a119b8 ,  0xcdfaf4f8 ,  0x4cf9cd0f ,  0x27c96a84 ,  0x6d11117e ,  \r\n0x7f8cf847 ,  0x74ceede5 ,  0xc88905e6 ,  0x60215841 ,  0x7172875a ,  0x736e993a ,  0x010aa53c ,  0x43d53c2b ,  \r\n0xf0d91a93 ,  0x0d983b56 ,  0xf816663c ,  0xe5d13363 ,  0x0a61737c ,  0x09d51150 ,  0x83a5ac2f ,  0x3e884905 ,  \r\n0x7b01aeb5 ,  0x600a6ea7 ,  0xb7678f7b ,  0x72b38977 ,  0x068018f2 ,  0xce6ae45b ,  0x29188aa8 ,  0xe5a0b1e9 ,  \r\n0xc04c2b86 ,  0x8bd14d75 ,  0x648781f3 ,  0xdbae1e0a ,  0xddcdd8ae ,  0xab4d81a3 ,  0x446baaba ,  0x1cc0c19d ,  \r\n0x17be4f90 ,  0x82c0e65d ,  0x676f9c95 ,  0x5c708db2 ,  0x6fd4c867 ,  0xa5106ef0 ,  0x19dde49d ,  0x78182f95 ,  \r\n0xd089cd81 ,  0xa32e98fe ,  0xbe306c82 ,  0x6cd83d8c ,  0x037f1bde ,  0x0b15722d ,  0xeddc1e22 ,  0x93c76559 ,  \r\n0x8a2f571b ,  0x92cc81b4 ,  0x021b7477 ,  0x67523904 ,  0xc95dbccc ,  0xac17ee9d ,  0x944e46bc ,  0x0781867e ,  \r\n0xc854dd9d ,  0x26e2c30c ,  0x858c0416 ,  0x6d397708 ,  0xebe29c58 ,  0xc80ced86 ,  0xd496b4ab ,  0xbe45e6f5 ,  \r\n0x10d24706 ,  0xacf8187a ,  0x96f523cb ,  0x2227e143 ,  0x78c36564 ,  0x4643adc2 ,  0x4729d97a ,  0xcff93e0d ,  \r\n0x25484bbd ,  0x91c6798e ,  0x95f773f4 ,  0x44204675 ,  0x2eda57ba ,  0x06d313ef ,  0xeeaa4466 ,  0x2dfa7530 ,  \r\n0xa8af0c9b ,  0x39f1535e ,  0x0cc2b7bd ,  0x38a76c0e ,  0x4f41071d ,  0xcdaf2475 ,  0x49a6eff8 ,  0x01621748 ,  \r\n0x36ebacab ,  0xbd6d9a29 ,  0x44d1cd65 ,  0x40815dfd ,  0x55fa5a1a ,  0x87cce9e9 ,  0xae559b45 ,  0xd76b4c26 ,  \r\n0x637d60ad ,  0xde29f5f9 ,  0x97491cbb ,  0xfb350040 ,  0xffe7f997 ,  0x201c9dcd ,  0xe61320e9 ,  0xa90987a3 ,  \r\n0xe24afa83 ,  0x61c1e6fc ,  0xcc87ff62 ,  0xf1c9d8fa ,  0x4fd04546 ,  0x90ecc76e ,  0x46e456b9 ,  0x305dceb8 ,  \r\n0xf627e68c ,  0x2d286815 ,  0xc705bbfd ,  0x101b6df3 ,  0x892dae62 ,  0xd5b7fb44 ,  0xea1d5c94 ,  0x5332e3cb ,  \r\n0xf856f88a ,  0xb341b0e9 ,  0x28408d9d ,  0x5421bc17 ,  0xeb9af9bc ,  0x602371c5 ,  0x67985a91 ,  0xd774907f ,  \r\n0x7c4d697d ,  0x9370b0b8 ,  0x6ff5cebb ,  0x7d465744 ,  0x674ceac0 ,  0xea9102fc ,  0x0de94784 ,  0xc793de69 ,  \r\n0xfe599bb1 ,  0xc6ad952f ,  0x6d6ca9c3 ,  0x928c3f91 ,  0xf9022f05 ,  0x24a164dc ,  0xe5e98cd3 ,  0x7649efdb ,  \r\n0x6df3bcdb ,  0x5d1e9ff1 ,  0x17f5d010 ,  0xe2686ea1 ,  0x6eac77fe ,  0x7bb5c585 ,  0x88d90cbb ,  0x18689163 ,  \r\n0x67c9efa5 ,  0xc0b76d9b ,  0x960efbab ,  0xbd872807 ,  0x70f4c474 ,  0x56c29d20 ,  0xd1541d15 ,  0x88137033 ,  \r\n0xe3f02b3e ,  0xb6d9b28d ,  0x53a077ba ,  0xeedcd29e ,  0xa50a6c1d ,  0x12c2801e ,  0x52ba335b ,  0x35984614 ,  \r\n0xe2599aa8 ,  0xaf94ed1d ,  0xd90d4767 ,  0x202c7d07 ,  0x77bec4f4 ,  0xfa71bc80 ,  0xfc5c8b76 ,  0x8d0fbbfc ,  \r\n0xda366dc6 ,  0x8b32a0c7 ,  0x1b36f7fc ,  0x6642dcbc ,  0x6fe7e724 ,  0x8b5fa782 ,  0xc4227404 ,  0x3a7d1da7 ,  \r\n0x517ed658 ,  0x8a18df6d ,  0x3e5c9b23 ,  0x1fbd51ef ,  0x1470601d ,  0x3400389c ,  0x676b065d ,  0x8864ad80 ,  \r\n0xea6f1a9c ,  0x2db484e1 ,  0x608785f0 ,  0x8dd384af ,  0x69d26699 ,  0x409c4e16 ,  0x77f9986a ,  0x7f491266 ,  \r\n0x883ea6cf ,  0xeaa06072 ,  0xfa2e5db5 ,  0x352594b4 ,  0x9156bb89 ,  0xa2fbbbfb ,  0xac3989c7 ,  0x6e2422b1 ,  \r\n0x581f3560 ,  0x1009a9b5 ,  0x7e5ad9cd ,  0xa9fc0a6e ,  0x43e5998e ,  0x7f8778f9 ,  0xf038f8e1 ,  0x5415c2e8 ,  \r\n0x6499b731 ,  0xb82389ae ,  0x05d4d819 ,  0x0f06440e ,  0xf1735aa0 ,  0x986430ee ,  0x47ec952c ,  0xbf149cc5 ,  \r\n0xb3cb2cb6 ,  0x3f41e8c2 ,  0x271ac51b ,  0x48ac5ded ,  0xf76a0469 ,  0x717bba4d ,  0x4f5c90d6 ,  0x3b74f756 ,  \r\n0x1824110a ,  0xa4fd43e3 ,  0x1eb0507c ,  0xa9375c08 ,  0x157c59a7 ,  0x0cad8f51 ,  0xd66031a0 ,  0xabb5343f ,  \r\n0xe533fa43 ,  0x1996e2bb ,  0xd7953a71 ,  0xd2529b94 ,  0x58f0fa07 ,  0x4c9b1877 ,  0x057e990d ,  0x8bfe19c4 ,  \r\n0xa8e2c0c9 ,  0x99fcaada ,  0x69d2aaca ,  0xdc1c4642 ,  0xf4d22307 ,  0x7fe27e8c ,  0x1366aa07 ,  0x1594e637 ,  \r\n0xce1066bf ,  0xdb922552 ,  0x9930b52a ,  0xaeaa9a3e ,  0x31ff7eb4 ,  0x5e1f945a ,  0x150ac49c ,  0x0ccdac2d ,  \r\n0xd8a8a217 ,  0xb82ea6e5 ,  0xd6a74659 ,  0x67b7e3e6 ,  0x836eef4a ,  0xb6f90074 ,  0x7fa3ea4b ,  0xcb038123 ,  \r\n0xbf069f55 ,  0x1fa83fc4 ,  0xd6ebdb23 ,  0x16f0a137 ,  0x19a7110d ,  0x5ff3b55f ,  0xfb633868 ,  0xb466f845 ,  \r\n0xbce0c198 ,  0x88404296 ,  0xddbdd88b ,  0x7fc52546 ,  0x63a553f8 ,  0xa728405a ,  0x378a2bce ,  0x6862e570 ,  \r\n0xefb77e7d ,  0xc611625e ,  0x32515c15 ,  0x6984b765 ,  0xe8405976 ,  0x9ba386fd ,  0xd4eed4d9 ,  0xf8fe0309 ,  \r\n0x0ce54601 ,  0xbaf879c2 ,  0xd8524057 ,  0x1d8c1d7a ,  0x72c0a3a9 ,  0x5a1ffbde ,  0x82f33a45 ,  0x5143f446 ,  \r\n0x29c7e182 ,  0xe536c32f ,  0x5a6f245b ,  0x44272adb ,  0xcb701d9c ,  0xf76137ec ,  0x0841f145 ,  0xe7042ecc ,  \r\n0xf1277dd7 ,  0x745cf92c ,  0xa8fe65fe ,  0xd3e2d7cf ,  0x54c513ef ,  0x6079bc2d ,  0xb66336b0 ,  0x101e383b ,  \r\n0xbcd75753 ,  0x25be238a ,  0x56a6f0be ,  0xeeffcc17 ,  0x5ea31f3d ,  0x0ae772f5 ,  0xf76de3de ,  0x1bbecdad ,  \r\n0xc9107d43 ,  0xf7e38dce ,  0x618358cd ,  0x5c833f04 ,  0xf6975906 ,  0xde4177e5 ,  0x67d314dc ,  0xb4760f3e ,  \r\n0x56ce5888 ,  0x0e8345a8 ,  0xbff6b1bf ,  0x78dfb112 ,  0xf1709c1e ,  0x7bb8ed8b ,  0x902402b9 ,  0xdaa64ae0 ,  \r\n0x46b71d89 ,  0x7eee035f ,  0xbe376509 ,  0x99648f3a ,  0x0863ea1f ,  0x49ad8887 ,  0x79bdecc5 ,  0x3c10b568 ,  \r\n0x5f2e4bae ,  0x04ef20ab ,  0x72f8ce7b ,  0x521e1ebe ,  0x14525535 ,  0x2e8af95b ,  0x9094ccfd ,  0xbcf36713 ,  \r\n0xc73953ef ,  0xd4b91474 ,  0x6554ec2d ,  0xe3885c96 ,  0x03dc73b7 ,  0x931688a9 ,  0xcbbef182 ,  0x2b77cfc9 ,  \r\n0x632a32bd ,  0xd2115dcc ,  0x1ae5533d ,  0x32684e13 ,  0x4cc5a004 ,  0x13321bde ,  0x62cbd38d ,  0x78383a3b ,  \r\n0xd00686f1 ,  0x9f601ee7 ,  0x7eaf23de ,  0x3110c492 ,  0x9c351209 ,  0x7eb89d52 ,  0x6d566eac ,  0xc2efd226 ,  \r\n0x32e9fac5 ,  0x52227274 ,  0x09f84725 ,  0xb8d0b605 ,  0x72291f02 ,  0x71b5c34b ,  0x3dbfcbb8 ,  0x04a02263 ,  \r\n0x55ba597f ,  0xd4e4037d ,  0xc813e1be ,  0xffddeefa ,  0xc3c058f3 ,  0x87010f2e ,  0x1dfcf55f ,  0xc694eeeb ,  \r\n0xa9c01a74 ,  0x98c2fc6b ,  0xe57e1428 ,  0xdd265a71 ,  0x836b956d ,  0x7e46ab1a ,  0x5835d541 ,  0x50b32505 ,  \r\n0xe640913c ,  0xbb486079 ,  0xfe496263 ,  0x113c5b69 ,  0x93cd6620 ,  0x5efe823b ,  0x2d657b40 ,  0xb46dfc6c ,  \r\n0x57710c69 ,  0xfe9fadeb ,  0xb5f8728a ,  0xe3224170 ,  0xca28b751 ,  0xfdabae56 ,  0x5ab12c3c ,  0xa697c457 ,  \r\n0xd28fa2b7 ,  0x056579f2 ,  0x9fd9d810 ,  0xe3557478 ,  0xd88d89ab ,  0xa72a9422 ,  0x6d47abd0 ,  0x405bcbd9 ,  \r\n0x6f83ebaf ,  0x13caec76 ,  0xfceb9ee2 ,  0x2e922df7 ,  0xce9856df ,  0xc05e9322 ,  0x2772c854 ,  0xb67f2a32 ,  \r\n0x6d1af28d ,  0x3a78cf77 ,  0xdff411e4 ,  0x61c74ca9 ,  0xed8b842e ,  0x72880845 ,  0x6e857085 ,  0xc6404932 ,  \r\n0xee37f6bc ,  0x27116f48 ,  0x5e9ec45a ,  0x8ea2a51f ,  0xa5573db7 ,  0xa746d036 ,  0x486b4768 ,  0x5b438f3b ,  \r\n0x18c54a5c ,  0x64fcf08e ,  0xe993cdc1 ,  0x35c1ead3 ,  0x9de07de7 ,  0x321b841c ,  0x87423c5e ,  0x071aa0f6 ,  \r\n0x962eb75b ,  0xbb06bdd2 ,  0xdcdb5363 ,  0x389752f2 ,  0x83d9cc88 ,  0xd014adc6 ,  0xc71121bb ,  0x2372f938 ,  \r\n0xcaff2650 ,  0x62be8951 ,  0x56dccaff ,  0xac4084c0 ,  0x09712e95 ,  0x1d3c288f ,  0x1b085744 ,  0xe1d3cfef ,  \r\n0x5c9a812e ,  0x6611fd59 ,  0x85e46044 ,  0x1981d885 ,  0x5a4c903f ,  0x43f30d4b ,  0x7d1d601b ,  0xdd3c3391 ,  \r\n0x030ec65e ,  0xc12878cd ,  0x72e795fe ,  0xd0c76abd ,  0x1ec085db ,  0x7cbb61fa ,  0x93e8dd1e ,  0x8582eb06 ,  \r\n0x73563144 ,  0x049d4e7e ,  0x5fd5aefe ,  0x7b842a00 ,  0x75ced665 ,  0xbb32d458 ,  0x4e83bba7 ,  0x8f15151f ,  \r\n0x7795a125 ,  0xf0842455 ,  0x499af99d ,  0x565cc7fa ,  0xa3b1278d ,  0x3f27ce74 ,  0x96ca058e ,  0x8a497443 ,  \r\n0xa6fb8cae ,  0xc115aa21 ,  0x17504923 ,  0xe4932402 ,  0xaea886c2 ,  0x8eb79af5 ,  0xebd5ea6b ,  0xc7980d3b ,  \r\n0x71369315 ,  0x796e6a66 ,  0x3a7ec708 ,  0xb05175c8 ,  0xe02b74e7 ,  0xeb377ad3 ,  0x6c8c1f54 ,  0xb980c374 ,  \r\n0x59aee281 ,  0x449cb799 ,  0xe01f5605 ,  0xed0e085e ,  0xc9a1a3b4 ,  0xaac481b1 ,  0xc935c39c ,  0xb7d8ce7f ]\r\n\r\nfunction InjectConstants( state_vector, round_index ):\r\n    for i in [0:16] do:\r\n        output_vector[i] = state_vector[i] xor injection_constants[round_index*16 + i]\r\n    return output_vector\r\n```\r\n\r\n<a name=\"Addition-Rotation-Addition\"></a>\r\n## Addition-Rotation-Addition\r\n\r\nThe addition-rotation-addition step is the only one that involves modular addition. The state vector is partitioned into 8 pairs, and every pair undergoes a sequence of three operations. First, the element on the right is added to the one on the left; then both words are rotated by 8 bits in opposite directions; then the left is added to the right. The next pseudocode makes this formal.\r\n\r\n```\r\nfunction ApplyARA( state_vector ):\r\n    for i in [0:8] do:\r\n        output_vector[2*i] = state_vector[2*i] + state_vector[2*i + 1]\r\n        output_vector[2*i] = output_vector[2*i] <<< 8\r\n        output_vector[2*i + 1] = state_vector[2*i + 1] <<< 24\r\n        output_vector[2*i + 1] = output_vector[2*i + 1] + output_vector[2*i]\r\n    return output_vector\r\n```\r\n\r\n<a name=\"Full-Permutation\"></a>\r\n## Full Permutation\r\n\r\nFor the sake of completeness, the following pseudocode computes the full permutation.\r\n\r\n```\r\nfunction F( input ):\r\n    state = input\r\n    for i in [0:43] do:\r\n        state = ApplyBitMatrix( state )\r\n        state = ApplyCirculantMultiplication( state )\r\n        state = InjectConstants( state, i )\r\n        state = ApplyARA( state )\r\n    return state\r\n```\r\n\r\n<a name=\"Eaglesong-Hash\"></a>\r\n# Eaglesong Hash\r\n\r\nThe Eaglesong hash function is obtained by taking the sponge code and instantiating the parameters with:\r\n\r\n - `r = 256`\r\n - `c = 256`\r\n - `output_length = 256`\r\n - `d = 0x06`\r\n\r\n<a name=\"Reference-Implementations\"></a>\r\n# Reference Implementations\r\n\r\nThis RFC is supplemented by two reference implementations, one in [C](eaglesong.c) and one in [python](eaglesong.py). Attached is also a wrapper ([C](hash.c) and [python](hash.py)) for computing the Eaglesong hash of a string read from stdin.\r\n\r\nTo compile and run the C program:\r\n```\r\n$> gcc -o hash eaglesong.c hash.c\r\n$> ./hash\r\n > Hello, world!\r\n > [Ctrl-D]\r\n > 64867e2441d162615dc2430b6bcb4d3f4b95e4d0db529fca1eece73c077d72d6\r\n```\r\n\r\nTo run the python program:\r\n```\r\n$> python hash.py\r\n > Hello, world!\r\n > [Ctrl-D]\r\n > 64867e2441d162615dc2430b6bcb4d3f4b95e4d0db529fca1eece73c077d72d6\r\n```\r\n\r\n","\\0011-transaction-filter-protocol\\0011-transaction-filter-protocol.md":"---\r\nNumber: \"0011\"\r\nCategory: Standards Track\r\nStatus: Proposal\r\nAuthor: Quake Wang\r\nOrganization: Nervos Foundation\r\nCreated: 2018-12-11\r\n---\r\n\r\n# Transaction Filter Protocol\r\n\r\n## Abstract\r\n\r\nTransaction filter protocol allows peers to reduce the amount of transaction data they send. Peer which wants to retrieve transactions of interest, has the option of setting filters on each connection. A filter is defined as a [Bloom filter](http://en.wikipedia.org/wiki/Bloom_filter) on data derived from transactions.\r\n\r\n## Motivation\r\n\r\nThe purpose of transaction filter protocol is to allow low-capacity peers (smartphones, browser extensions, embedded devices, etc) to maintain a high-security assurance about the up to date state of some particular transactions of the chain or verify the execution of transactions.\r\n\r\nThese peers do not attempt to fully verify the block chain, instead just checking that [block headers connect](../0004-ckb-block-sync/0004-ckb-block-sync.md#connecting-header) together correctly and trusting that the transactions in the block of highest difficulty are in fact valid.\r\n\r\nWithout this protocol, peers have to download the entire blocks and accept all broadcast transactions, then throw away majority of the transactions. This slows down the synchronization process, wastes users bandwidth and increases memory usage.\r\n\r\n## Messages\r\n\r\n*Message serialization format is [CFB](../0008-serialization/0008-serialization.md)*\r\n\r\n### SetFilter\r\n\r\nUpon receiving a `SetFilter` message, the remote peer will immediately restrict the transactions that it broadcasts to the ones matching the filter, where the [matching algorithm](#filter-matching-algorithm) is specified as below.\r\n\r\n```\r\ntable SetFilter {\r\n    filter: [uint8];\r\n    num_hashes: uint8;\r\n    hash_seed: uint32;\r\n}\r\n```\r\n\r\n`filter`: A bit field of arbitrary byte-aligned size. The maximum size is 36,000 bytes.\r\n\r\n`num_hashes`: The number of hash functions to use in this filter. The maximum value allowed in this field is 20. This maximum value and `filter` maximum size allow to store ~10,000 items and the false positive rate is 0.0001%.\r\n\r\n`hash_seed`: We use [Kirsch-Mitzenmacher-Optimization](https://www.eecs.harvard.edu/~michaelm/postscripts/tr-02-05.pdf) hash function in this protocol, `hash_seed` is a random offset, `h1` is low uint32 of hash value, `h2` is high uint32 of hash value, and the nth hash value is `(hash_seed + h1 + n * h2) mod filter_size`.\r\n\r\n### AddFilter\r\n\r\nUpon receiving a `AddFilter` message, the given bit data will be added to the exsiting filter via bitwise OR operator. A filter must have been previously provided using `SetFilter`. This messsage is useful if a new filter is added to a peer whilst it has connections to the network open, alsp avoids the need to re-calculate and send an entirely new filter to every peer.\r\n\r\n```\r\ntable AddFilter {\r\n    filter: [uint8];\r\n}\r\n```\r\n\r\n`filter`: A bit field of arbitrary byte-aligned size. The data size must be litter than or equal to previously provided filter size.\r\n\r\n### ClearFilter\r\n\r\nThe `ClearFilter` message tells the receiving peer to remove a previously-set bloom filter.\r\n\r\n```\r\ntable ClearFilter {\r\n}\r\n```\r\n\r\nThe `ClearFilter` message has no arguments at all.\r\n\r\n\r\n### FilteredBlock\r\n\r\nAfter a filter has been set, peers don't merely stop announcing non-matching transactions, they can also serve filtered blocks. This message is a replacement for `Block` message of sync protocol and `CompactBlock` message of relay protocol.\r\n\r\n```\r\ntable FilteredBlock {\r\n    header: Header;\r\n    transactions: [IndexTransaction];\r\n    hashes: [H256];\r\n}\r\n\r\ntable IndexTransaction {\r\n    index:                      uint32;\r\n    transaction:                Transaction;\r\n}\r\n```\r\n\r\n`header`: Standard block header struct.\r\n\r\n`transactions`: Standard transaction struct plus transaction index.\r\n\r\n`hashes`: Partial [Merkle](../0006-merkle-tree/0006-merkle-tree.md#merkle-proof) branch proof.\r\n\r\n## Filter matching algorithm\r\n\r\nThe filter can be tested against all broadcast transactions, to determine if a transaction matches the filter, the following algorithm is used. Once a match is found the algorithm aborts.\r\n\r\n1. Test the hash of the transaction itself.\r\n2. For each CellInput, test the hash of `previous_output`.\r\n3. For each CellOutput, test the `lock hash` and `type hash` of script.\r\n4. Otherwise there is no match.\r\n","\\0012-node-discovery\\0012-node-discovery.md":"---\r\nNumber: \"0012\"\r\nCategory: Standards Track\r\nStatus: Proposal\r\nAuthor: Linfeng Qian, JinYang Jiang\r\nOrganization: Nervos Foundation\r\nCreated: 2018-11-28\r\n---\r\n\r\n# CKB Node Discovery Protocol\r\n\r\nCKB Node Discovery Protocol mainly refers to [Satoshi Client Node Discovery][0]. The differences between them are summarized below:\r\n\r\n* The node version number is included in the `GetNodes` message.\r\n* The `Nodes` message is used to periodically broadcast all nodes currently connected.\r\n* We use `multiaddr` as the format of node addresses (It MUST NOT include `/p2p/` segment otherwise it's considered as *misbehavior* and a low score SHOULD be given.)\r\n\r\nEvery time client startup, if PeerStore's address list is empty, it SHOULD try to issue DNS requests to initialize address list. If DNS requests don't work it SHOULD fallback to the hard-coded address list.\r\n\r\n## Discovery Methods\r\n### DNS Addresses\r\nAt the first time startup (bootstrap stage), if the discovery service is needed, the local node SHOULD issues DNS requests to learn about the addresses of other peer nodes. The client includes a list of seed hostnames for DNS services.\r\n\r\n### Hard-Coded \"Seed\" Addresses\r\nThe client contains some hard-coded \"seed\" IP addresses that represent CKB nodes. Those addresses are used only if all DNS requests fail. Once the local node has enough addresses (presumably learned from the seed nodes), the client SHOULD close seed node connections to avoid overloading those nodes.\r\n\r\n\"Seed\" nodes are nodes that generally have a high uptime and have had many connections to many other nodes.\r\n\r\n### Protocol Message\r\n#### `GetNodes` Message\r\nWhen all the following conditions are met, the local node will send a `GetNodes` message:\r\n\r\n  1. It's an outbound connection (for resisting [fingerprinting attack][3]).\r\n  2. The other node's version must bigger than a preset value.\r\n  3. The number of addresses currently stored is less than `ADDRESSES_THRESHOLD` (default 1000). \r\n\r\n\r\n#### `Nodes` Message\r\nWhen the client receives a `GetNodes` request, it SHOULD return a `Nodes` message if this kind of reception is the first time and the connection is an inbound connection, the `announce` field is set to `false`. At regular intervals, local node SHOULD broadcast all connected `Node` information in `Nodes` message to all connected nodes, the `announce` field is set to `true`. When local node received a `Nodes` message and it's `announce` field is `true`, local node SHOULD relay those node addresses that are [routable][1].\r\n\r\nThe `announce` field here is to distinguish a `Nodes` as a response of `GetNodes` or a broadcast message, so it's convenient to apply different rules for punishing misbehaviors. The main rules:\r\n\r\n* A node can only send one `Nodes` message (announce=false) as a response of `GetNodes` message.\r\n* Among a node's broadcast messages only the first `Nodes` message (announce=true) can include more than `ANNOUNCE_THRESHOLD` (default 10) node information, in case other peers send too many node information.\r\n\r\nThe number of `addresses` field of each `Node` in all `Nodes` messages cannot exceed `MAX_NODE_ADDRESSES` (default 3).\r\n\r\n## Resist Typical Attacks\r\n### Fingerprinting Attack\r\n[Related paper][3]\r\n\r\n`GetNodes` can only send to an outbound connection.\r\n\r\n## Data Structures\r\nWe use [CFB][2] as serialize/deserialize format, the *schema*:\r\n\r\n```\r\ntable DiscoveryMessage {\r\n    payload: DiscoveryPayload;\r\n}\r\n\r\nunion DiscoveryPayload {\r\n    GetNodes,\r\n    Nodes,\r\n}\r\n\r\ntable GetNodes {\r\n    version: uint32;\r\n    count: uint32;\r\n}\r\n\r\ntable Nodes {\r\n    announce: bool;\r\n    items: [Node];\r\n}\r\n\r\ntable Node {\r\n    node_id: Bytes;\r\n    addresses: [Bytes];\r\n}\r\n```\r\n\r\n## Flow Diagram\r\n### Node Bootstrap\r\n![](images/bootstrap.png)\r\n### Send `GetNodes` Message\r\n![](images/get-nodes.png)\r\n### Announce Connected Nodes\r\n![](images/announce-nodes.png)\r\n\r\n[0]: https://en.bitcoin.it/wiki/Satoshi_Client_Node_Discovery\r\n[1]: https://www.iana.org/assignments/iana-ipv4-special-registry/iana-ipv4-special-registry.xhtml\r\n[2]: https://github.com/nervosnetwork/rfcs/blob/master/rfcs/0008-serialization/0008-serialization.md\r\n[3]: https://arxiv.org/pdf/1410.6079.pdf\r\n","\\0012-node-discovery\\0012-node-discovery.zh.md":"---\r\nNumber: \"0012\"\r\nCategory: Standards Track\r\nStatus: Proposal\r\nAuthor: Linfeng Qian, JinYang Jiang\r\nOrganization: Nervos Foundation\r\nCreated: 2018-11-28\r\n---\r\n\r\n# CKB 节点发现协议\r\n\r\nCKB 节点发现协议主要参考了[比特币的协议][0]。主要不同点如下:\r\n* 节点版本号包含在 `GetNodes` 消息中\r\n* 通过 `Nodes` 消息来定时广播当前连接的所有节点\r\n* 我们使用 `multiaddr` 作为节点地址的格式 (不允许出现 `/p2p/` 段，如果违反会被认为是*不良*行为并被打低分)\r\n\r\n每次客户端启动时，如果 PeerStore 中的地址列表为空就会尝试通过 DNS 的方式获取初始地址，如果 DNS 的方式失败了就使用硬编码的种子地址来初始化地址列表。\r\n\r\n## 节点发现的手段\r\n### DNS 获取地址\r\n第一次启动的时候(引导阶段)，如果需要节点发现服务，客户端会尝试向内置的 DNS 服务器发送 DNS 请求来获取种子服务器地址。\r\n\r\n### 硬编码的「种子」地址\r\n客户端会硬编码一些「种子」节点地址，这些地址只有在 DNS 获取地址失败的时候被使用。当通过这些种子节点获取了足够多的地址后需要断开这些连接，防止它们过载。这些「种子」地址的时间戳被设置为 0 所以不会加入到 `GetNodes` 请求的返回值中。\r\n\r\n「种子」节点是那些在线时间较长而且和很多其它节点互连的节点。\r\n\r\n### 协议消息\r\n\r\n#### `GetNodes` 消息\r\n当满足所有以下条件时，节点会发送一个 `GetNodes` 请求：\r\n\r\n  1. 这个连接是自己主动发起的 (防御[指纹攻击][3])\r\n  2. 对方的版本号大于一个预设的值\r\n  3. 当前存储的地址数量小于 `ADDRESSES_THRESHOLD` (默认 1000) 个\r\n\r\n#### `Nodes` 消息\r\n\r\n当客户端收到一个 `GetNodes` 请求时，如果是第一次收到 `GetNodes` 消息而且这个连接是对方主动发起的就会返回一个 `Nodes` 消息，该 `Nodes` 消息的 `announce` 字段为 `false`。每隔一定时间当前节点会将当前连接的节点信息以及本节点信息以 `Nodes` 消息广播给当前连接的所有节点，`announce` 字段为 `true`。当前收到 `announce` 字段为 `true` 的 `Nodes` 消息时会对地址[可路由][1]的那些节点地址进行转发。\r\n\r\n这里 `announce` 字段的目的是为了区分 `Nodes` 消息是作为 `GetNodes` 消息的返回值还是广播消息，可以方便应用不同的规则来对节点的恶意行为做相应的处罚。涉及到的规则主要有:\r\n\r\n* 一个节点只能有一个 `Nodes` 消息 (announce=false) 作为 `GetNodes` 消息的返回值。\r\n* 一个节点的广播消息中只能第一个 `Nodes` 消息 (announce=true) 包含的节点信息数量超过 `ANNOUNCE_THRESHOLD` (默认 10) 个，这是为了防止其它节点发送过多的 `Node` 信息。\r\n\r\n所有 `Nodes` 消息中的每个 `Node` 中的 `addresses` 的数量不能超过 `MAX_NODE_ADDRESSES` (默认 3) 个。\r\n\r\n## 对主要攻击方式的处理\r\n### 指纹攻击 (fingerprinting attack)\r\n[相关论文][3]\r\n\r\n`GetNodes` 消息只能通过 outbound 连接发送出去。\r\n\r\n## 相关数据结构\r\n我们使用 [CFB][2] 作为数据序列化格式，以下为相关数据结构的 schema:\r\n\r\n```\r\ntable DiscoveryMessage {\r\n    payload: DiscoveryPayload;\r\n}\r\n\r\nunion DiscoveryPayload {\r\n    GetNodes,\r\n    Nodes,\r\n}\r\n\r\ntable GetNodes {\r\n    version: uint32;\r\n    count: uint32;\r\n}\r\n\r\ntable Nodes {\r\n    announce: bool;\r\n    items: [Node];\r\n}\r\n\r\ntable Node {\r\n    node_id: Bytes;\r\n    addresses: [Bytes];\r\n}\r\n```\r\n\r\n## 流程图\r\n### 节点 Bootstrap\r\n![](images/bootstrap.png)\r\n### 发送 `GetNodes` 消息\r\n![](images/get-nodes.png)\r\n### 广播当前连接的节点信息\r\n![](images/announce-nodes.png)\r\n\r\n[0]: https://en.bitcoin.it/wiki/Satoshi_Client_Node_Discovery\r\n[1]: https://www.iana.org/assignments/iana-ipv4-special-registry/iana-ipv4-special-registry.xhtml\r\n[2]: https://github.com/nervosnetwork/rfcs/blob/master/rfcs/0008-serialization/0008-serialization.md\r\n[3]: https://arxiv.org/pdf/1410.6079.pdf\r\n","\\0013-get-block-template\\0013-get-block-template.md":"---\r\nNumber: \"0013\"\r\nCategory: Standards Track\r\nStatus: Proposal\r\nAuthor: Dingwei Zhang\r\nOrganization: Nervos Foundation\r\nCreated: 2019-01-02\r\n---\r\n\r\n# get_block_template\r\n\r\n## Abstract\r\n\r\nThis RFC describes the decentralized CKB mining protocol.\r\n\r\n\r\n## Motivation\r\n\r\nThe original `get_work` [[btc][1] [eth][2]] mining protocol simply issues block headers for a miner to solve, the miner is kept in the dark, and has no influence over block creation. `get_block_template` moves block creation to the miner, the entire block structure is sent, and left to the miner to (optionally) customize and assemble, miner are enabled to audit and possibly modify the block before hashing it, this improves the security of the CKB network by making blocks decentralized.\r\n\r\n## Specification\r\n\r\n### Block Template Request\r\n\r\nA JSON-RPC method is defined, called `get_block_template`. It accepts exactly three argument:\r\n\r\n| Key          | Required | Type   | Description                                         |\r\n| ------------ | -------- | ------ | --------------------------------------------------- |\r\n| cycles_limit | No       | Number | maximum number of cycles to include in template     |\r\n| bytes_limit  | No       | Number | maximum number of bytes to use for the entire block |\r\n| max_version  | No       | Number | highest block version number supported              |\r\n\r\nFor `cycles_limit`, `bytes_limit` and `max_version`, if omitted, the default limit (consensus level) is used.\r\nServers SHOULD respect these desired maximums (if those maximums exceed consensus level limit, Servers SHOULD instead return the consensus level limit), but are NOT required to, clients SHOULD check that the returned template satisfies their requirements appropriately.\r\n\r\n`get_block_template` MUST return a JSON Object containing the following keys:\r\n\r\n| Key                   | Required | Type             | Description                                                                  |\r\n| --------------------- | -------- | ---------------- | ---------------------------------------------------------------------------- |\r\n| version               | Yes      | Number           | block version                                                                |\r\n| difficulty            | Yes      | String           | difficulty in hex-encoded string                                             |\r\n| current_time          | Yes      | Number           | the current time as seen by the server (recommended for block time)          |\r\n| number                | Yes      | Number           | the number of the block we are looking for                                   |\r\n| parent_hash           | Yes      | String           | the hash of the parent block, in hex-encoded string                          |\r\n| cycles_limit          | No       | Number           | maximum number of cycles allowed in blocks                                   |\r\n| bytes_limit           | No       | Number           | maximum number of bytes allowed in blocks                                    |\r\n| commit_transactions   | Should   | Array of Objects | objects containing information for CKB transactions (excluding cellbase)     |\r\n| proposal_transactions | Should   | Array of String  | array of hex-encoded transaction proposal_short_id                           |\r\n| cellbase              | Yes      | Object           | information for cellbase transaction                                         |\r\n| work_id               | No       | String           | if provided, this value must be returned with results (see Block Submission) |\r\n\r\n#### Transaction Object\r\n\r\nThe Objects listed in the response's \"commit_transactions\" key contains these keys:\r\n\r\n| Key      | Required | Type             | Description                                                                                                                                                                                                                       |\r\n| -------- | -------- | ---------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\r\n| hash     | Yes      | String           | the hash of the transaction                                                                                                                                                                                                       |\r\n| required | No       | Boolean          | if provided and true, this transaction must be in the final block                                                                                                                                                                 |\r\n| cycles   | No       | Number           | total number of cycles, if key is not present, cycles is unknown and clients MUST NOT assume there aren't any                                                                                                                     |\r\n| depends  | No       | Array of Numbers | other transactions before this one (by 1-based index in \"transactions\" list) that must be present in the final block if this one is; if key is not present, dependencies are unknown and clients MUST NOT assume there aren't any |\r\n| data     | Yes      | String           | transaction [CFB][3] bytes in  hex-encoded string                                                                                                                                                                                 |\r\n\r\n### Block Submission\r\n\r\nA JSON-RPC method is defined, called `submit_block`. to submit potential blocks (or shares). It accepts two arguments: the first is always a String of the hex-encoded block [CFB][3] bytes to submit; the second is String of work_id.\r\n\r\n| Key     | Required | Type   | Description                                                           |\r\n| ------- | -------- | ------ | --------------------------------------------------------------------- |\r\n| data    | Yes      | String | block [CFB][3] bytes in  hex-encoded string                           |\r\n| work_id | No       | String | if the server provided a workid, it MUST be included with submissions |\r\n\r\n### References\r\n\r\n* bitcoin Getwork, https://en.bitcoin.it/wiki/Getwork\r\n* ethereum Getwork, https://github.com/ethereum/wiki/wiki/JSON-RPC#eth_getwork\r\n* CFB Encoding, https://github.com/nervosnetwork/rfcs/pull/47\r\n\r\n[1]: https://en.bitcoin.it/wiki/Getwork\r\n[2]: https://github.com/ethereum/wiki/wiki/JSON-RPC#eth_getwork\r\n[3]: https://github.com/nervosnetwork/rfcs/pull/47\r\n","\\0014-vm-cycle-limits\\0014-vm-cycle-limits.md":"---\r\nNumber: \"0014\"\r\nCategory: Standards Track\r\nStatus: Proposal\r\nAuthor: Xuejie Xiao\r\nOrganization: Nervos Foundation\r\nCreated: 2019-01-04\r\n---\r\n\r\n# VM Cycle Limits\r\n\r\n## Introduction\r\n\r\nThis RFC describes cycle limits used to regulate VM scripts.\r\n\r\nCKB VM is a flexible VM that is free to implement many control flow constructs, such as loops or branches. As a result, we will need to enforce certain rules in CKB VM to prevent malicious scripts, such as a script with infinite loops.\r\n\r\nWe introduce a concept called `cycles`, each VM instruction or syscall will consume some amount of cycles. At consensus level, a scalar `max_block_cycles` field is defined so that the sum of cycles consumed by all scripts in a block cannot exceed this value. Otherwise, the block will be rejected. This way we can guarantee all scripts running in CKB VM will halt, or result in error state.\r\n\r\n## Consensus Change\r\n\r\nAs mentioned above, a new scalar `max_block_cycles` field is added to chain spec as a consensus rule, it puts a hard limit on how many cycles a block's scripts can consume. No block can consume cycles larger than `max_block_cycles`.\r\n\r\nNote there's no limit on the cycles for an individual transaction or a script. As long as the whole block consumes cycles less than `max_block_cycles`, a transaction or a script in that block are free to consume how many cycles they want.\r\n\r\n## Cycle Measures\r\n\r\nHere we will specify the cycles needed by each CKB VM instructions or syscalls. Note right now in the RFC, we define hard rules for each instruction or syscall here, in future this might be moved into consensus rules so we can change them more easily.\r\n\r\nNote that right now, the cycles used here aren't carefully tested, what's more, hardcoding cycles here is also a temporary solution. In future we plan to have a different RFC that:\r\n\r\n1. Use numbers that go through more testing and real benchmarks to be more realistic.\r\n2. Put cycle rules in a cell so we can change them without needing hardforks.\r\n\r\n### Instruction Cycles\r\n\r\nAll CKB VM instructions consume 1 cycle except the following ones:\r\n\r\n| Instruction | Cycles |\r\n|-------------|--------|\r\n| JALR        | 3      |\r\n| JAL         | 3      |\r\n| J           | 3      |\r\n| JR          | 3      |\r\n| BEQ         | 3      |\r\n| BNE         | 3      |\r\n| BLT         | 3      |\r\n| BGE         | 3      |\r\n| BLTU        | 3      |\r\n| BGEU        | 3      |\r\n| BEQZ        | 3      |\r\n| BNEZ        | 3      |\r\n| LD          | 2      |\r\n| SD          | 2      |\r\n| LDSP        | 2      |\r\n| SDSP        | 2      |\r\n| LW          | 3      |\r\n| LH          | 3      |\r\n| LB          | 3      |\r\n| LWU         | 3      |\r\n| LHU         | 3      |\r\n| LBU         | 3      |\r\n| SW          | 3      |\r\n| SH          | 3      |\r\n| SB          | 3      |\r\n| LWSP        | 3      |\r\n| SWSP        | 3      |\r\n| MUL         | 5      |\r\n| MULW        | 5      |\r\n| MULH        | 5      |\r\n| MULHU       | 5      |\r\n| MULHSU      | 5      |\r\n| DIV         | 16     |\r\n| DIVW        | 16     |\r\n| DIVU        | 16     |\r\n| DIVUW       | 16     |\r\n| REM         | 16     |\r\n| REMW        | 16     |\r\n| REMU        | 16     |\r\n| REMUW       | 16     |\r\n| ECALL       | 0      |\r\n| EBREAK      | 0      |\r\n\r\nIn a nutshell, the following principles are applied in deciding those numbers:\r\n\r\n* Branches should be more expensive than normal instructions.\r\n* Memory accesses should be more expensive than normal instructions, but since we are using 64-bit system, accessing 64-bit value should take less time than non 64-bit value.\r\n* Multiplication and divisions should be much more expensive than normal instructions.\r\n\r\n### Syscall Cycles\r\n\r\nEach syscall in CKB has different rules for consuming cycles:\r\n\r\n#### Load TX Hash\r\n\r\n*Load TX Hash* syscall first consumes 10 initial cycles, it then measures the size of the serialized transaction hash(for now, this is 32 bytes): for every single byte in the data, it consumes 10 more cycles.\r\n\r\nNote that even though the script only requires part of the serialized TX hash, the syscall still charges based on the full serialized hash size.\r\n\r\n#### Load Current Script Hash\r\n\r\n*Load Current Script Hash* syscall first consumes 10 initial cycles, it then measures the size of the serialized script hash(for now, this is 32 bytes): for every single byte in the data, it consumes 10 more cycles.\r\n\r\nNote that even though the script only requires part of the serialized TX hash, the syscall still charges based on the full serialized hash size.\r\n\r\n#### Load Cell\r\n\r\n*Load Cell* syscall first consumes 100 initial cycles, it then measures the size of the serialized cell structure data: for every single byte in the serialized data, it consumes 100 more cycles.\r\n\r\nNotice the charged cycles here is 10 times the cycles charged in `Load Cell By Field` syscall, this is because we are discouraging the use of this syscall. One should only use this if they really need the full serialized Cell structure.\r\n\r\nNote that even though the script only requires part of the serialized Cell structure data, the syscall still charges based on the full serialized data size.\r\n\r\n#### Load Cell By Field\r\n\r\n*Load Cell By Field* syscall first consumes 10 initial cycles, it then measures the size of the serialized data from the specified field: for every single byte in the serialized data, it consumes 10 more cycles.\r\n\r\nNote that even though the script only requires part of the specified serialized field, the syscall still charges based on the full serialized field size.\r\n\r\n#### Load Input\r\n\r\n*Load Input* syscall first consumes 100 initial cycles, it then measures the size of the serialized input data: for every single byte in the serialized data, it consumes 100 more cycles.\r\n\r\nNotice the charged cycles here is 10 times the cycles charged in `Load Input By Field` syscall, this is because we are discouraging the use of this syscall. One should only use this if they really need the full serialized input.\r\n\r\nNote that even though the script only requires part of the serialized input data, the syscall still charges based on the full serialized data size.\r\n\r\n#### Load Input By Field\r\n\r\n*Load Input By Field* syscall first consumes 10 initial cycles, it then measures the size of the serialized data from the specified field: for every single byte in the serialized data, it consumes 10 more cycles.\r\n\r\nNote that even though the script only requires part of the serialized data, the syscall still charges based on the full serialized data size.\r\n\r\n#### Load Header\r\n\r\n*Load Header* syscall first consumes 10 initial cycles, it then measures the size of the serialized header: for every single byte in the serialized data, it consumes 10 more cycles.\r\n\r\nNote that even though the script only requires part of the serialized header data, the syscall still charges based on the full serialized data size.\r\n\r\n#### Debug\r\n\r\n*Debug* syscall first consumes 10 initial cycles, it then consumes 10 more cycles for every single byte in the debug parameter string.\r\n\r\n#### Final note\r\n\r\nNotice that the two numbers used here (10 and 100) haven't gone through full testing, right now they are picked based on the following principles:\r\n\r\n* We want each syscall to at least consume some cycles even thought some syscall might return no data, hence we add either 10 or 100 initial cycles to each syscall.\r\n* Syscalls should in general be more expensive than normal instructions to discourage using them unless necessary, hence we are using a scale of 10 or 100 here to make them significantly bigger than most norma instructions.\r\n* We want to encourage using *Load Cell By Field* instead of *Load Cell*, since the former one makes easier implementation and less likely to be attacked, that's why *Load Cell* syscall use a factor of 100, while *Load Cell By Field* only use a factor of 10.\r\n\r\nIn future a different RFC might revise those numbers and even put those rules in a cell for easier changes.\r\n","\\0015-ckb-cryptoeconomics\\0015-ckb-cryptoeconomics.md":"---\r\nNumber: \"0015\"\r\nCategory: Informational\r\nStatus: Draft\r\nAuthor: Kevin Wang, Jan Xie, Jiasun Li, David Zou\r\nOrganization: Nervos Foundation\r\nCreated: 2019-03-08\r\n---\r\n# Crypto-Economics of the Nervos Common Knowledge Base\r\n\r\n## 1. The Objectives of a Token Economics Design\r\n\r\nPublic permission-less blockchains are open and distributed systems with diverse groups of participants. A well-designed crypto-economics model is to provide incentives so that participants' pursuit of own economic interests leads to desired emergent behaviors in alignment with the protocol, to contribute to the blockchain network's success.\r\n\r\nMore specifically, the design of a crypto-economic system must provide answers to the following questions:\r\n\r\n- How can the economic model ensure the security of the protocol?\r\n- How can the economic model ensure long term sustainability of the protocol?\r\n- How can the economic model align the objectives of different actors to grow the value of the protocol network?\r\n\r\n## 2. The Crypto-economics Design of Bitcoin\r\n\r\nThe Bitcoin protocol uses its native currency to incentivize miners to validate and produce blocks. The Nakamoto Consensus considers the longest chain as the valid chain, which encourages block producing miners to propagate new blocks as soon as they produce them and validate blocks as soon as they receive them. This ensures that the whole network achieves consensus on the global state.\r\n\r\nThe native tokens of the Bitcoin network function both as a utility token and an asset. When bitcoins function as a utility, they represent a \"Medium of Exchange\" (MoE) and can be used to pay transaction fees; when they function as an asset, they represent a \"Store of Value\" (SoV) and can be used to preserve value over time. The two use cases are not mutually exclusive. They are both important for the network to function. However, it's important to study the economic motives of the users of both use cases as a guide to analyze the sustainability of the Bitcoin network.\r\n\r\nThe Bitcoin protocol constrains the network's transaction throughput by using a fixed block size limit. Users bid with fees on the limited throughput to have their transactions processed. With this auction like mechanism, transaction fees are determined by the transaction demand - the more demand there is on the network, the higher the transaction fee a user has to pay to beat the competition and have their transaction included in the block.\r\n\r\n### Bitcoin as a Medium of Exchange Network\r\n\r\nThe Medium of Exchange use case views the Bitcoin network primarily as a peer to peer value transfer network. MoE users don't have to hold bitcoins to benefit from the network - it's the transactions in themselves that provide value. In fact, there are specialized Bitcoin payment services to provide access to liquidity and allow senders and receivers to acquire and dispose of Bitcoins just in time to perform the value transfer, without having to hold the cryptocurrency. MoE users are not concerned with price or the movement of price but care about the fiat equivalent cost of the transaction fees.\r\n\r\nIt's challenging for Bitcoin to become a dominant MoE network. If the protocol calibrates its block time and the block size limit, thereby fixing the supply of transactions, the success of the network will necessarily increase the cost of transactions and reduce its competitiveness among other similar purposed blockchains as well as its own forks; If the protocol aims to keep the transaction cost low and increase the supply of transactions with faster block time or bigger blocks, it could compromise both security and decentralization through higher fork rate and increased cost of consensus participation.\r\n\r\n\r\n### Bitcoin as a Store of Value Network\r\n\r\nStore of Value users view the Bitcoin network as a protocol to provide security to its native cryptocurrency as an asset that can preserve value over time. They see the Medium of Exchange use case as the necessary function to go in and out of this asset. A store of value user, especially the ones who hold the cryptocurrency for a long time, doesn't care much about the transaction cost, as they can amortize it over time. They do care about the value of a Bitcoin, which depends on the network's security and decentralization - if the network becomes less secure and can be attacked easily, it'll stop being perceived as a store of value and the tokens will lose value; if the network becomes centralized, Bitcoin as an asset no longer has independent value, but has to assume counter-party risk.\r\n\r\nFor Bitcoin to succeed as an SoV network, it must continue to keep its monetary policy stable and its network secure and decentralized. However, Bitcoin's monetary policy has a hard cap, and after all the coins are mined, the network can only pay for the miners with transaction fees. It's still an open question whether this model could be sustainable, especially considering Store of Value networks themselves tend not to produce many transactions.\r\n\r\n### Who Compensates the Miners Over the Long Run?\r\n\r\n\r\nSecurity and decentralization are two essential properties of a blockchain network, and they come with a high cost that must be paid to the operators of the network. Bitcoin's current model has network security entirely paid with transaction fees after all the coins are mined. However, the MoE users have very limited time exposure to the network's security risk, therefore won't be willing to pay for it;  the SoV users have prolonged exposure to the network's security risk and are willing to pay for it, but they produce nearly no transactions.\r\n\r\nBitcoin's consensus mechanism incentivizes miners to recognize the longest chain as the network's canonical state. Miner's ongoing supply of hashing power doesn't only provide security for the current block, but the immutability of all the blocks before it on the canonical chain. Relying on the SoV users to make one time payments for the ongoing security protection they receive from miners is not sustainable.\r\n\r\nIn an SoV network, relying on inflation to fund network security is more incentive compatible with the users. An inflation based block reward mechanism represents indirect payments from the beneficiaries of the network's ongoing security to the providers of such security, in proportion to the duration that they enjoy the service.\r\n\r\n## 3. Preservational and Transactional Smart Contract Platforms\r\n\r\nSmart contract platforms like Ethereum come with Turing-complete programmability and can support a much wider variety of use cases. The native tokens are typically used to price and pay for the cost of decentralized computation. Like the Bitcoin network, smart contract platforms also have the dual functions of preserving value and performing transactions. They differ from the payment networks in that the value they preserve is not only their own native tokens but also the internal states of decentralized applications, for example, crypto-assets ownership in ERC20 smart contracts.\r\n\r\nAnother significant difference is that transactions on smart contract platforms are much more \"portable\". It's much easier to take advantage of the more advanced scripting capability of smart contract platforms to develop interoperability protocols to move transactions to a more cost-effective transactional blockchain and then securely settle back to the main \"system of record\" blockchains.\r\n\r\nThe economic models of smart contract platforms face similar polarization tendency of payment networks. With their superior interoperable capabilities, smart contract platforms are going to be even more specialized into transactional platforms and preservation platforms.  Economically, this bifurcation comes from the fact that the two use cases have different ways of utilizing system resources - transactions consume instantaneous but renewable computation and bandwidth resources, and preservation requires long term occupation of the global state. An economic model optimized for one is unlikely to be optimal for the other.\r\n\r\nCompetitive transactional platforms need to prioritize for low transaction cost. Transactional users are willing to accept less-optimal security, because of their only moment-in-time, limited exposure to security risk. They're willing to accept the possibility of censored transactions, as long as there are options to take their transactions elsewhere. A transactional platform that invests in either security or censorship resistance will have higher cost of transactions, reflected either with higher transaction fees or high capital cost for stakes in a \"stake for access\" model, making the network less competitive. This is especially true when a well-designed inter-blockchain protocol can allow trust-less state transfers and fraud repudiation of transactions. We already see examples of transactional users prioritizing cost over security in centralized crypto-asset exchanges and not-so-decentralized blockchains - despite their flaws, they're still popular because of their transactional efficiency.\r\n\r\nCompetitive preservation platforms need to be sustainably secure and censorship-resistant. It requires an economic model designed not around transactions that happen moment-in-time, but around the ongoing occupation of the global state, and have users pay for the network infrastructure metered in their consumption of this critical resource.\r\n\r\n## 4. Store of Assets\r\n\r\nOne of the most important use cases for smart contract platforms is to issue tokens to represent ownership of assets. These crypto-assets can have their own communities and markets, and their values are independent of the value of their platform tokens. On the other hand, these assets depend on the platform to process transactions and provide security. Payment networks like Bitcoin can be seen as single asset platforms, where smart contract platforms are multi-asset platforms. Similar to the concept of \"Store of Value\" in the context of Bitcoin, we call the utility that smart contract platforms preserve the value of its crypto-assets \"Store of Assets\".\r\n\r\nPreservation focused smart contract platforms must have a Store of Assets token economics design. The level of platform security has to grow along with the asset value it preserves. Otherwise, as asset value grows, it will be increasingly profitable to \"double-spend\" assets by attacking the consensus process of the platform.\r\n\r\nNone of the current smart contract platforms are designed as Store of Assets platforms. Their token economics are designed either to facilitate transactions (for example, Ethereum's native tokens are to pay for the decentralized computation) or to fulfill staking requirements. In either case, the growth in asset value doesn't necessarily raise miner's income to provide more security.\r\n\r\nEvery multi-asset platform is an ecosystem of independent projects. The security of the platform can be seen as \"public goods\" that benefit all projects. To make the ecosystem sustainable from a security point of view, there has to be a clear mechanism that the platform captures the economic success of the ecosystem to raise its own level of security. In other words, a Store of Assets platform has to be able to translate the demand of crypto-assets to the revenue of its miners, often through raising the value of the native tokens with which the miners are compensated. Otherwise, the platform's level of security becomes the ceiling of assets' value. When the value of an asset rises such that typical transactions can no longer be sufficiently protected by the platform, the liquidity would dry up and the demand of the asset would fade.\r\n\r\nDecentralized multi-assets smart contract platforms have to be Store of Assets to be sustainable.\r\n\r\n## 5. Decentralization and the Need for Bounded State\r\n\r\nLike other long term store of value systems, a Store of Assets platform has to be neutral and free of risks of censorship and confiscation. These are the properties that made gold the world's favorite the store of value for thousands of years. For open, permission-less blockchain networks, censorship resistance comes down to having the broadest consensus scope with a low barrier for consensus and full node participation. Compared to payment networks, running a full node for a smart contract system is more resource intensive. Therefore a Store of Assets platform must take measures to protect the operating cost of full nodes to keep the network sufficiently decentralized.\r\n\r\nBoth Bitcoin and Ethereum throttle transaction throughput to ensure participation is not limited to only \"super computers\" - Bitcoin throttles on bandwidth and Ethereum throttles on computation. However, they haven't taken effective measures to contain the ever growing global state necessary for consensus participation and independent transaction validation.  This is especially a centralization force for high throughput smart contract platforms, where the global state grows even faster.\r\n\r\nIn Bitcoin, the global state is the UTXO set, and its growth rate is effectively capped with the block size limit. Users are encouraged to create UTXOs efficiently, since every new UTXO adds overhead to the transaction where it's created, making the transaction more expensive. However, once a UTXO is created, it doesn't cost anything to have it occupy the global state forever.\r\n\r\nIn Ethereum, the global state is represented with the EVM's state trie, the data structure that contains the balances and internal states of all accounts. When new accounts or new contract values are created, the size of the global state expands. Ethereum charges fixed amounts of Gas for inserting new values into its state storage and offers fixed amounts of Gas as transaction refund when values are removed. Ethereum's approach is a step in the right direction, but still has several issues:\r\n\r\n\r\n- Neither the size nor the growth rate of the global state is bounded, this gives very little certainty in the cost of full node participation.\r\n- The system raises one-time revenue for expanding the state storage, but miners and full nodes have to bear the cost of storage over time.\r\n- There's no obvious reason why the cost of expanding storage should be priced in fixed Gas amounts, which is designed as measurement to price units of computation.\r\n- The \"pay once, occupy forever\" state storage model gives very little incentive for users to voluntarily clear state, and do so sooner than later.\r\n\r\nThe Ethereum community is actively working on this problem, and the leading solution is to charge smart contract \"state rent\" - contracts have to periodically pay fees based on the size of its state. If the rent is not paid, the contract goes to \"hibernation\" and is not accessible before the payment is current again. We see several difficult-to-solve problems with this approach:\r\n\r\n\r\n- Many contracts, especially popular ERC20 contracts, represent decentralized communities and express asset ownership of many users. It's a difficult problem to coordinate all the users to pay for state rent in a fair and efficient way.\r\n- Even if a contract is current on its rent payment, it still may not be fully functional because some of its dependent contracts may be behind on their payments.\r\n- The user experience for contracts with state rent is sub-optimal\r\n\r\nWe believe a well-designed mechanism to regulate the state storage has to be able to achieve the following goals:\r\n\r\n\r\n- The growth of the global state has to be bounded to give predictability for full node participation. Ideally, the cost is well within the range of non-professional participants to keep the network maximally decentralized. Keeping this barrier low allows participants of the decentralized network to verify history and state independently, without having to trust a third party or service. This is fundamentally the reason why public blockchains are valuable.\r\n- With bounded growth of the global state, the price for expanding it and the rewards for reducing it should be determined by the market. In particular, it's desirable to have the cost of expanding state storage higher when it's mostly full, and lower when it's mostly empty.\r\n- The system has to be able to continuously raise revenue from its state users to pay miners for providing this resource. This serves both purposes of balancing miner's economics and providing incentives for users to clear unnecessary states sooner than later.\r\n\r\nJust like how Bitcoin throttles and forces pricing on bandwidth and Ethereum throttles and forces pricing on computation, to keep a blockchain network long term decentralized and sustainable, we have to come up with a way to constrain and price the global state. This is especially important for preservation focused, Store of Assets networks, where usage of the network is not about transactions that mostly happen off-chain, but ongoing occupation of the global state.\r\n\r\n## 6. The Economic Model of the Nervos Common Knowledge Base\r\n\r\nThe Nervos Common Knowledge Base (Nervos CKB for short) is a preservation focused, \"Store of Assets\" blockchain. Architecturally, it's designed to best support on-chain state and off-chain computation; economically, it's designed to provide sustainable security and decentralization.  Nervos CKB is the base layer of the overall Nervos Network.\r\n\r\n\r\n### Native Tokens\r\n\r\nThe native token for the Nervos CKB is the \"Common Knowledge Byte\", or \"CK Byte\" for short. The CK Bytes represent cell capacity in bytes, and they give owners the ability to occupy a piece of the blockchain's overall global state. For example, if Alice owns 1000 CK Bytes, she can create a cell with 1000 bytes in capacity, or multiple cells that add up to 1000 bytes in capacity. She can use the 1000 bytes to store assets, application state, or other types of common knowledge.\r\n\r\nA cell's occupied capacity could be equal to or less than its specified capacity. For example, for a 1000 byte cell, 4 bytes would be used to specify its own capacity, 64 bytes for the lock script and 128 bytes for storing state. Then the cell's current occupied capacity is 196 bytes, but with room to grow up to 1000 bytes.\r\n\r\nThe smallest unit of the native token is \"CK Shannon\": `1 CK Byte = 100_000_000 CK Shannons`.\r\n\"CK Shannon\" is the indivisible unit.\r\n\"CK Shannon\" is designed for the scenes that people want to transfer value less than one \"CK Byte\".\r\n\r\n### Token Issuance\r\n\r\nThere are two types of native token issuance. The \"base issuance\" has a finite total supply with a Bitcoin like issuance schedule - the number of base issuance halves approximately every 4 years until all the base issuance tokens are mined out. All base issuance tokens are rewarded to the miners as incentives to protect the network.\r\n\r\nThe \"secondary issuance\" is designed to collect state rent, and has issuance amount that is constant over time. After base issuance stops, there will only be secondary issuance.\r\n\r\n\r\n### Collecting State Rent with Secondary Issuance and the NervosDAO\r\n\r\nSince the native tokens represent right to expand the global state, the issuance policy of the native tokens bounds the state growth. As state storage is bounded and becomes a scarce resource like bandwidth in Bitcoin and computation throughput in Ethereum, they can be market priced and traded. State rent adds the necessary time dimension to the fee structure of state storage occupation. Instead of mandating periodic rent payments, we use a two-step approach as a \"targeted inflation\" scheme to collect this rent:\r\n\r\n\r\n- On top of the base issuance, we add the secondary issuance which can be seen as \"inflation tax\" to all existing token holders. For users who use their CK Bytes to store state, this recurring inflation tax is how they pay state rent to the miners.\r\n- However, we would have also collected rent from the CK Bytes that are not used to store state, and we need to return to them what we collected. We allow those users to deposit and lock their native tokens into a special contract called the NervosDAO. The NervosDAO receives part of the \"secondary issuance\" to make up for the otherwise unfair dilution.\r\n\r\nLet's suppose at the time of a secondary issuance event, 60% of all CK Bytes are used to store state, 35% of all CK Bytes are deposited and locked in the NervosDAO, and 5% of all CK Bytes are kept liquid. Then 60% of the secondary issuance goes to the miners, 35% of the issuance goes to the NervosDAO to be distributed to the locked tokens proportionally. The use of the rest of the secondary issuance - in this example, 5% of the that issuance - is determined by the community through the governance mechanism. Before the community can reach agreement, this part of the secondary issuance is going to be burned.\r\n\r\nFor long term token holders, as long as they lock their tokens in the NervosDAO, the inflationary effect of secondary issuance is only nominal. For them it's as if the secondary issuance doesn't exist, and they're holding hard-capped tokens like Bitcoin.\r\n\r\n\r\n### Miner Compensation\r\n\r\nMiners are compensated with both block rewards and transaction fees. They receive all the base issuance, and part of the secondary issuance. In the long term when base issuance stops, miners still receive state rent income that's independent of transactions but tied to the adoption of the common knowledge base.\r\n\r\n\r\n### Paying for Transaction Fees\r\n\r\nA decentralized blockchain network's transaction capacity is always limited. Transaction fees serve the dual purposes of establishing a market for the limited transaction capacity and as protection against spams. In Bitcoin, transaction fees are expressed with the difference between the outputs and inputs; In Ethereum, the user specify the per computation unit price they're willing to pay with `gasprice`, and use `gaslimit` to establish a budget for the entire transaction.\r\n\r\nTo ensure decentralization, the Nervos CKB restricts both computation and bandwidth throughput, effectively making it an auction for users to use those system resources. When submitting a transaction, the user can leave the total input cell capacities exceeding the total output cell capacities, leaving the difference as transaction fees expressed in the native tokens, payable to the miner that creates the block containing the transaction.\r\n\r\nThe number of units of computation (called \"cycles\") are added to the peer-to-peer messages between the full nodes. When producing blocks, miners order transactions based on both transaction fees and the number of computation cycles necessary for transaction validation, maximizing its per-computation-cycle income within the computation and bandwidth throughput restrictions.\r\n\r\nIn the Nervos CKB, the transaction fees can be paid with the native tokens, user defined tokens or a combination of both.\r\n\r\n### Paying for Transaction Fees with User Defined Tokens\r\n\r\nUsers are also free to use other tokens (for example, stable coins) to pay transactions fees, a concept known as \"Economic Abstraction\". Note that even without explicit protocol support, it's always possible to have users make arrangements with miners to pay transaction fees in other tokens outside of the protocol. This is often seen as a threat for many platforms - if the platform's native tokens are purely to facilitate transactions, this would take away its intrinsic value and cause a collapse.\r\n\r\nWith the Nervos CKB, economic abstraction is possible because the payment methods are not hard-coded in transactions. We embrace economic abstraction and the benefits it brings. Since the intrinsic value of the native tokens is based not on transaction payment, economic abstraction doesn't pose a threat to the stability of our economic model. We do expect, however, the native tokens themselves are going to be the payment method of choice for vast majority of users and use cases - the native tokens are going to be the most widely held tokens in the Nervos ecosystem, and everyone who owns assets necessarily owns the Nervos natives tokens as state storage capacity that the assets occupy.\r\n\r\nFor more a more detailed analysis on transaction payments, please see Appendix 1.\r\n\r\n## 7. An Economic Model Designed for Preservation\r\n\r\nThe economic model of the Nervos CKB is designed specifically to preserve assets and other types of common knowledge. Let's bring back the 3 high level design goals and examine our design in this context:\r\n\r\n\r\n- How can the economic model ensure the security of the protocol?\r\n- How can the economic model ensure long term sustainability of the protocol?\r\n- How can the economic model align the objectives of different actors to grow the value of the protocol network?\r\n\r\n\r\n### Security and Sustainability of the Protocol\r\n\r\nThe main design choices we made to ensure security of the Nervos CKB as a \"Store of Assets\" protocol are:\r\n\r\n\r\n- Our native tokens represent claim to capacity in the state storage. This means the demand to holding assets on the platform directly puts demand on owning the native tokens. This creates an effective value capture mechanism into the native tokens from the assets they preserve. We claim that this is the only sustainable way that a \"Store of Assets\" platform can grow its security budget over time, instead of entirely basing it on speculation and altruism.\r\n- The secondary issuance makes sure miner compensation is predictable and based on preservation demand instead of transactional demand. It also eliminates potential incentive incompatibility of the Nakamoto Consensus nodes after block reward stops. This is also important in a future when most transactions move to the layer 2, leaving a starved layer 1.\r\n- The NervosDAO serves as the counter-force to the inflationary effects of secondary issuance, to ensure long term token holders are not diluted by this issuance.\r\n\r\nFor a purpose of keeping the network decentralized and censorship resistant, we believe it's important to limit the resource requirements of consensus and full nodes. We protect the operating cost of nodes by regulating the throughput of computation and bandwidth, similar to how it's accomplished with Bitcoin and Ethereum. We regulate the state storage with a combination of a \"cap and trade\" pricing scheme and opportunity cost based cost model for storage users.\r\n\r\n\r\n### Aligning the Interests of Network Participants\r\n\r\nIn a typical smart contract platform, participants of the network have different interests - users want cheaper transactions, developers want adoption of their applications, miners want higher income, and holders want appreciation of their tokens. Those interests are not well aligned, and oftentimes in conflict - for example, more adoption won't give cheaper transactions (they'll be more expensive as more demand is put on the blockchain); cheaper transactions won't give more income to the miners; higher token price won't help with transaction cost (the opposite could happen if users don't adjust their local transaction fee setting). Decentralized computation platforms provide value through processing transactions. The price of their tokens doesn't materially change the intrinsic value of the network. For example, Ether's price doubling doesn't increase or decrease Ethereum's intrinsic value as a decentralized computation platform, because the introduction of Gas in the first place is to de-couple the price of computations from the price actions of Ether the cryptocurrency. This makes token holders of Ethereum only take the role of a speculator, instead of active contributors that can increase the value of the network.\r\n\r\nIn the Nervos CKB, Store of Assets users want security of their assets; developers want more adoption, reflected in more assets preserved; miners want higher income and token holders want price appreciation of their tokens. Higher token price supports everyone's objective - the network would be more secure, miners get higher income, and token holders get better return.\r\n\r\nAligning all participants' incentives allows the network to best harness network effects to grow its intrinsic value. It also produces a more cohesive community and makes the system less prune to governance challenges.\r\n\r\n### Bootstrapping Network Effect and Network Growth\r\n\r\nAs the network grows to secure more assets and common knowledge, more native tokens of the Nervos CKB are going to become occupied. This accrues value to the native tokens by reducing circulating supply and providing positive support to the market price of the tokens. The higher price and increased share of secondary issuance motivate miners to expand operations and make the network more secure, increasing the intrinsic value of the network and the native tokens, attracting more and higher value preservation usage.\r\n\r\nThe pro-cyclical loop of the network's adoption and network's intrinsic value provides a powerful growth engine for the network. Combined with how the network's value accrues to the native tokens and gets captured by long term holders, it makes the network's native token an excellent candidate for store of value. Compared to Bitcoin as a monetary store of value, the Nervos CKB is similarly designed to be secure and long term decentralized. We believe Nervos CKB has a more balanced and sustainable economic model than Bitcoin, and also comes with the intrinsic utility of securing crypto-assets and common knowledge.\r\n\r\n\r\n### Developer's Cost in a \"First Class Asset\" Platform\r\n\r\nIn Ethereum, the top-level abstraction is its accounts. Assets are expressed as state owned by smart contract accounts. In the Nervos CKB, assets are the first class abstraction with cells, where ownership is expressed with the lock script of a transaction output, a concept known as \"[First Class Assets](https://medium.com/nervosnetwork/first-class-asset-ff4feaf370c4)\". In other words, just like Bitcoin, assets in the Common Knowledge Base are owned by users directly instead of being kept custody in a smart contract.\r\n\r\nThe \"First Class Asset\" design allows the state storage cost of owning assets put not on developers, but on individual users. For example, a developer could create a User Defined Token with 400 bytes of code as validation rules, and every record of asset ownership would take 64 bytes. Even if the assets were to have 10,000 owners, the developer would still only need to use 400 CK Bytes.\r\n\r\nFor developers, we expect the capital cost of building projects on the CKB is moderate even in a scenario that the price of the native tokens were to go up degrees of magnitude higher. For users, the cost of the 64 CK Bytes to own an asset on the Nervos CKB would also be trivial for a long time even in the most aggressive adoption assumption of the platform.\r\n\r\nIn the future where those cost were to become meaningfully expensive, it's always possible for developers to rely on lending to bootstrap their projects and for users to move their assets off the Common Knowledge Base on to other transaction blockchains in the Nervos Network if they're willing to take the corresponding trade-offs. Please see the \"Nervos Network\" section for more details.\r\n\r\n### Lending\r\n\r\nNervos CKB will support native token lending to improve the liquidity of the CK Bytes thanks to the programming ability provided by CKB-VM and the Cell model. Since the utility of the native token is realized through possession instead of transactions, it's possible to have risk-free un-collateralized lending for CK Bytes locked for known duration of time.  Entrepreneurs can borrow the CK Bytes they need with much lower capital cost for a period such as 6 months to work on prototypes and prove their business model. Long term users can lend out their tokens to earn extra income.\r\n\r\nThe effective interest rate of lending is determined by the market supply and demand, but the current state of token utilization also plays a big role. Higher utilization of the available global state means fewer tokens can be made available for lending. This makes the lending interest higher and makes it more attractive to release state and lock tokens in the NervosDAO to earn income. It serves the purpose to help reduce the global state: lower utilization of the available state means more tokens can be lent out. It makes the lending interest rate lower to encourage adoption.\r\n\r\n\r\n### Nervos Network\r\n\r\nThe Nervos CKB is the base layer of the Nervos Network with the highest security, decentralization, transaction cost and state storage cost. Just like how Bitcoin and Ethereum could scale off-chain with lightning network and plasma solutions, Nervos CKB also embraces off-chain scaling solutions and allow users to preserve and transact assets off-chain. When using off-chain solutions, users and developers can choose their own trade-offs between cost, security, latency and liveness properties.\r\n\r\nOwning and transacting assets on the Nervos CKB come with the highest capital and transaction cost, but is also the most secure. It's best suited for high value assets and long term asset preservation; Layer 2 solutions can provide scaling for both transaction throughput and state storage, but they would come with either weakened security assumptions or mandate extra steps of repudiation. They also often require participants to be online within a time window. If both are acceptable (likely for owning and transacting low value assets for short duration), the Nervos CKB can be used as security anchor to other transaction blockchains, to effectively magnify both its transaction and state storage capacities.\r\n\r\nIf operators of transaction blockchains don't want to introduce extra security assumptions, they can mandate that high value assets be issued on the CKB and low value assets be issued on transactional blockchains. Then they can use CK Bytes on the CKB to store periodic block commits, challenges and proofs from the transactional blockchains - critical common knowledge for secure off-chain transaction repudiation. If a transaction chain doesn't mind introducing an extra layer of security assumption with a committee-based consensus protocol, they could also have their validators bond CK Bytes on the CKB to explicitly adjust security parameters.\r\n\r\n## 8. Applications of the Token Economics Model\r\n\r\nThe economic model of the Nervos CKB provides building blocks that application developers can use directly as part of their own economic model. We'll list subscriptions and liquidity income as two such possible building blocks.\r\n\r\n### Subscriptions\r\n\r\nRecurring payment or subscription is a typical economic model for services offered on the blockchain that span over some duration of time. One such example is the off-chain transaction monitoring service that's often needed for layer 2 solutions. On the Nervos CKB, duration based services can ask their users to lock certain amount of native tokens in the NervosDAO and designate the service providers as the beneficiaries of the generated interest income in a subscription based model. Users can stop using the services by withdrawing their tokens from the NervosDAO.\r\n\r\nIn fact, Store of Assets users that occupy global state can be seen as paying an ongoing subscription metered by the size of their state, and the beneficiaries are the miners that provide the security service.\r\n\r\n### Liquidity Income\r\n\r\nIn a Plasma like layer 2 solution, a typical pattern is that users would deposit native tokens in a smart contract on the layer 1 blockchain in exchange for transaction tokens on the layer 2. A layer 2 operator with sufficient reputation can have users commit to fixed duration deposits, and then use such deposits to provide liquidity to the lending market and earn income. This gives operators of layer 2 solutions an additional revenue stream on top of the fees collected on layer 2.\r\n\r\n## Appendix 1: Transaction Cost Analysis\r\n\r\nNervos CKB uses Proof of Work based Nakamoto consensus, similar to what's used in Bitcoin - for more details, please see the \"Nervos Consensus Paper\"\r\n\r\nThe economics of the consensus process is designed to incentivize nodes to participate in the consensus process and provide measurements that nodes can use to prioritize transactions.  At the core, it's designed to help consensus nodes answer the question: \"Is this transaction worth to be included in the next block if I had the opportunity to produce the block?\"\r\n\r\nA block producing node can do a cost/benefit analysis to answer this question. The benefit of including a transaction is to be able to collect its transaction fee, and the cost of including a transaction in a block has three parts:\r\n\r\n\r\n- Fee Estimation Cost (![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/01.png) ): this is the cost to estimate the maximum possible income if a node where to include a transaction\r\n- Transaction Verification Cost (![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/02.png) ): blocks containing invalid transactions will be rejected by the consensus process, therefore block producing nodes have to verify transactions before including them in a new block.\r\n- State Transition Cost (![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/03.png)）: after a block is produced, the block producing node has to perform local state transitions defined by state machines of the transactions in the block.\r\n\r\nIn particular, transaction verification, ![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/04.png)  has two possible steps:\r\n\r\n- ![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/05.png): Authorization Verification Cost\r\n\r\n- ![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/06.png): State Transition Verification Cost\r\n\r\nWe use CPC and EVC to represent Complete Processing Cost and Estimation and Verification Cost:\r\n\r\n- CPC: Complete Processing Cost\r\n  - ![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/07.png)\r\n- EVC: Estimation and Verification Cost;\r\n  - ![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/08.png)\r\n\r\n### Bitcoin's Transaction Cost Analysis\r\n\r\nBitcoin allows flexible authorization verification with the Bitcoin Script. Users can script the authorization rules and build smart contracts through ![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/09.png) when creating transactions. Bitcoin has a fixed state transition semantic, which is to spend and create new UTXOs. In Bitcoin, the result of the state transitions are already included in transactions, therefore the State Transition Cost (STC) is 0.\r\n\r\nBitcoin uses the amount difference of the inputs and outputs to express transaction fees. Therefore, the cost of estimating transaction fees scales to ![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/10.png) where ![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/11.png) is the total number of inputs and outputs.\r\n\r\nAuthorization verification in Bitcoin requires running scripts of all inputs. Because the Bitcoin Script prohibits JUMP/looping, the computation complexity can roughly scale to the length of the input scripts, as![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/12.png), where ![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/13.png) is the number of inputs and ![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/14.png) is the average script length of an input. Therefore, the total cost of ![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/15.png) roughly scales to the size of total transaction.\r\n\r\nBitcoin's state transition rules are simple, and nodes only have to verify the total input amount is the same as the total output amount. Therefore, the ![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/16.png) in Bitcoin is the same as ![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/17.png), also scaling to ![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/18.png).\r\n\r\nIn total, Bitcoin's cost of processing a transaction roughly scales to the size of the transaction:\r\n![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/19.png)\r\n\r\n### Ethereum's Transaction Cost Analysis\r\n\r\nEthereum comes with Turing-complete scriptability, and gives users more flexibility to customize state transition rules with smart contracts. Ethereum transactions include *gaslimit* and *gasprice*, and the transaction fees are calculated using the product of their multiplication. Therefore, ![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/20.png) is ![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/21.png).\r\n\r\nUnlike Bitcoin, Ethereum's transactions only include the computation commands of state transitions, instead of the results of the state transitions. Therefore, Ethereum's transaction verification is limited to authorization verification, and doesn't have state transition verification. The rules of authorization verification in Ethereum are:\r\n\r\n\r\n- Verify the validility of the Secp256k1 signatures, with computation complexity of ![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/22.png)\r\n- Verify the nonce match of the transaction and the account that starts the transaction, with computation complexity of ![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/23.png)\r\n- Verify the account that starts transaction has enough ether to pay for the transaction fees and the amount transferred. This requires access to the account's current balance. Ignoring the global state size's impact on account access, we can assume the complexity of this step is also ![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/23.png).\r\n\r\nBased on the above, the overall authorization verification complexity in Ethereum is ![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/24.png).\r\n\r\nSince every byte of the transaction data comes with cost ![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/25.png), the larger ![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/26.png) is, the more gas it needs, up to the *gaslimit* ![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/27.png)specified. Therefore,\r\n\r\n![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/54.png)\r\n\r\nEthereum comes with a Turing complete VM, and the computation of the result state could include logic of any complexity. Ethereum transaction's ![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/28.png) caps the upper bound of computation, therefore ![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/29.png)。To summarize all the above:\r\n\r\n![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/30.png)\r\n\r\n![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/31.png)\r\n\r\n![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/32.png)\r\n\r\nDifferent from Bitcoin, ![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/33.png) for the Ethereum nodes is less than ![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/34.png). This is because Ethereum nodes only compute the result state after transactions are included in the block. This is also the reason that transaction results on Ethereum could be invalid, (e.g. exceptions in contract invocation or the gas limit is exceeded),  but the Bitcoin blockchain only has successfully executed transactions and valid results.\r\n\r\n### Nervos CKB's Transaction Cost Analysis\r\n\r\nNervos CKB's transactions are structured with inputs and outputs, similar to Bitcoin's. Therefore, the ![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/35.png) and ![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/36.png) for the Nervos CKB are the same as those of Bitcoin's:\r\n\r\n![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/37.png)\r\n\r\n![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/38.png)\r\n\r\nBecause CKB transactions include the result of the transactions as outputs, therefore:\r\n\r\n![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/39.png)\r\n\r\n### Cycles as Measurement Units of Computation Complexity\r\n\r\nWe introduce \"cycle\" as a unit of measurement for computation complexity in the CKB, similar to the \"gas\" concept in Ethereum. Nervos CKB's VM is a RISC-V CPU simulator, therefore cycles here refer to real CPU computation cycles in the VM. The cycle number for an instruction represents the relative computation cost of that instruction. Transactions in the Nervos CKB require the sender to specify the number of cycles required for its verification. Nodes can opt to set an acceptable cycle upper bound *cyclemax*, and only process transactions with fewer cycles. We'll also introduce *cycles* to a block, with its value equal to the sum of all specified transaction cycles.  The value of *cycles* in a block can't exceed the value *blockcyclesmax*, which are set and can be automatically adjusted by the system.\r\n\r\nNodes can set their *cyclemax* to different values. *cyclemax* only impacts how a block producing node accepts new transactions, not how a node accepts transactions in a new block. Therefore, it's not going to cause inconsistency in the validation of blocks. A valid block needs valid proof of work, and this cost discourages a block producing node to include an invalid transaction with high *cycles* value.\r\n\r\nThe following table shows the runtime differences in Bitcoin, Ethereum and the Nervos CKB.\r\n\r\n|          | Authorization (![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/40.png)） | State Validation (![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/41.png)) | State Transition（![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/42.png)） |\r\n| -------- | ---------------------------- | ----------------------------- | ------------------------ |\r\n| Bitcoin  | Generalized                  | Fixed                         | None                     |\r\n| Ethereum | Fixed                        | None                          | Generalized              |\r\n| CKB      | Generalized                  | Generalized                   | None                     |\r\n\r\n\r\nHere's a summary of the computational complexity of different parts of the consensus process for Bitcoin, Ethereum and Nervos CKB (![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/55.png) means cycle limit)\r\n\r\n|          |![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/43.png)     |![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/44.png)         |![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/45.png)          |![](https://raw.githubusercontent.com/Jack0814/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/46.png)          | ![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/47.png)         |\r\n| -------- | ------------- | ---------------- | ---------------- | ---------------- | ---------------- |\r\n| Bitcoin  |![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/48.png)| ![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/49.png)    | ![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/50.png)         | ![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/49.png)   | ![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/49.png)  |\r\n| Ethereum |![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/51.png)     |![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/49.png)    | ![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/52.png) | ![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/49.png)     | ![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/52.png) |\r\n| CKB      | ![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/48.png)  | ![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/53.png) | ![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/50.png)           | ![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/53.png)  | ![](https://raw.githubusercontent.com/nervosnetwork/rfcs/master/rfcs/0015-ckb-cryptoeconomics/images/53.png) |\r\n","\\0017-tx-valid-since\\0017-tx-valid-since.md":"---\r\nNumber: \"0017\"\r\nCategory: Standards Track\r\nStatus: Proposal\r\nAuthor: Jinyang Jiang\r\nOrganization: Nervos Foundation\r\nCreated: 2019-03-11\r\n---\r\n\r\n# Transaction valid since\r\n\r\n## Abstract\r\n\r\nThis RFC suggests adding a new consensus rule to prevent a cell to be spent before a certain block timestamp or a block number.\r\n\r\n## Summary \r\n\r\nTransaction input adds a new `u64` (unsigned 64-bit integer) type field `since`, which prevents the transaction to be mined before an absolute or relative time.\r\n\r\nThe highest 8 bits of `since` is `flags`, the remain `56` bits represent `value`, `flags` allow us to determine behaviours:\r\n* `flags & (1 << 7)` represent `relative_flag`.\r\n* `flags & (1 << 6)` and `flags & (1 << 5)` together represent `metric_flag`.\r\n    * `since` use a block based lock-time if `metric_flag` is `00`, `value` can be explained as a block number or a relative block number.\r\n    * `since` use a epoch based lock-time if `metric_flag` is `01`, `value` can be explained as a epoch number or a relative epoch number.\r\n    * `since` use a time based lock-time if `metric_flag` is `10`, `value` can be explained as a block timestamp(unix time) or a relative seconds.\r\n    * `metric_flag` `11` is invalid.\r\n* other 6 `flags` bits remain for other use.\r\n\r\nThe consensus to validate this field described as follow:\r\n* iterate inputs, and validate each input by following rules.\r\n* ignore this validate rule if all 64 bits of `since` are 0.\r\n* check `metric_flag` flag:\r\n    * the lower 56 bits of `since` represent block number if `metric_flag` is `00`.\r\n    * the lower 56 bits of `since` represent epoch number if `metric_flag` is `01`.\r\n    * the lower 56 bits of `since` represent block timestamp if `metric_flag` is `10`.\r\n* check `relative_flag`:\r\n    * consider field as absolute lock time if `relative_flag` is `0`:\r\n        * fail the validation if tip's block number or epoch number or block timestamp is less than `since` field.\r\n    * consider field as relative lock time if `relative_flag` is `1`:\r\n        * find the block which produced the input cell, get the block timestamp or block number or epoch number based on `metric_flag` flag.\r\n        * fail the validation if tip's number or epoch number or timestamp minus block's number or epoch number or timestamp is less than `since` field.\r\n* Otherwise, the validation SHOULD continue.\r\n\r\nA cell lock script can check the `since` field of an input and return invalid when `since` not satisfied condition, to indirectly prevent cell to be spent.\r\n\r\nThis provides the ability to implement time-based fund lock scripts:\r\n\r\n``` ruby\r\n# absolute time lock\r\n# cell only can be spent when block number greater than 10000.\r\ndef unlock?\r\n  input = CKB.load_current_input\r\n  # fail if it is relative lock\r\n  return false if input.since[63] == 1\r\n  # fail if metric_flag is not block_number\r\n  return false (input.since & 0x6000_0000_0000_0000) != (0b0000_0000 << 56)\r\n  input.since > 10000\r\nend\r\n```\r\n\r\n``` ruby\r\n# relative time lock\r\n# cell only can be spent after 3 days after block that produced this cell get confirmed\r\ndef unlock?\r\n  input = CKB.load_current_input\r\n  # fail if it is absolute lock\r\n  return false if input.since[63].zero?\r\n  # fail if metric_flag is not timestamp\r\n  return false (input.since & 0x6000_0000_0000_0000) != (0b0100_0000 << 56)\r\n  # extract lower 56 bits and convert to seconds\r\n  time = since & 0x00ffffffffffffff\r\n  # check time must greater than 3 days\r\n  time > 3 * 24 * 3600\r\nend\r\n```\r\n\r\n``` ruby\r\n# relative time lock with epoch number\r\n# cell only can be spent in next epoch\r\ndef unlock?\r\n  input = CKB.load_current_input\r\n  # fail if it is absolute lock\r\n  return false if input.since[63].zero?\r\n  # fail if metric_flag is not epoch number\r\n  return false (input.since & 0x6000_0000_0000_0000) != (0b0010_0000 << 56)\r\n  # extract lower 56 bits and convert to value\r\n  epoch_number = since & 0x00ffffffffffffff\r\n  # enforce only can unlock in next or further epochs\r\n  epoch_number >= 1\r\nend\r\n```\r\n\r\n## Detailed Specification\r\n\r\n`since` SHOULD be validated with the median timestamp of the past 11 blocks to instead the block timestamp when `metric flag` is `10`, this prevents miner lie on the timestamp for earning more fees by including more transactions that immature.\r\n\r\nThe median block time calculated from the past 11 blocks timestamp (from block's parent), we pick the older timestamp as median if blocks number is not enough and is odd, the details behavior defined as the following code:\r\n\r\n``` rust\r\npub trait BlockMedianTimeContext {\r\n    fn median_block_count(&self) -> u64;\r\n    /// block timestamp\r\n    fn timestamp(&self, block_number: BlockNumber) -> Option<u64>;\r\n    /// ancestor timestamps from a block\r\n    fn ancestor_timestamps(&self, block_number: BlockNumber) -> Vec<u64> {\r\n        let count = self.median_block_count();\r\n        (block_number.saturating_sub(count)..=block_number)\r\n            .filter_map(|n| self.timestamp(n))\r\n            .collect()\r\n    }\r\n\r\n    /// get block median time\r\n    fn block_median_time(&self, block_number: BlockNumber) -> Option<u64> {\r\n        let mut timestamps: Vec<u64> = self.ancestor_timestamps(block_number);\r\n        timestamps.sort_by(|a, b| a.cmp(b));\r\n        // return greater one if count is even.\r\n        timestamps.get(timestamps.len() / 2).cloned()\r\n    }\r\n}\r\n```\r\n\r\nValidation of transaction `since` defined as follow code:\r\n\r\n``` rust\r\nconst LOCK_TYPE_FLAG: u64 = 1 << 63;\r\nconst METRIC_TYPE_FLAG_MASK: u64 = 0x6000_0000_0000_0000;\r\nconst VALUE_MASK: u64 = 0x00ff_ffff_ffff_ffff;\r\nconst REMAIN_FLAGS_BITS: u64 = 0x1f00_0000_0000_0000;\r\n\r\nenum SinceMetric {\r\n    BlockNumber(u64),\r\n    EpochNumber(u64),\r\n    Timestamp(u64),\r\n}\r\n\r\n/// RFC 0017\r\n#[derive(Copy, Clone, Debug)]\r\nstruct Since(u64);\r\n\r\nimpl Since {\r\n    pub fn is_absolute(self) -> bool {\r\n        self.0 & LOCK_TYPE_FLAG == 0\r\n    }\r\n\r\n    #[inline]\r\n    pub fn is_relative(self) -> bool {\r\n        !self.is_absolute()\r\n    }\r\n\r\n    pub fn flags_is_valid(self) -> bool {\r\n        (self.0 & REMAIN_FLAGS_BITS == 0)\r\n            && ((self.0 & METRIC_TYPE_FLAG_MASK) != (0b0110_0000 << 56))\r\n    }\r\n\r\n    fn extract_metric(self) -> Option<SinceMetric> {\r\n        let value = self.0 & VALUE_MASK;\r\n        match self.0 & METRIC_TYPE_FLAG_MASK {\r\n            //0b0000_0000\r\n            0x0000_0000_0000_0000 => Some(SinceMetric::BlockNumber(value)),\r\n            //0b0010_0000\r\n            0x2000_0000_0000_0000 => Some(SinceMetric::EpochNumber(value)),\r\n            //0b0100_0000\r\n            0x4000_0000_0000_0000 => Some(SinceMetric::Timestamp(value * 1000)),\r\n            _ => None,\r\n        }\r\n    }\r\n}\r\n\r\n/// https://github.com/nervosnetwork/rfcs/blob/master/rfcs/0017-tx-valid-since/0017-tx-valid-since.md#detailed-specification\r\npub struct SinceVerifier<'a, M> {\r\n    rtx: &'a ResolvedTransaction<'a>,\r\n    block_median_time_context: &'a M,\r\n    tip_number: BlockNumber,\r\n    tip_epoch_number: EpochNumber,\r\n    median_timestamps_cache: RefCell<LruCache<BlockNumber, Option<u64>>>,\r\n}\r\n\r\nimpl<'a, M> SinceVerifier<'a, M>\r\nwhere\r\n    M: BlockMedianTimeContext,\r\n{\r\n    pub fn new(\r\n        rtx: &'a ResolvedTransaction,\r\n        block_median_time_context: &'a M,\r\n        tip_number: BlockNumber,\r\n        tip_epoch_number: BlockNumber,\r\n    ) -> Self {\r\n        let median_timestamps_cache = RefCell::new(LruCache::new(rtx.resolved_inputs.len()));\r\n        SinceVerifier {\r\n            rtx,\r\n            block_median_time_context,\r\n            tip_number,\r\n            tip_epoch_number,\r\n            median_timestamps_cache,\r\n        }\r\n    }\r\n\r\n    fn block_median_time(&self, n: BlockNumber) -> Option<u64> {\r\n        let result = self.median_timestamps_cache.borrow().get(&n).cloned();\r\n        match result {\r\n            Some(r) => r,\r\n            None => {\r\n                let timestamp = self.block_median_time_context.block_median_time(n);\r\n                self.median_timestamps_cache\r\n                    .borrow_mut()\r\n                    .insert(n, timestamp);\r\n                timestamp\r\n            }\r\n        }\r\n    }\r\n\r\n    fn verify_absolute_lock(&self, since: Since) -> Result<(), TransactionError> {\r\n        if since.is_absolute() {\r\n            match since.extract_metric() {\r\n                Some(SinceMetric::BlockNumber(block_number)) => {\r\n                    if self.tip_number < block_number {\r\n                        return Err(TransactionError::Immature);\r\n                    }\r\n                }\r\n                Some(SinceMetric::EpochNumber(epoch_number)) => {\r\n                    if self.tip_epoch_number < epoch_number {\r\n                        return Err(TransactionError::Immature);\r\n                    }\r\n                }\r\n                Some(SinceMetric::Timestamp(timestamp)) => {\r\n                    let tip_timestamp = self\r\n                        .block_median_time(self.tip_number.saturating_sub(1))\r\n                        .unwrap_or_else(|| 0);\r\n                    if tip_timestamp < timestamp {\r\n                        return Err(TransactionError::Immature);\r\n                    }\r\n                }\r\n                None => {\r\n                    return Err(TransactionError::InvalidSince);\r\n                }\r\n            }\r\n        }\r\n        Ok(())\r\n    }\r\n    fn verify_relative_lock(\r\n        &self,\r\n        since: Since,\r\n        cell_meta: &CellMeta,\r\n    ) -> Result<(), TransactionError> {\r\n        if since.is_relative() {\r\n            // cell still in tx_pool\r\n            let (cell_block_number, cell_epoch_number) = match cell_meta.block_info {\r\n                Some(ref block_info) => (block_info.number, block_info.epoch),\r\n                None => return Err(TransactionError::Immature),\r\n            };\r\n            match since.extract_metric() {\r\n                Some(SinceMetric::BlockNumber(block_number)) => {\r\n                    if self.tip_number < cell_block_number + block_number {\r\n                        return Err(TransactionError::Immature);\r\n                    }\r\n                }\r\n                Some(SinceMetric::EpochNumber(epoch_number)) => {\r\n                    if self.tip_epoch_number < cell_epoch_number + epoch_number {\r\n                        return Err(TransactionError::Immature);\r\n                    }\r\n                }\r\n                Some(SinceMetric::Timestamp(timestamp)) => {\r\n                    let tip_timestamp = self\r\n                        .block_median_time(self.tip_number.saturating_sub(1))\r\n                        .unwrap_or_else(|| 0);\r\n                    let median_timestamp = self\r\n                        .block_median_time(cell_block_number.saturating_sub(1))\r\n                        .unwrap_or_else(|| 0);\r\n                    if tip_timestamp < median_timestamp + timestamp {\r\n                        return Err(TransactionError::Immature);\r\n                    }\r\n                }\r\n                None => {\r\n                    return Err(TransactionError::InvalidSince);\r\n                }\r\n            }\r\n        }\r\n        Ok(())\r\n    }\r\n\r\n    pub fn verify(&self) -> Result<(), TransactionError> {\r\n        for (resolved_out_point, input) in self\r\n            .rtx\r\n            .resolved_inputs\r\n            .iter()\r\n            .zip(self.rtx.transaction.inputs())\r\n        {\r\n            if resolved_out_point.cell().is_none() {\r\n                continue;\r\n            }\r\n            let cell_meta = resolved_out_point.cell().unwrap();\r\n            // ignore empty since\r\n            if input.since == 0 {\r\n                continue;\r\n            }\r\n            let since = Since(input.since);\r\n            // check remain flags\r\n            if !since.flags_is_valid() {\r\n                return Err(TransactionError::InvalidSince);\r\n            }\r\n\r\n            // verify time lock\r\n            self.verify_absolute_lock(since)?;\r\n            self.verify_relative_lock(since, cell_meta)?;\r\n        }\r\n        Ok(())\r\n    }\r\n}\r\n```\r\n\r\n","\\0019-data-structures\\0019-data-structures.md":"---\r\nNumber: \"0019\"\r\nCategory: Informational\r\nStatus: Draft\r\nAuthor: Haichao Zhu, Xuejie Xiao\r\nOrganization: Nervos Foundation\r\nCreated: 2019-03-26\r\n---\r\n\r\n# Data Structures of Nervos CKB\r\n\r\nThis documents explains all the basic data structures used in CKB.\r\n\r\n* [Cell](#Cell)\r\n* [Script](#Script)\r\n* [Transaction](#Transaction)\r\n* [Block](#Block)\r\n\r\n\r\n\r\n## Cell\r\n\r\n### Example\r\n\r\n```json\r\n{\r\n    \"capacity\": 500_000_000_000_000,\r\n    \"data\": \"0x\",\r\n    \"lock\": {\r\n      \"args\": [],\r\n      \"binary_hash\": \"0xa58a960b28d6e283546e38740e80142da94f88e88d5114d8dc91312b8da4765a\"\r\n    },\r\n    \"type\": null\r\n}\r\n```\r\n\r\n## Description\r\n\r\n| Name       | Type       | Description                                                  |\r\n| :--------- | :--------- | :----------------------------------------------------------- |\r\n| `capacity` | uint64     | **The size of the cell (in shannons).** When a new cell is generated (via transaction), one of the verification rule is `capacity_in_bytes >= len(capacity) + len(data) + len(type) + len(lock)`. This value also represents the balance of CKB coin, just like the `nValue` field in the Bitcoin's CTxOut. (E.g. Alice owns 100 CKB coins means she can unlock a group of cells that has 100 amount of `bytes` (which is 10_000_000_000 amount of `shannons`) in total.) |\r\n| `data`     | Bytes      | **Arbitrary data.** This part is for storing states or scripts.  In order to make this cell valid on-chain, the data filled in this field should comply with the logics and rules defined by `type`. |\r\n| `type`     | `Script`   | **A Script that defines the type of the cell.** It limits how the `data` field of the new cells can be changed from old cells. `type` is required to has a data structure of `script`. **This field is optional.** |\r\n| `lock`     | `Script`   | **A Script that defines the ownership of the cell**, just like the `scriptPubKey` field in the Bitcoin's CTxOut. Whoever can provide unlock arguments that makes the execution of this script success can consume this cell as input in an transaction (i.e. has the ownership of this cell). |\r\n\r\n\r\n\r\nMore information about Cell can be found in the [whitepaper](https://github.com/nervosnetwork/rfcs/blob/master/rfcs/0002-ckb/0002-ckb.md#42-cell).\r\n\r\n\r\n\r\n## Script\r\n\r\n### Example\r\n\r\n```json\r\n{\r\n  \"binary_hash\": \"0x12b464bcab8f55822501cdb91ea35ea707d72ec970363972388a0c49b94d377c\",\r\n  \"args\": [\r\n    \"3044022038f282cffdd26e2a050d7779ddc29be81a7e2f8a73706d2b7a6fde8a78e950ee0220538657b4c01be3e77827a82e92d33a923e864c55b88fd18cd5e5b25597432e9b\",\r\n    \"1\"\r\n  ]\r\n}\r\n```\r\n\r\n\r\n\r\n### Description\r\n\r\n| Name          | Type       | Description                                                  |\r\n| :------------ | :--------- | :----------------------------------------------------------- |\r\n| `binary_hash` | H256(hash) | **The hash of ELF formatted RISC-V binary that contains a CKB script.** For space efficiency consideration, the actual script is attached to current transaction as a dep cell, the hash specified here should match the hash of cell data part in the dep cell. The actual binary is loaded into an CKB-VM instance when they are specified upon the transaction verification. |\r\n| `args`        | [Bytes]    | **An array of arguments as the script input.** The arguments here are imported into the CKB-VM instance as input arguments for the scripts. Note that for lock scripts, the corresponding CellInput would have another args field which is appended to the array here to form the complete input argument list. |\r\n\r\n\r\n\r\nWhen a script is validated, CKB will run it in a RISC-V VM, `args` will be included via UNIX standard `argc`/`argv` convention. For more information on the CKB VM please refer to [CKB VM RFC](../0003-ckb-vm/0003-ckb-vm.md).\r\n\r\nFor more information regardingt how `Script` structure is implemented please refer to the [CKB repo](https://github.com/nervosnetwork/ckb).\r\n\r\n\r\n\r\n## Transaction\r\n\r\n### Example\r\n\r\n```json\r\n{\r\n    \"deps\": [],\r\n    \"inputs\": [\r\n      {\r\n        \"previous_output\": {\r\n          \"hash\": \"0x0000000000000000000000000000000000000000000000000000000000000000\",\r\n          \"index\": 4294967295\r\n        },\r\n        \"args\": []\r\n      }\r\n    ],\r\n    \"outputs\": [\r\n      {\r\n        \"capacity\": 500_000_000_000_000,\r\n        \"data\": \"0x\",\r\n        \"lock\": {\r\n          \"args\": [],\r\n          \"binary_hash\": \"0xa58a960b28d6e283546e38740e80142da94f88e88d5114d8dc91312b8da4765a\"\r\n        },\r\n        \"type\": null\r\n      }\r\n    ],\r\n    \"version\": 0\r\n}\r\n```\r\n\r\n### Description\r\n\r\n#### Transaction\r\n\r\n| Name              | Type                           | Description                                                  |\r\n| ----------------- | ------------------------------ | ------------------------------------------------------------ |\r\n| `version`         | uint32                         | **The version of the transaction.** It‘s used to distinguish transactions when there's a fork happened to the blockchain system. |\r\n| `deps`            | [`outpoint`]                   | **An array of `outpoint` that point to the cells that are dependencies of this transaction.** Only live cells can be listed here. The cells listed are read-only. |\r\n| `inputs`          | [{`previous_output` , `args`}] | **An array of {`previous_output`, `args`}.** |\r\n| `previous_output` | `outpoint`                     | **A cell outpoint that point to the cells used as inputs.** Input cells are in fact the output of previous transactions, hence they are noted as `previous_output` here. These cells are referred through  `outpoint`, which contains the transaction `hash` of the previous transaction, as well as this cell's `index` in its transaction's output list. |\r\n| `args`            | [Bytes]                        | **Additional input arguments provided by transaction creator to make the execution of corresponding lock script success**. One example here, is that signatures might be include here to make sure a signature verification lock script passes. |\r\n| `outputs`         | [`cell`]                       | **An array of cells that are used as outputs**, i.e. the newly generated cells. These are the cells may be used as inputs for other transactions. Each of the Cell has the same structure to [the Cell section](#cell) above. |\r\n\r\n\r\n\r\n#### OutPoint\r\n\r\n\r\n\r\n| Name             | Type               | Description                                                  |\r\n| ---------------- | ------------------ | ------------------------------------------------------------ |\r\n| `outpoint`       | {`hash` , `index`} | **An outpoint is pointer to a specific cell.** This is used in a transaction to refer a cell that is generated in a previous transaction. |\r\n| `outpoint.hash`  | H256(hash)         | **The hash of the transaction that this cell belongs to.**   |\r\n| `outpoint.index` | uint32             | **The index of the cell in its transaction's output list.**  |\r\n\r\n\r\n\r\n\r\n\r\nMore information about the Transaction of Nervos CKB can be found in [whitepaper](../0002-ckb/0002-ckb.md#44-transaction).\r\n\r\n\r\n\r\n## Block\r\n\r\n### Example\r\n\r\n```json\r\n{\r\n  \"transactions\": [\r\n    {\r\n      \"deps\": [],\r\n      \"inputs\": [\r\n          {\r\n            \"previous_output\": {\r\n              \"hash\": \"0x0000000000000000000000000000000000000000000000000000000000000000\",\r\n              \"index\": 4294967295\r\n            },\r\n            \"args\": []\r\n          }\r\n      ],\r\n      \"outputs\": [\r\n        {\r\n          \"capacity\": 500_000_000_000_000,\r\n          \"data\": \"0x\",\r\n          \"lock\": {\r\n            \"args\": [],\r\n            \"binary_hash\": \"0xa58a960b28d6e283546e38740e80142da94f88e88d5114d8dc91312b8da4765a\"\r\n          },\r\n          \"type\": null\r\n        }\r\n      ],\r\n      \"version\": 0\r\n    }\r\n  ],\r\n  \"header\": {\r\n    \"difficulty\": \"0x100\",\r\n    \"number\": 11,\r\n    \"parent_hash\": \"0x255f65bf9dc00bcd9f9b8be8624be222cba16b51366208a8267f1925eb40e7e4\",\r\n    \"seal\": {\r\n        \"nonce\": 503529102265201399,\r\n        \"proof\": \"0x\"\r\n    },\r\n    \"timestamp\": 1551155125985,\r\n    \"txs_commit\": \"0xabeb06aea75b59ec316db9d21243ee3f0b0ad0723e50f57761cef7e07974b9b5\",\r\n    \"txs_proposal\": \"0x0000000000000000000000000000000000000000000000000000000000000000\",\r\n    \"uncles_count\": 1,\r\n    \"uncles_hash\": \"0x99cf8710e59303bfac236b57256fcea2c58192f2c9c39d1ea4c19cbcf88b4952\",\r\n    \"version\": 0\r\n  },\r\n  \"proposals\": [],\r\n  \"uncles\": [\r\n    {\r\n    \"cellbase\": {\r\n        ...\r\n    },\r\n    \"header\": {\r\n        ...\r\n    },\r\n    \"proposals\": []\r\n    }\r\n  ]\r\n}\r\n```\r\n\r\n### Description\r\n\r\n#### Block\r\n\r\n| Name                    | Type            | Description                                                  |\r\n| ----------------------- | --------------- | ------------------------------------------------------------ |\r\n| `header`                | `Header`        | **The block header of the block.** This part contains some metadata of the block. See [the Header section](#header) below for the details of this part. |\r\n| `trasactions`           | [`Transaction`] | **An array of committed transactions contained in the block.** Each element of this array has the same structure as [the Transaction structure](#transaction) above. |\r\n| `proposals`             | [string]        | **An array of hex-encoded short transaction ID of the proposed transactions.** |\r\n| `uncles`                | [`UncleBlock`]  | **An array of uncle blocks of the block.** See [the UncleBlock section](#uncleblock) below for the details of this part. |\r\n\r\n#### Header\r\n\r\n(`header` is a sub-structure of `block` and `UncleBlock`.)\r\n\r\n| Name           | Type                | Description                                                  |\r\n| -------------- | ------------------- | ------------------------------------------------------------ |\r\n| `difficulty`   | Bytes               | **The difficulty of the PoW puzzle.**                        |\r\n| `number`       | uint64              | **The block height.**                                        |\r\n| `parent_hash`  | H256(hash)          | **The hash of the parent block.**                            |\r\n| `seal`         | `nonce` and `proof` | **The seal of a block.** After finished the block assembling, the miner can start to do the calculation for finding the solution of the PoW puzzle. The \"solution\" here is called `seal`. |\r\n| `seal.nonce`   | uint64              | **The nonce.** Similar to [the nonce in Bitcoin](https://en.bitcoin.it/wiki/Nonce). |\r\n| `seal.proof`   | Bytes               | **The solution of the PoW puzzle.**                          |\r\n| `timestamp`    | uint64              | **A [Unix time](http://en.wikipedia.org/wiki/Unix_time) timestamp.** |\r\n| `txs_commit`   | H256(hash)          | **The Merkle Root of the Merkle trie with the hash of transactions as leaves.** |\r\n| `txs_proposal` | H256(hash)          | **The Merkle Root of the Merkle trie with the hash of short transaction IDs as leaves.** |\r\n| `uncles_count` | uint32              | **The number of uncle blocks.**                              |\r\n| `uncles_hash`  | H256(hash)          | **The hash of the serialized uncle blocks data.** This will later be changed to using [CFB Encoding](https://github.com/nervosnetwork/cfb). |\r\n| `version`      | uint32              | **The version of the block**. This is for solving the compatibility issues might be occurred after a fork. |\r\n\r\n#### UncleBlock\r\n\r\n(`UncleBlock` is a sub-structure of `Block`.)\r\n\r\n| Name                    | Type          | Description                                                  |\r\n| ----------------------- | ------------- | ------------------------------------------------------------ |\r\n| `cellbase`              | `Transaction` | **The cellbase transaction of the uncle block.** The inner structure of this part is same as [the Transaction structure](#transaction) above. |\r\n| `header`                | `Header`      | **The block header of the uncle block.** The inner structure of this part is same as [the Header structure](#header) above. |\r\n| `proposals`             | [`string`]    | **An array of short transaction IDs of the proposed transactions in the uncle block.** |\r\n\r\n","\\0020-ckb-consensus-protocol\\0020-ckb-consensus-protocol.md":"---\r\nNumber: \"0020\"\r\nCategory: Informational\r\nStatus: Draft\r\nAuthor: Ren Zhang\r\nOrganization: Nervos Foundation\r\nCreated: 2019-6-19\r\n---\r\n# CKB Consensus Protocol\r\n\r\n* [Abstract](#Abstract)\r\n* [Motivation](#Motivation)\r\n* [Technical Overview](#Technical-Overview)\r\n  * [Eliminating the Bottleneck in Block Propagation](#Eliminating-the-Bottleneck-in-Block-Propagation)\r\n  * [Utilizing the Shortened Latency for Higher Throughput](#Utilizing-the-Shortened-Latency-for-Higher-Throughput)\r\n  * [Mitigating Selfish Mining Attacks](#Mitigating-Selfish-Mining-Attacks)\r\n* [Specification](#Specification)\r\n  * [Two-Step Transaction Confirmation](#Two-Step-Transaction-Confirmation)\r\n  * [Dynamic Difficulty Adjustment Mechanism](#Dynamic-Difficulty-Adjustment-Mechanism)\r\n\r\n<a name=\"Abstract\"></a>\r\n## Abstract\r\n\r\nBitcoin's Nakamoto Consensus (NC) is well-received due to its simplicity and low communication overhead. However, NC suffers from two kinds of drawback: first, its transaction processing throughput is far from satisfactory; second, it is vulnerable to a selfish mining attack, where attackers can gain more block rewards by deviating from the protocol's prescribed behavior.\r\n\r\nThe CKB consensus protocol is a variant of NC that raises its performance limit and selfish mining resistance while keeping its merits. By identifying and eliminating the bottleneck in NC's block propagation latency, our protocol supports very short block interval without sacrificing security. The shortened block interval not only raises the throughput, but also lowers the transaction confirmation latency. By incorporating all valid blocks in the difficulty adjustment, selfish mining is no longer profitable in our protocol.\r\n\r\n<a name=\"Motivation\"></a>\r\n## Motivation\r\n\r\nAlthough a number of non-NC consensus mechanisms have been proposed, NC has the following threefold advantage comparing with its alternatives. First, its security is carefully scrutinized and well-understood [[1](https://www.cs.cornell.edu/~ie53/publications/btcProcFC.pdf), [2](https://eprint.iacr.org/2014/765.pdf), [3](https://fc16.ifca.ai/preproceedings/30_Sapirshtein.pdf), [4](https://eprint.iacr.org/2016/454.pdf), [5](https://eprint.iacr.org/2016/1048.pdf), [6](https://eprint.iacr.org/2018/800.pdf), [7](https://eprint.iacr.org/2018/129.pdf), [8](https://arxiv.org/abs/1607.02420)], whereas alternative protocols often open new attack vectors, either unintentionally [[1](http://fc19.ifca.ai/preproceedings/180-preproceedings.pdf), [2](https://www.esat.kuleuven.be/cosic/publications/article-3005.pdf)] or by relying on security assumptions that are difficult to realize in practice [[1](https://arxiv.org/abs/1711.03936), [2](https://arxiv.org/abs/1809.06528)]. Second, NC minimizes the consensus protocol's communication overhead. In the best-case scenario, propagating a 1MB block in Bitcoin is equivalent to broadcasting a compact block message of roughly 13KB [[1](https://github.com/bitcoin/bips/blob/master/bip-0152.mediawiki), [2](https://www.youtube.com/watch?v=EHIuuKCm53o)]; valid blocks are immediately accepted by all honest nodes. In contrast, alternative protocols often demand a non-negligible communication overhead to certify that certain nodes witness a block. For example, [Algorand](https://algorandcom.cdn.prismic.io/algorandcom%2Fa26acb80-b80c-46ff-a1ab-a8121f74f3a3_p51-gilad.pdf) demands that each block be accompanied by 300KB of block certificate. Third, NC's chain-based topology ensures that a transaction global order is determined at block generation, which is compatible with all smart contract programming models. Protocols adopting other topologies either [abandon the global order](https://allquantor.at/blockchainbib/pdf/sompolinsky2016spectre.pdf) or establish it after a long confirmation delay [[1](https://eprint.iacr.org/2018/104.pdf), [2](https://eprint.iacr.org/2017/300.pdf)], limiting their efficiency or functionality.\r\n\r\nDespite NC's merits, a scalability barrier hinders it from processing more than a few transactions per second. Two parameters collectively cap the system's throughput: the maximum block size and the expected block interval. For example, Bitcoin enforces a roughly 4MB block size upper bound and targets a 10-minute block interval and  with its **difficulty adjustment mechanism**, translating to roughly ten transactions per second (TPS). Increasing the block size or reducing the block interval leads to longer block propagation latency or more frequent block generation events, respectively; both approaches raise the fraction of blocks generated during other blocks' propagation, thus raising the fraction of competing blocks. As at most one block among the competing ones contributes to transaction confirmation, the nodes' bandwidth on propagating other **orphaned blocks** is wasted, limiting the system's effective throughput. Moreover, raising the orphan rate downgrades the protocol's security by lowering the difficulty of double-spending attacks [[1](<https://fc15.ifca.ai/preproceedings/paper_30.pdf>), [2](<https://fc15.ifca.ai/preproceedings/paper_101.pdf>)].\r\n\r\nMoreover, the security of NC is undermined by a [**selfish mining attack**](https://www.cs.cornell.edu/~ie53/publications/btcProcFC.pdf), which allows attackers to gain unfair block rewards by deliberately orphaning blocks mined by other miners. Researchers observe that the unfair profit roots in NC's difficulty adjustment mechanism, which neglects orphaned blocks when estimating the network's computing power. Through this mechanism, the increased orphan rate caused by selfish mining leads to lower mining difficulty, enabling the attacker's higher time-averaged block reward [[1](https://eprint.iacr.org/2016/555.pdf), [2](https://github.com/ethereum/EIPs/blob/master/EIPS/eip-100.md), [3](https://arxiv.org/abs/1805.08281)].\r\n\r\nIn this RFC, we present the CKB consensus protocol, a consensus protocol that raises NC's performance limit and selfish mining resistance while keeping all NC's merits. Our protocol supports very short block interval by reducing the block propagation latency. The shortened block interval not only raises the blockchain's throughput, but also minimizes the transaction confirmation latency without decreasing the level of confidence, as the orphan rate remains low. Selfish mining is no longer profitable as we incorporate all blocks, including uncles, in the difficulty adjustment when estimating the network's computing power, so that the new difficulty is independent of the orphan rate.\r\n\r\n<a name=\"Technical-Overview\"></a>\r\n## Technical Overview\r\n\r\nOur consensus protocol makes three changes to NC.\r\n\r\n<a name=\"#Eliminating-the-Bottleneck-in-Block-Propagation\"></a>\r\n### Eliminating the Bottleneck in Block Propagation\r\n\r\n[Bitcoin's developers identify](https://www.youtube.com/watch?v=EHIuuKCm53o) that when the block interval decreases, the bottleneck in block propagation latency is transferring **fresh transactions**, which are newly broadcast transactions that have not finished propagating to the network when embedded in the latest block. Nodes that have not received these transactions must request them before forwarding the block to their neighbors. The resulted delay not only limits the blockchain's performance, but can also be exploited in a **de facto selfish mining attack**, where attackers deliberately embed fresh transactions in their blocks, hoping that the longer propagation latency gives them an advantage in finding the next block to gain more rewards.\r\n\r\nDeparting from this observation, our protocol eliminates the bottleneck by decoupling NC's transaction confirmation into two separate steps: **propose** and **commit**. A transaction is proposed if its truncated hash, named `txpid`, is embedded in the **proposal zone** of a blockchain block or its **uncles**---orphaned blocks that are referred to by the blockchain block. Newly proposed transactions affect neither the block validity nor the block propagation, as a node can start transferring the block to its neighbors before receiving these transactions. The transaction is committed if it appears in the **commitment zone** in a window starting several blocks after its proposal. This two-step confirmation rule eliminates the block propagation bottleneck, as committed transactions in a new block are already received and verified by all nodes when they are proposed. The new rule also effectively mitigates de facto selfish mining by limiting the attack time window.\r\n\r\n<a name=\"Utilizing-the-Shortened-Latency-for-Higher-Throughput\"></a>\r\n### Utilizing the Shortened Latency for Higher Throughput\r\n\r\nOur protocol prescribes that blockchain blocks refer to all orphaned blocks as uncles. This information allows us to estimate the current block propagation latency and dynamically adjust the expected block interval, increasing the throughput when the latency improves. Accordingly, our difficulty adjustment targets a fixed orphan rate to utilize the shortened latency without compromising security. The protocol hard-codes the upper and lower bounds of the interval to defend against DoS attacks and avoid overloading the nodes. In addition, the block reward is adjusted proportionally to the expected block interval within an epoch, so that the expected time-averaged reward is independent of the block interval.\r\n\r\n<a name=\"Mitigating-Selfish-Mining-Attacks\"></a>\r\n### Mitigating Selfish Mining Attacks\r\n\r\nOur protocol incorporate all blocks, including uncles, in the difficulty adjustment when estimating the network's computing power, so that the new difficulty is independent of the orphan rate, following the suggestion of [Vitalik](https://github.com/ethereum/EIPs/blob/master/EIPS/eip-100.md), [Grunspan and Perez-Marco](https://arxiv.org/abs/1805.08281).\r\n\r\nIn addition, we prove that selfish mining is no longer profitable in our protocol. This prove is non-trivial as Vitalik, Grunspan and Perez-Marco's informal arguments do not rule out the possibility that the attacker adapts to the modified mechanism and still gets unfair block reward. For example, the attacker may temporarily turn off some mining gears in the first epoch, causing the modified difficulty adjustment algorithm to underestimate the network's computing power, and starts selfish mining in the second epoch for a higher overall time-averaged reward. We prove that in our protocol, selfish mining is not profitable regardless of how the attacker divides its mining power among honest mining, selfish mining and idle, and how many epochs the attack involves. The detailed proof will be released later.\r\n\r\n<a name=\"Specification\"></a>\r\n## Specification\r\n\r\n<a name=\"Two-Step-Transaction-Confirmation\"></a>\r\n### Two-Step Transaction Confirmation\r\n\r\nIn our protocol, we use a two-step transaction confirmation to eliminate the aforementioned block propagation bottleneck, regardless of how short the block interval is. We start by defining the two steps and the block structure, and then introduce the new block propagation protocol. \r\n\r\n#### Definitions\r\n\r\n> **Definition 1:** A transaction’s proposal id `txpid` is defined as the first *l* bits of the transaction hash `txid`.\r\n\r\nIn our protocol, `txpid` does not need to be as globally unique as `txid`, as a `txpid` is used to identify a transaction among several neighboring blocks. Since we embed `txpid`s in both blocks and compact blocks, sending only the truncated `txid`s could reduce the bandwidth consumption. \r\n\r\nWhen multiple transactions share the same `txpid`s, all of them are considered proposed. In practice, we can set *l* to be large enough so that the computational effort of finding a collision is non-trivial.\r\n\r\n> **Definition 2:** A block *B*<sub>1</sub> is considered to be the *uncle* of another block *B*<sub>2</sub> if all of the following conditions are met:\r\n>​\t(1) *B*<sub>1</sub> and *B*<sub>2</sub> are in the same epoch, sharing the same difficulty;\r\n>​\t(2) height(*B*<sub>2</sub>) > height(*B*<sub>1</sub>);\r\n>​\t(3) *B*<sub>2</sub> is the first block in its chain to refer to *B*<sub>1</sub>. \r\n\r\nOur uncle definition is different from [that of Ethereum](https://github.com/ethereum/wiki/wiki/White-Paper#modified-ghost-implementation), in that we do not consider how far away the two blocks' first common ancestor is, as long as the two blocks are in the same epoch.\r\n\r\n> **Definition 3:** A transaction is *proposed* at height *h*<sub>p</sub> if its `txpid` is in the proposal zone of the main chain block with height *h*<sub>p</sub> and this block’s uncles. \r\n\r\nIt is possible that a proposed transaction is previously proposed, in conflict with other transactions, or even malformed. These incidents do not affect the block’s validity, as the proposal zone is used to facilitate transaction synchronization.\r\n\r\n> **Definition 4:** A non-coinbase transaction is *committed* at height *h*<sub>c</sub> if all of the following conditions are met: \r\n> ​\t(1) the transaction is proposed at height *h*<sub>p</sub> of the same chain, and *w<sub>close</sub>  ≤  h<sub>c</sub> − h*<sub>p</sub>  ≤  *w<sub>far</sub>*\r\n> ​\t(2) the transaction is in the commitment zone of the main chain block with height *h*<sub>c</sub>; \r\n> ​\t(3) the transaction is not in conflict with any previously-committed transactions in the main chain. \r\n> The coinbase transaction is committed at height *h*<sub>c</sub> if it satisfies (2).\r\n\r\n*w<sub>close</sub>* and *w<sub>far</sub>* define the closest and farthest on-chain distance between a transaction’s proposal and commitment. We require *w<sub>close</sub>*  to be large enough so that *w<sub>close</sub>* block intervals are long enough for a transaction to be propagated to the network. \r\n\r\nThese two parameters are also set according to the maximum number of transactions in the proposed transaction pool of a node’s memory. As the total number of proposed transactions is limited, they can be stored in the memory so that there is no need to fetch a newly committed transaction from the hard disk in most occasions. \r\n\r\nA transaction is considered embedded in the blockchain when it is committed. Therefore, a receiver that requires σ confirmations needs to wait for at least *w<sub>close</sub>* +σ blocks after the transaction is broadcast to have confidence in the transaction. \r\n\r\nIn practice, this *w<sub>close</sub>* - block extra delay is compensated by our protocol’s shortened block interval, so that the usability is not affected.\r\n\r\n#### Block and Compact Block Structure\r\n\r\nA block in our protocol includes the following fields:\r\n\r\n| Name            | Description                          |\r\n| :-------------- | :----------------------------------- |\r\n| header          | block metadata                       |\r\n| commitment zone | transactions committed in this block |\r\n| proposal zone   | `txpid`s proposed in this block      |\r\n| uncle headers   | headers of uncle blocks              |\r\n| uncles’ proposal zones   | `txpid`s proposed in the uncles              |\r\n\r\nSimilar to NC, in our protocol, a compact block replaces a block’s commitment zone with the transactions’ `shortid`s, a salt and a list of prefilled transactions. All other fields remain unchanged in [the compact block](https://github.com/bitcoin/bips/blob/master/bip-0152.mediawiki).\r\n\r\nAdditional block structure rules:\r\n\r\n- The total size of the first four fields should be no larger than the hard-coded **block size limit**. The main purpose of implementing a block size limit is to avoid overloading public nodes' bandwidth. The uncle blocks’ proposal zones do not count in the limit as they are usually already synchronized when the block is mined. \r\n- The number of `txpid`s in a proposal zone also has a hard-coded upper bound.\r\n\r\nTwo heuristic requirements may help practitioners choose the parameters. First, the upper bound number of `txpid`s in a proposal zone should be no smaller than the maximum number of committed transactions in a block, so that even if *w<sub>close</sub>=w<sub>far</sub>*, this bound is not the protocol's throughput bottleneck. Second, ideally the compact block should be no bigger than 80KB. According to [a 2016 study by Croman et al.](https://fc16.ifca.ai/bitcoin/papers/CDE+16.pdf), messages no larger than 80KB have similar propagation latency in the Bitcoin network; larger messages propagate slower as the network throughput becomes the bottleneck. This number may change as the network condition improves.\r\n\r\n#### Block Propagation Protocol\r\n\r\nIn line with [[1](https://www.cs.cornell.edu/~ie53/publications/btcProcFC.pdf), [2](https://arxiv.org/abs/1312.7013), [3](https://eprint.iacr.org/2014/007.pdf)], nodes should broadcast all blocks with valid proofs-of-work, including orphans, as they may be referred to in the main chain as uncles. Valid proofs-of-work cannot be utilized to pollute the network, as constructing them is time-consuming. \r\n\r\nOur protocol’s block propagation protocol removes the extra round trip of fresh transactions in most occasions. When the round trip is inevitable, our protocol ensures that it only lasts for one hop in the propagation. This is achieved by the following three rules: \r\n\r\n1. If some committed transactions are previously unknown to the sending node, they will be embedded in the prefilled transaction list and sent along with the compact block. This only happens in a de facto selfish mining attack, as otherwise transactions are synchronized when they are proposed. This modification removes the extra round trip if the sender and the receiver share the same list of proposed, but-not-broadcast transactions. \r\n2. If certain committed transactions are still missing, the receiver queries the sender with a short timeout. Triggering this mechanism requires not only a successful de facto selfish mining attack, but also an attack on transaction propagation to cause inconsistent proposed transaction pools among the nodes. Failing to send these transactions in time leads to the receiver disconnecting and blacklisting the sender. Blocks with incomplete commitment zones will not be propagated further.\r\n\r\n3. As long as the commitment zone is complete and valid, a node can start forwarding the compact block before receiving all newly-proposed transactions. In our protocol, a node requests the newly-proposed transactions from the upstream peer and sends compact blocks to other peers simultaneously. This modification does not downgrade the security as transactions in the proposal zone do not affect the block’s validity.\r\n\r\n\r\nThe first two rules ensure that the extra round trip caused by a de facto selfish mining attack never lasts for more than one hop.\r\n\r\n<a name=\"Dynamic-Difficulty-Adjustment-Mechanism\"></a>\r\n### Dynamic Difficulty Adjustment Mechanism\r\n\r\nWe modify the Nakamoto Consensus difficulty adjustment mechanism, so that: (1) Selfish mining is no longer profitable; (2) Throughput is dynamically adjusted based on the network’s bandwidth and latency. To achieve (1), our protocol incorporates all blocks, instead of only the main chain, in calculating the **adjusted hash rate estimation** of the last epoch, which determines the amount of computing effort required in the next epoch for each reward unit. To achieve (2), our protocol calculates the number of main chain blocks in the next epoch with the last epoch’s orphan rate. The block reward and target are then computed by combining these results. \r\n\r\nAdditional constraints are introduced to maximize the protocol’s compatibility:\r\n\r\n1. All epochs have the same expected length *L<sub>ideal</sub>*, and the maximum block reward issued in an epoch R(*i*) depends only on the epoch number *i*, so that the dynamic block interval does not complicate the reward issuance policy. \r\n\r\n2. Several upper and lower bounds are applied to the hash rate estimation and the number of main chain blocks, so that our protocol does not harm the decentralization or attack-resistance of the network.\r\n\r\n#### Notations\r\n\r\nSimilar to Nakamoto Consensus , our protocol’s difficulty adjustment algorithm is executed at the end of every epoch. It takes four inputs:\r\n\r\n| Name            | Description                          |\r\n| :-------------- | :----------------------------------- |\r\n| *T*<sub>*i*</sub>          | Last epoch’s target                       |\r\n| *L*<sub>*i*</sub> | Last epoch’s duration: the timestamp difference between epoch *i* and epoch (*i* − 1)’s last blocks |\r\n| *C*<sub>*i*,m</sub>   | Last epoch’s main chain block count      |\r\n| *C*<sub>*i*,o</sub>   | Last epoch’s orphan block count:  the number of uncles embedded in epoch *i*’s main chain         |\r\n\r\nAmong these inputs, *T<sub>i</sub>* and *C*<sub>*i*,m</sub> are determined by the last iteration of difficulty adjustment; *L*<sub>*i*</sub> and *C*<sub>*i*,o</sub> are measured after the epoch ends. The orphan rate *o*<sub>*i*</sub> is calculated as *C*<sub>*i*,o</sub> / *C*<sub>*i*,m</sub>. We do not include *C*<sub>*i*,o</sub> in the denominator to simplify the equation. As some orphans at the end of the epoch might be excluded from the main chain by an attack, *o*<sub>*i*</sub> is a lower bound of the actual number. However, [the proportion of deliberately excluded orphans is negligible](https://eprint.iacr.org/2014/765.pdf) as long as the epoch is long enough, as the difficulty of orphaning a chain grows exponentially with the chain length. \r\n\r\nThe algorithm outputs three values:\r\n\r\n| Name            | Description                          |\r\n| :-------------- | :----------------------------------- |\r\n| *T*<sub>*i*+1</sub>          | Next epoch’s target                       |\r\n| *C*<sub>i+1,m</sub> | Next epoch’s main chain block count |\r\n| *r*<sub>*i*+1</sub>   | Next epoch’s block reward     |\r\n\r\nIf the network hash rate and block propagation latency remains constant, *o*<sub>*i*+1</sub> should reach the ideal value *o*<sub>ideal</sub>, unless *C*<sub>*i*+1,m</sub> is equal to its upper bound *C*<sub>m</sub><sup>max</sup>  or its lower bound *C*<sub>m</sub><sup>min</sup> . Epoch *i* + 1 ends when it reaches *C*<sub>*i*+1,m</sub> main chain blocks, regardless of how many uncles are embedded.\r\n\r\n#### Computing the Adjusted Hash Rate Estimation\r\n\r\nThe adjusted hash rate estimation, denoted as *HPS<sub>i</sub>* is computed by applying a dampening factor τ to the last epoch’s actual hash rate ![1559068235154](images/1559068235154.png). The actual hash rate is calculated as follows:\r\n\r\n![1559064934639](images/1559064934639.png)\r\n\r\nwhere:\r\n\r\n- HSpace is the size of the entire hash space, e.g., 2^256 in Bitcoin,\r\n- HSpace/*T<sub>i</sub>* is the expected number of hash operations to find a valid block, and \r\n- *C*<sub>*i*,m</sub> + *C*<sub>*i*,o</sub> is the total number of blocks in epoch *i*\r\n\r\n![1559068266162](images/1559068266162.png) is computed by dividing the expected total hash operations with the duration *L<sub>i</sub>*\r\n\r\nNow we apply the dampening filter:\r\n\r\n![1559064108898](images/1559064108898.png)\r\n\r\nwhere *HPS*<sub>*i*−1</sub> denotes the adjusted hash rate estimation output by the last iteration of the difficulty adjustment algorithm. The dampening factor ensures that the adjusted hash rate estimation does not change more than a factor of τ between two consecutive epochs. This adjustment is equivalent to the Nakamoto Consensus application of a dampening filter. Bounding the adjustment speed prevents the attacker from arbitrarily biasing the difficulty and forging a blockchain, even if some victims’ network is temporarily controlled by the attacker.\r\n\r\n#### Modeling Block Propagation\r\n\r\nIt is difficult, if not impossible, to model the detailed block propagation procedure, given that the network topology changes constantly over time. Luckily, for our purpose, it is adequate to express the influence of block propagation with two parameters, which will be used to compute *C*<sub>*i*+1,m</sub>  later.\r\n\r\nWe assume all blocks follow a similar propagation model, in line with [[1](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.395.8058&rep=rep1&type=pdf), [2](https://fc16.ifca.ai/bitcoin/papers/CDE+16.pdf)]. In the last epoch, it takes *d* seconds for a block to be propagated to the entire network, and during this process, the average fraction of mining power working on the block’s parent is *p*. Therefore, during this *d* seconds, *HPS*<sub>*i* </sub> × *dp* hash operations work on the parent, thus not contributing to extending the blockchain, while the rest *HPS*<sub>*i*</sub> × *d*(1 − *p*) hashes work on the new block. Consequently, in the last epoch, the total number of hashes that do not extend the blockchain is *HPS*<sub>*i*</sub>  × *dp* × *C*<sub>*i*,m</sub>. If some of these hashes lead to a block, one of the competing blocks will be orphaned. The number of hash operations working on observed orphaned blocks is HSpace/*T*<sub>*i*</sub> × *C*<sub>*i*,o</sub>. If we ignore the rare event that more than two competing blocks are found at the same height, we have:\r\n\r\n![1559064685714](images/1559064685714.png)\r\n\r\nnamely\r\n\r\n![1559064995366](images/1559064995366.png)\r\n\r\n\r\n\r\nIf we join this equation with Equation (2), we can solve for *dp*:\r\n\r\n![1559065017925](images/1559065017925.png)\r\n\r\nwhere *o<sub>i</sub>* is last epoch’s orphan rate.\r\n\r\n#### Computing the Next Epoch’s Main Chain Block Number\r\nIf the next epoch’s block propagation proceeds identically to the last epoch, the value *dp* should remain unchanged. In order to achieve the ideal orphan rate *o*<sub>ideal</sub> and the ideal epoch duration *L*<sub>ideal</sub>, following the same reasoning with Equation (4). We should have:\r\n\r\n![1559065197341](images/1559065197341.png)\r\n\r\n\r\n\r\nwhere ![1559065416713](images/1559065416713.png)is the number of main chain blocks in the next epoch, if our only goal is to achieve *o*<sub>ideal</sub> and *L*<sub>ideal</sub> . \r\n\r\nBy joining Equation (4) and (5), we can solve for ![1559065488436](images/1559065416713.png):\r\n\r\n![1559065517956](images/1559065517956.png)\r\n\r\n\r\n\r\nNow we can apply the upper and lower bounds to![1559065488436](images/1559065416713.png) and get *C*<sub>*i*+1,m</sub>:\r\n\r\n![1559065670251](images/1559065670251.png)\r\n\r\nApplying a lower bound ensures that an attacker cannot mine orphaned blocks deliberately to arbitrarily increase the block interval; applying an upper bound ensures that our protocol does not confirm more transactions than the capacity of most nodes.\r\n\r\n#### Determining the Target Difficulty\r\n\r\nFirst, we introduce an adjusted orphan rate estimation ![1559065968791](images/1559065968791.png), which will be used to compute the target:\r\n\r\n![1559065997745](images/1559065997745.png)\r\n\r\n\r\n\r\nUsing ![1559065968791](images/1559065968791.png) instead of *o*<sub>ideal</sub> prevents some undesirable situations when the main chain block number reaches the upper or lower bound. Now we can compute *T*<sub>*i*+1</sub>:\r\n\r\n![1559066101731](images/1559066101731.png)\r\n\r\nwhere ![1559066131427](images/1559066131427.png) is the total hashes, ![1559066158164](images/1559066158164.png)is the total number of blocks. \r\n\r\nThe denominator in Equation (7) is the number of hashes required to find a block.\r\n\r\nNote that if none of the edge cases are triggered, such as ![1559066233715](images/1559066233715.png)![1559066249700](images/1559066249700.png) or ![1559066329440](images/1559066329440.png)  , we can combine Equations (2), (6), and (7) and get:\r\n\r\n![1559066373372](images/1559066373372.png)\r\n\r\n\r\n\r\nThis result is consistent with our intuition. On one hand, if the last epoch’s orphan rate *o*<sub>*i*</sub> is larger than the ideal value *o*<sub>ideal</sub>, the target lowers, thus increasing the difficulty of finding a block and raising the block interval if the total hash rate is unchanged. Therefore, the orphan rate is lowered as it is more unlikely to find a block during another block’s propagation. On the other hand, the target increases if the last epoch’s orphan rate is lower than the ideal value, decreasing the block interval and raising the system’s throughput.\r\n\r\n#### Computing the Reward for Each Block\r\n\r\nNow we can compute the reward for each block:\r\n\r\n![1559066526598](images/1559066526598.png)\r\n\r\nThe two cases differ only in the edge cases. The first case guarantees that the total reward issued in epoch *i* + 1 will not exceed R(*i* + 1).\r\n\r\n","\\0021-ckb-address-format\\0021-ckb-address-format.md":"---\r\nNumber: 21\r\nCategory: Standards Track\r\nStatus: Proposal\r\nAuthor: Cipher Wang\r\nOrganization: Nervos Foundation\r\nCreated: 2019-01-20\r\n---\r\n\r\n# CKB Address Format\r\n\r\n## Abstract\r\n\r\n*CKB Address Format* is an application level cell **lock script** display recommendation. The lock script consists of three key parameters, including *code_hash*, *hash_type* and *args*. CKB address packages lock script into a single line, verifiable, and human read friendly format.\r\n\r\n## Data Structure\r\n\r\n### Payload Format Types\r\n\r\nTo generate a CKB address, we firstly encode lock script to bytes array, name *payload*. And secondly, we wrap the payload into final address format.\r\n\r\nThere are several methods to convert lock script into payload bytes array. We use 1 byte to identify the payload format.\r\n\r\n| format type |                   description                  |\r\n|:-----------:|------------------------------------------------|\r\n|  0x01       | short version for locks with popular code_hash |\r\n|  0x02       | full version with hash_type = \"Data\"           |\r\n|  0x04       | full version with hash_type = \"Type\"           |\r\n\r\n### Short Payload Format\r\n\r\nShort payload format is a compact format which identifies common used code_hash by 1 byte code_hash_index instead of 32 bytes code_hash.\r\n\r\n```c\r\npayload = 0x01 | code_hash_index | single_arg\r\n```\r\n\r\nTo translate payload to lock script, one can convert code_hash_index to code_hash and hash_type with the following *popular code_hash table*. And single_arg as the args.\r\n\r\n\r\n| code_hash_index |        code_hash     |   hash_type  |      args     |\r\n|:---------------:|----------------------|:------------:|---------------|\r\n|      0x00       | SECP256K1 + blake160 |     Type     |  blake160(PK) |\r\n|      0x01       | SECP256K1 + hash160  |     Type     |  hash160(PK)  |\r\n\r\n\\* The blake160 here means the leading 20 bytes truncation of Blake2b hash result.\r\n\\* The hash160 here means Bitcoin address calculation *double hash* algorithm, which means `hash160(PK) := Ripemd160(SHA256(PK))`.\r\n\r\n### Full Payload Format\r\n\r\nFull payload format directly encodes all data field of lock script.\r\n\r\n```c\r\npayload = 0x02/0x04 | code_hash | len(arg[0]) | arg[0] | ...\r\n```\r\n\r\nThe first byte identifies the lock script's hash_type, 0x02 for \"Data\", 0x04 for \"Type\". We convert every element of args to plain bytes array format, and add a length number in front of every array. To keep it simple, we limit every argument size to maxium 256, which is 1 byte.\r\n\r\n## Wrap to Address\r\n\r\nWe follow [Bitcoin base32 address format (BIP-173)][bip173] rules to wraps payload in to address, which uses Bech32 encoding and a [BCH checksum][bch].\r\n\r\nThe original version of Bech32 allows at most 90 characters long. Similar with [BOLT][BOLT_url], we simply remove the length limit. The error correction function is disabled when the Bech32 string is longer than 90. We don't intent to use this function anyway, because there is a risk to get wrong correction result.\r\n\r\nA Bech32 string consists of the **human-readable part**, the **separator**, and the **data part**. The last 6 characters of data part is checksum. The data part is base32 encoded. Here is the readable translation of base32 encoding table.\r\n\r\n|       |0|1|2|3|4|5|6|7|\r\n|-------|-|-|-|-|-|-|-|-|\r\n|**+0** |q|p|z|r|y|9|x|8|\r\n|**+8** |g|f|2|t|v|d|w|0|\r\n|**+16**|s|3|j|n|5|4|k|h|\r\n|**+24**|c|e|6|m|u|a|7|l|\r\n\r\nThe human-readable part is \"**ckb**\" for CKB mainnet, and \"**ckt**\" for the testnet. The separator is always \"1\".\r\n\r\n![](images/ckb-address.png)\r\n\r\n## Examples and Demo Code\r\n\r\n```py\r\n    # for short payload format\r\n    pk = \"13e41d6F9292555916f17B4882a5477C01270142\"\r\n    address = \"ckb1qyqp8eqad7ffy42ezmchkjyz54rhcqf8q9pqrn323p\"\r\n\r\n    # for full payload format\r\n    code_hash = \"48a2ce278d84e1102b67d01ac8a23b31a81cc54e922e3db3ec94d2ec4356c67c\"\r\n    hash_type = \"Data\"\r\n    args = ['dde7801c073dfb3464c7b1f05b806bb2bbb84e99', '00c1ddf9c135061b7635ca51e735fc2b03cee339']\r\n    address = \"ckb1qfy29n383kzwzyptvlgp4j9z8vc6s8x9f6fzu0dnaj2d9mzr2mr8c9xau7qpcpealv6xf3a37pdcq6ajhwuyaxg5qrqam7wpx5rpka34efg7wd0u9vpuaceeu5fsh5\"\r\n```\r\n\r\nDemo code: https://github.com/CipherWang/ckb-address-demo \r\n\r\n[bip173]: https://github.com/bitcoin/bips/blob/master/bip-0173.mediawiki\r\n\r\n[bch]: https://en.wikipedia.org/wiki/BCH_code\r\n\r\n[BOLT_url]: https://github.com/lightningnetwork/lightning-rfc/blob/master/11-payment-encoding.md\r\n"}}